{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thick-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import MultiIndex, Int16Dtype\n",
    "Ref=pd.read_csv('Ref.csv')\n",
    "Ref[\"CO\"] = 1000 * Ref[\"CO\"]\n",
    "Ref['Date'] = pd.to_datetime(Ref['Date_Time'])\n",
    "Ref=Ref.set_index('Date')\n",
    "Ref.drop('Date_Time',axis = 1, inplace = True)\n",
    "Ref=Ref.resample('5min').mean()\n",
    "Ref=Ref[76463:137376]\n",
    "Ref_CO=Ref['CO'].to_list()\n",
    "Ref_NO2=Ref['NO2'].to_list()\n",
    "Ref_SO2=Ref['SO2'].to_list()\n",
    "Ref_O3=Ref['O3'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-edward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-30 19:00:00</th>\n",
       "      <td>242.060716</td>\n",
       "      <td>18.870796</td>\n",
       "      <td>71.072939</td>\n",
       "      <td>188.915151</td>\n",
       "      <td>91.471325</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 20:00:00</th>\n",
       "      <td>218.655079</td>\n",
       "      <td>18.056864</td>\n",
       "      <td>75.132153</td>\n",
       "      <td>178.704750</td>\n",
       "      <td>84.176485</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 21:00:00</th>\n",
       "      <td>194.147868</td>\n",
       "      <td>17.090891</td>\n",
       "      <td>81.315038</td>\n",
       "      <td>161.421792</td>\n",
       "      <td>73.858294</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 22:00:00</th>\n",
       "      <td>206.432889</td>\n",
       "      <td>16.714085</td>\n",
       "      <td>82.568155</td>\n",
       "      <td>197.744043</td>\n",
       "      <td>83.279611</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 23:00:00</th>\n",
       "      <td>173.577429</td>\n",
       "      <td>16.365121</td>\n",
       "      <td>83.662401</td>\n",
       "      <td>162.157636</td>\n",
       "      <td>68.669099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lab1       Temp         RH         Ref  Net Signal  \\\n",
       "Date                                                                            \n",
       "2020-04-30 19:00:00  242.060716  18.870796  71.072939  188.915151   91.471325   \n",
       "2020-04-30 20:00:00  218.655079  18.056864  75.132153  178.704750   84.176485   \n",
       "2020-04-30 21:00:00  194.147868  17.090891  81.315038  161.421792   73.858294   \n",
       "2020-04-30 22:00:00  206.432889  16.714085  82.568155  197.744043   83.279611   \n",
       "2020-04-30 23:00:00  173.577429  16.365121  83.662401  162.157636   68.669099   \n",
       "\n",
       "                     Month  Day_of_week   Day  Hour  \n",
       "Date                                                 \n",
       "2020-04-30 19:00:00    4.0          3.0  30.0  19.0  \n",
       "2020-04-30 20:00:00    4.0          3.0  30.0  20.0  \n",
       "2020-04-30 21:00:00    4.0          3.0  30.0  21.0  \n",
       "2020-04-30 22:00:00    4.0          3.0  30.0  22.0  \n",
       "2020-04-30 23:00:00    4.0          3.0  30.0  23.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('CO.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO['Ref']=Ref_CO\n",
    "index_names = Data_CO[ (Data_CO['WE'] >1000)].index\n",
    "Data_CO.drop(index_names, inplace = True)\n",
    "WE=Data_CO['WE'].to_list()\n",
    "AE=Data_CO['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "data = pd.read_csv('Conc_CO.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO.drop(index_names, inplace = True)\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "CO_Data=CO_Data.resample('h').mean()\n",
    "CO_Data=CO_Data.dropna()\n",
    "CO_Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prime-morning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60913"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('O3.txt', header = None,low_memory=False)\n",
    "data.columns=['AE','WE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "Data_O3['Ref']=Ref_O3\n",
    "WE=Data_O3['WE'].to_list()\n",
    "AE=Data_O3['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna()\n",
    "data = pd.read_csv('Conc_O3.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna() \n",
    "O3_Data=O3_Data.resample('h').mean()\n",
    "O3_Data=O3_Data.dropna()\n",
    "O3_Data.head()\n",
    "\n",
    "ref_O3=Data_O3['Ref'].to_list()\n",
    "len(ref_O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "underlying-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('NO2.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_NO2=data\n",
    "Data_NO2['Ref']=Ref_NO2\n",
    "WE=Data_NO2['WE'].to_list()\n",
    "AE=Data_NO2['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_NO2['Net Signal']=signal\n",
    "Data_NO2['Month']=Data_NO2.index.month\n",
    "Data_NO2['Day_of_week']=Data_NO2.index.dayofweek\n",
    "Data_NO2['Day']=Data_NO2.index.day\n",
    "Data_NO2['Hour']=Data_NO2.index.hour\n",
    "NO2_Data=Data_NO2\n",
    "NO2_Data=NO2_Data[(NO2_Data[NO2_Data.columns] >= 0).all(axis=1)]\n",
    "NO2_Data=NO2_Data.dropna()\n",
    "data = pd.read_csv('Conc_NO2.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "subscript = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\") \n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_NO2=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_NO2['Net Signal']=signal\n",
    "Data_NO2['Month']=Data_NO2.index.month\n",
    "Data_NO2['Day_of_week']=Data_NO2.index.dayofweek\n",
    "Data_NO2['Day']=Data_NO2.index.day\n",
    "Data_NO2['Hour']=Data_NO2.index.hour\n",
    "Data_NO2['Ref_O3']=ref_O3\n",
    "NO2_Data=Data_NO2\n",
    "NO2_Data=NO2_Data[(NO2_Data[NO2_Data.columns] >= 0).all(axis=1)]\n",
    "NO2_Data=NO2_Data.dropna()\n",
    "NO2_Data=NO2_Data.resample('h').mean()\n",
    "NO2_Data=NO2_Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lyric-fifteen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-02 11:00:00</th>\n",
       "      <td>621.625704</td>\n",
       "      <td>26.378438</td>\n",
       "      <td>58.063437</td>\n",
       "      <td>46.094860</td>\n",
       "      <td>3.605625</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:00:00</th>\n",
       "      <td>725.154408</td>\n",
       "      <td>25.795055</td>\n",
       "      <td>48.256857</td>\n",
       "      <td>57.532808</td>\n",
       "      <td>13.865109</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.384051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:00:00</th>\n",
       "      <td>108.196313</td>\n",
       "      <td>32.344264</td>\n",
       "      <td>37.260757</td>\n",
       "      <td>47.259008</td>\n",
       "      <td>11.447809</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.255772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:00:00</th>\n",
       "      <td>135.822676</td>\n",
       "      <td>34.926112</td>\n",
       "      <td>35.013036</td>\n",
       "      <td>42.114260</td>\n",
       "      <td>10.075221</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.268034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 12:00:00</th>\n",
       "      <td>203.757758</td>\n",
       "      <td>36.201221</td>\n",
       "      <td>31.829282</td>\n",
       "      <td>45.701366</td>\n",
       "      <td>7.624153</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.770444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                           \n",
       "2019-10-02 11:00:00  621.625704  26.378438  58.063437  46.094860    3.605625   \n",
       "2019-10-02 12:00:00  725.154408  25.795055  48.256857  57.532808   13.865109   \n",
       "2019-10-07 10:00:00  108.196313  32.344264  37.260757  47.259008   11.447809   \n",
       "2019-10-07 11:00:00  135.822676  34.926112  35.013036  42.114260   10.075221   \n",
       "2019-10-07 12:00:00  203.757758  36.201221  31.829282  45.701366    7.624153   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour    Ref_NO2  \n",
       "Date                                                           \n",
       "2019-10-02 11:00:00   10.0          2.0  2.0  11.0  15.230400  \n",
       "2019-10-02 12:00:00   10.0          2.0  2.0  12.0   5.384051  \n",
       "2019-10-07 10:00:00   10.0          0.0  7.0  10.0   4.255772  \n",
       "2019-10-07 11:00:00   10.0          0.0  7.0  11.0  16.268034  \n",
       "2019-10-07 12:00:00   10.0          0.0  7.0  12.0  12.770444  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('O3.txt', header = None,low_memory=False)\n",
    "data.columns=['AE','WE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "Data_O3['Ref']=Ref_O3\n",
    "WE=Data_O3['WE'].to_list()\n",
    "AE=Data_O3['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna()\n",
    "data = pd.read_csv('Conc_O3.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "ref_NO2=Data_NO2['Ref'].to_list()\n",
    "Data_O3['Ref_NO2']=ref_NO2\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna()\n",
    "O3_Data=O3_Data.resample('h').mean()\n",
    "O3_Data=O3_Data.dropna()\n",
    "O3_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-spoke",
   "metadata": {},
   "source": [
    "# CO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empirical-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import sMAPE, smape_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "#'Ref_NO2','Ref_SO2','Ref_O3',\n",
    "#,'Month','Day_of_week','Day','Hour'\n",
    "X=CO_Data[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour']]\n",
    "y=CO_Data['Ref']\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size = 0.2,shuffle=True)\n",
    "#train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "selective-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from flaml.model import SKLearnEstimator\n",
    "# SKLearnEstimator is derived from BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "diverse-functionality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 12-06 08:41:13] {2599} INFO - task = regression\n",
      "[flaml.automl: 12-06 08:41:13] {2601} INFO - Data split method: uniform\n",
      "[flaml.automl: 12-06 08:41:13] {2604} INFO - Evaluation method: cv\n",
      "[flaml.automl: 12-06 08:41:13] {2726} INFO - Minimizing error metric: rmse\n",
      "[flaml.automl: 12-06 08:41:13] {2870} INFO - List of ML learners in AutoML Run: ['xgboost']\n",
      "[flaml.automl: 12-06 08:41:13] {3166} INFO - iteration 0, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:13] {3296} INFO - Estimated sufficient time budget=2574s. Estimated necessary time budget=3s.\n",
      "[flaml.automl: 12-06 08:41:13] {3343} INFO -  at 0.3s,\testimator xgboost's best error=422.3201,\tbest estimator xgboost's best error=422.3201\n",
      "[flaml.automl: 12-06 08:41:13] {3166} INFO - iteration 1, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:13] {3343} INFO -  at 0.5s,\testimator xgboost's best error=422.3201,\tbest estimator xgboost's best error=422.3201\n",
      "[flaml.automl: 12-06 08:41:13] {3166} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:14] {3343} INFO -  at 0.7s,\testimator xgboost's best error=241.4554,\tbest estimator xgboost's best error=241.4554\n",
      "[flaml.automl: 12-06 08:41:14] {3166} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:14] {3343} INFO -  at 0.9s,\testimator xgboost's best error=158.3488,\tbest estimator xgboost's best error=158.3488\n",
      "[flaml.automl: 12-06 08:41:14] {3166} INFO - iteration 4, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:14] {3343} INFO -  at 1.0s,\testimator xgboost's best error=158.3488,\tbest estimator xgboost's best error=158.3488\n",
      "[flaml.automl: 12-06 08:41:14] {3166} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:14] {3343} INFO -  at 1.2s,\testimator xgboost's best error=158.3488,\tbest estimator xgboost's best error=158.3488\n",
      "[flaml.automl: 12-06 08:41:14] {3166} INFO - iteration 6, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:14] {3343} INFO -  at 1.4s,\testimator xgboost's best error=146.2672,\tbest estimator xgboost's best error=146.2672\n",
      "[flaml.automl: 12-06 08:41:14] {3166} INFO - iteration 7, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:15] {3343} INFO -  at 1.7s,\testimator xgboost's best error=140.0395,\tbest estimator xgboost's best error=140.0395\n",
      "[flaml.automl: 12-06 08:41:15] {3166} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:15] {3343} INFO -  at 2.0s,\testimator xgboost's best error=140.0395,\tbest estimator xgboost's best error=140.0395\n",
      "[flaml.automl: 12-06 08:41:15] {3166} INFO - iteration 9, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:15] {3343} INFO -  at 2.4s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:15] {3166} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:16] {3343} INFO -  at 2.7s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:16] {3166} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:16] {3343} INFO -  at 3.0s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:16] {3166} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:16] {3343} INFO -  at 3.4s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:16] {3166} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:17] {3343} INFO -  at 3.7s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:17] {3166} INFO - iteration 14, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:17] {3343} INFO -  at 4.1s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:17] {3166} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:17] {3343} INFO -  at 4.4s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:17] {3166} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:18] {3343} INFO -  at 4.7s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:18] {3166} INFO - iteration 17, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:18] {3343} INFO -  at 5.0s,\testimator xgboost's best error=132.5747,\tbest estimator xgboost's best error=132.5747\n",
      "[flaml.automl: 12-06 08:41:18] {3166} INFO - iteration 18, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:18] {3343} INFO -  at 5.4s,\testimator xgboost's best error=122.5872,\tbest estimator xgboost's best error=122.5872\n",
      "[flaml.automl: 12-06 08:41:18] {3166} INFO - iteration 19, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:18] {3343} INFO -  at 5.6s,\testimator xgboost's best error=122.5872,\tbest estimator xgboost's best error=122.5872\n",
      "[flaml.automl: 12-06 08:41:18] {3166} INFO - iteration 20, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:20] {3343} INFO -  at 6.8s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:20] {3166} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:21] {3343} INFO -  at 7.7s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:21] {3166} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:21] {3343} INFO -  at 8.5s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:21] {3166} INFO - iteration 23, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:22] {3343} INFO -  at 9.0s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:22] {3166} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:23] {3343} INFO -  at 10.2s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:23] {3166} INFO - iteration 25, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:24] {3343} INFO -  at 10.7s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:24] {3166} INFO - iteration 26, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:26] {3343} INFO -  at 13.1s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:26] {3166} INFO - iteration 27, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:27] {3343} INFO -  at 13.8s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:27] {3166} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:28] {3343} INFO -  at 14.9s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:28] {3166} INFO - iteration 29, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:28] {3343} INFO -  at 15.4s,\testimator xgboost's best error=118.5104,\tbest estimator xgboost's best error=118.5104\n",
      "[flaml.automl: 12-06 08:41:28] {3166} INFO - iteration 30, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:32] {3343} INFO -  at 19.0s,\testimator xgboost's best error=115.0889,\tbest estimator xgboost's best error=115.0889\n",
      "[flaml.automl: 12-06 08:41:32] {3166} INFO - iteration 31, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:35] {3343} INFO -  at 21.7s,\testimator xgboost's best error=115.0889,\tbest estimator xgboost's best error=115.0889\n",
      "[flaml.automl: 12-06 08:41:35] {3166} INFO - iteration 32, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:36] {3343} INFO -  at 23.6s,\testimator xgboost's best error=113.2659,\tbest estimator xgboost's best error=113.2659\n",
      "[flaml.automl: 12-06 08:41:36] {3166} INFO - iteration 33, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:41] {3343} INFO -  at 27.7s,\testimator xgboost's best error=113.2659,\tbest estimator xgboost's best error=113.2659\n",
      "[flaml.automl: 12-06 08:41:41] {3166} INFO - iteration 34, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:43] {3343} INFO -  at 29.8s,\testimator xgboost's best error=113.2659,\tbest estimator xgboost's best error=113.2659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 12-06 08:41:43] {3166} INFO - iteration 35, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:52] {3343} INFO -  at 38.8s,\testimator xgboost's best error=113.2659,\tbest estimator xgboost's best error=113.2659\n",
      "[flaml.automl: 12-06 08:41:52] {3166} INFO - iteration 36, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:52] {3343} INFO -  at 39.4s,\testimator xgboost's best error=113.2659,\tbest estimator xgboost's best error=113.2659\n",
      "[flaml.automl: 12-06 08:41:52] {3166} INFO - iteration 37, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:55] {3343} INFO -  at 42.0s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:41:55] {3166} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl: 12-06 08:41:57] {3343} INFO -  at 44.0s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:41:57] {3166} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:00] {3343} INFO -  at 47.3s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:00] {3166} INFO - iteration 40, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:02] {3343} INFO -  at 49.3s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:02] {3166} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:04] {3343} INFO -  at 50.8s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:04] {3166} INFO - iteration 42, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:10] {3343} INFO -  at 57.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:10] {3166} INFO - iteration 43, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:13] {3343} INFO -  at 60.3s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:13] {3166} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:15] {3343} INFO -  at 62.3s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:15] {3166} INFO - iteration 45, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:20] {3343} INFO -  at 67.2s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:20] {3166} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:21] {3343} INFO -  at 68.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:21] {3166} INFO - iteration 47, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:24] {3343} INFO -  at 71.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:24] {3166} INFO - iteration 48, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:27] {3343} INFO -  at 74.0s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:27] {3166} INFO - iteration 49, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:30] {3343} INFO -  at 77.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:30] {3166} INFO - iteration 50, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:33] {3343} INFO -  at 80.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:34] {3166} INFO - iteration 51, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:39] {3343} INFO -  at 85.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:39] {3166} INFO - iteration 52, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:40] {3343} INFO -  at 87.1s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:40] {3166} INFO - iteration 53, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:45] {3343} INFO -  at 91.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:45] {3166} INFO - iteration 54, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:47] {3343} INFO -  at 94.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:47] {3166} INFO - iteration 55, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:50] {3343} INFO -  at 97.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:50] {3166} INFO - iteration 56, current learner xgboost\n",
      "[flaml.automl: 12-06 08:42:52] {3343} INFO -  at 99.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:42:52] {3166} INFO - iteration 57, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:00] {3343} INFO -  at 107.2s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:00] {3166} INFO - iteration 58, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:01] {3343} INFO -  at 108.2s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:01] {3166} INFO - iteration 59, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:02] {3343} INFO -  at 109.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:02] {3166} INFO - iteration 60, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:07] {3343} INFO -  at 114.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:07] {3166} INFO - iteration 61, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:12] {3343} INFO -  at 118.9s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:12] {3166} INFO - iteration 62, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:14] {3343} INFO -  at 121.0s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:14] {3166} INFO - iteration 63, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:18] {3343} INFO -  at 124.9s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:18] {3166} INFO - iteration 64, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:21] {3343} INFO -  at 128.0s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:21] {3166} INFO - iteration 65, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:22] {3343} INFO -  at 129.0s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:22] {3166} INFO - iteration 66, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:32] {3343} INFO -  at 139.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:32] {3166} INFO - iteration 67, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:37] {3343} INFO -  at 143.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:37] {3166} INFO - iteration 68, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:38] {3343} INFO -  at 145.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:38] {3166} INFO - iteration 69, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:46] {3343} INFO -  at 153.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:46] {3166} INFO - iteration 70, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:47] {3343} INFO -  at 154.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:47] {3166} INFO - iteration 71, current learner xgboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 12-06 08:43:51] {3343} INFO -  at 158.1s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:51] {3166} INFO - iteration 72, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:53] {3343} INFO -  at 159.9s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:53] {3166} INFO - iteration 73, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:56] {3343} INFO -  at 163.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:56] {3166} INFO - iteration 74, current learner xgboost\n",
      "[flaml.automl: 12-06 08:43:58] {3343} INFO -  at 165.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:43:58] {3166} INFO - iteration 75, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:01] {3343} INFO -  at 167.9s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:01] {3166} INFO - iteration 76, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:04] {3343} INFO -  at 171.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:04] {3166} INFO - iteration 77, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:08] {3343} INFO -  at 175.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:08] {3166} INFO - iteration 78, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:10] {3343} INFO -  at 177.3s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:10] {3166} INFO - iteration 79, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:14] {3343} INFO -  at 181.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:14] {3166} INFO - iteration 80, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:16] {3343} INFO -  at 183.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:16] {3166} INFO - iteration 81, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:18] {3343} INFO -  at 185.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:18] {3166} INFO - iteration 82, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:22] {3343} INFO -  at 189.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:22] {3166} INFO - iteration 83, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:26] {3343} INFO -  at 193.1s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:26] {3166} INFO - iteration 84, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:28] {3343} INFO -  at 195.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:28] {3166} INFO - iteration 85, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:30] {3343} INFO -  at 197.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:30] {3166} INFO - iteration 86, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:36] {3343} INFO -  at 203.7s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:36] {3166} INFO - iteration 87, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:41] {3343} INFO -  at 208.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:41] {3166} INFO - iteration 88, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:43] {3343} INFO -  at 210.2s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:43] {3166} INFO - iteration 89, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:47] {3343} INFO -  at 213.9s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:47] {3166} INFO - iteration 90, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:48] {3343} INFO -  at 215.5s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:48] {3166} INFO - iteration 91, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:52] {3343} INFO -  at 218.9s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:52] {3166} INFO - iteration 92, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:54] {3343} INFO -  at 221.1s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:54] {3166} INFO - iteration 93, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:56] {3343} INFO -  at 223.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:56] {3166} INFO - iteration 94, current learner xgboost\n",
      "[flaml.automl: 12-06 08:44:59] {3343} INFO -  at 226.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:44:59] {3166} INFO - iteration 95, current learner xgboost\n",
      "[flaml.automl: 12-06 08:45:03] {3343} INFO -  at 230.6s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:45:03] {3166} INFO - iteration 96, current learner xgboost\n",
      "[flaml.automl: 12-06 08:45:05] {3343} INFO -  at 232.4s,\testimator xgboost's best error=109.9350,\tbest estimator xgboost's best error=109.9350\n",
      "[flaml.automl: 12-06 08:45:05] {3166} INFO - iteration 97, current learner xgboost\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dac9c6243897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Train with labeled input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lab1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mautoml_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestimator_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xgboost\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lab1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mR2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/automl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, dataframe, label, metric, task, n_jobs, log_file_name, estimator_list, time_budget, max_iter, sample, ensemble, eval_method, log_type, model_history, split_ratio, n_splits, log_training_metric, mem_thres, pred_time_limit, train_time_limit, X_val, y_val, sample_weight_val, groups_val, groups, verbose, retrain_full, split_type, learner_selector, hpo_method, starting_points, seed, n_concurrent_trials, keep_search_state, preserve_checkpoint, early_stop, append_log, auto_augment, min_sample_size, use_ray, metric_constraints, custom_hp, cv_score_agg_func, skip_transform, fit_kwargs_by_estimator, **fit_kwargs)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtraining_log_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_log\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msave_helper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2896\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/automl.py\u001b[0m in \u001b[0;36m_search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3444\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3445\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_ray\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3446\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3447\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/automl.py\u001b[0m in \u001b[0;36m_search_sequential\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3267\u001b[0m                     )\n\u001b[1;32m   3268\u001b[0m             \u001b[0mstart_run_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3269\u001b[0;31m             analysis = tune.run(\n\u001b[0m\u001b[1;32m   3270\u001b[0m                 \u001b[0msearch_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3271\u001b[0m                 \u001b[0msearch_alg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_alg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(evaluation_function, config, low_cost_partial_config, cat_hp_cost, metric, mode, time_budget_s, points_to_evaluate, evaluated_rewards, resource_attr, min_resource, max_resource, reduction_factor, scheduler, search_alg, verbose, local_dir, num_samples, resources_per_trial, config_constraints, metric_constraints, max_failure, use_ray, use_incumbent_result_in_evaluation, log_file_name, lexico_objectives, **ray_args)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"trial {num_trials} config: {trial_to_run.config}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_to_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/automl.py\u001b[0m in \u001b[0;36m_compute_with_config_base\u001b[0;34m(config_w_resource, state, estimator)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mpred_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0msampled_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0msampled_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/ml.py\u001b[0m in \u001b[0;36mcompute_estimator\u001b[0;34m(X_train, y_train, X_val, y_val, weight_val, groups_val, budget, kf, config_dic, task, estimator_name, eval_method, eval_metric, best_val_loss, n_jobs, estimator_class, cv_score_agg_func, log_training_metric, fit_kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m         )\n\u001b[1;32m    609\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         val_loss, metric_for_logging, train_time, pred_time = evaluate_model_CV(\n\u001b[0m\u001b[1;32m    611\u001b[0m             \u001b[0mconfig_dic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/ml.py\u001b[0m in \u001b[0;36mevaluate_model_CV\u001b[0;34m(config, estimator, X_train_all, y_train_all, budget, kf, task, eval_metric, best_val_loss, cv_score_agg_func, log_training_metric, fit_kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mgroups_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         val_loss_i, metric_i, train_time_i, pred_time_i = get_val_loss(\n\u001b[0m\u001b[1;32m    524\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/ml.py\u001b[0m in \u001b[0;36mget_val_loss\u001b[0;34m(config, estimator, X_train, y_train, X_val, y_val, weight_val, groups_val, eval_metric, obj, labels, budget, log_training_metric, fit_kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;31m#     fit_kwargs['X_val'] = X_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;31m#     fit_kwargs['y_val'] = y_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m     val_loss, metric_for_logging, pred_time, _ = _eval_estimator(\n\u001b[1;32m    414\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, budget, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tree_method\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpu_hist\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpu_per_trial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, budget, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/flaml/model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X_train, y_train, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# xgboost 1.6 doesn't display all the params in the model str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"flaml.model - {model} fit started with params {self.params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"flaml.model - {model} fit finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[1;32m    540\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Initialize an AutoML instance\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 21600,  # in seconds\n",
    "    \"metric\": 'rmse',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"california.log\",\n",
    "}\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train.drop(['Lab1'], axis=1), y_train=y_train,**automl_settings,estimator_list=[\"xgboost\"])\n",
    "pred=automl.predict(X_test.drop(['Lab1'], axis=1))\n",
    "R2=round(sm.r2_score(y_test, pred), 2)\n",
    "r=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "RMSE=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y),2)\n",
    "print(r,R2,RMSE)\n",
    "# Predict\n",
    "#print(automl.predict(X_test))\n",
    "# Print the best model\n",
    "print(automl.model.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-ethernet",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel':  ['rbf']} \n",
    "  \n",
    "grid = GridSearchCV(SVR(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train.drop(['Lab1'], axis=1), y_train)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-underground",
   "metadata": {},
   "source": [
    "from flaml.model import SKLearnEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-orientation",
   "metadata": {},
   "source": [
    "from flaml.model import SKLearnEstimator\n",
    "# SKLearnEstimator is derived from BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class RandomForestRegressor(SKLearnEstimator):\n",
    "    def __init__(self, task=\"binary\", **config):\n",
    "        super().__init__(task, **config)\n",
    "\n",
    "        if task in CLASSIFICATION:\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "            self.estimator_class =RandomForestClassifier \n",
    "        else:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "            self.estimator_class =RandomForestRegressor\n",
    "\n",
    "    @classmethod\n",
    "    def search_space(cls, data_size, task):\n",
    "        space = {\n",
    "            \"n_estimators\": {\n",
    "                \"domain\": {\"domain\": tune.loguniform(lower=100, upper=20000)},\n",
    "                \"low_cost_init_value\": 100,\n",
    "            },\n",
    "            \"max_features\": {\n",
    "                \"domain\": tune.loguniform(lower=0.1, upper=1),\n",
    "                \"low_cost_init_value\": 0.1,\n",
    "            },\n",
    "            \"max_leaf_nodes\": {\n",
    "                \"domain\": tune.lograndint(lower=100, upper=2500),\n",
    "                \"low_cost_init_value\": 100,},\n",
    "            \"min_samples_split\": {\"domain\": tune.lograndint(lower=1, upper=20),\n",
    "            \"init_value\": 20,},\n",
    "            \n",
    "            \"min_samples_leaf\": {\n",
    "                \"domain\": tune.lograndint(lower=1, upper=20),\n",
    "                \"init_value\": 20,\n",
    "            },\n",
    "        }\n",
    "        return space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-equity",
   "metadata": {},
   "source": [
    "from flaml import AutoML\n",
    "from flaml import tune\n",
    "automl = AutoML()\n",
    "automl.add_learner(\"rfr\",RandomForestRegressor )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-token",
   "metadata": {},
   "source": [
    "automl_settings = {\n",
    "    \"time_budget\": 1000,  # in seconds\n",
    "    \"metric\": 'rmse',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"california.log\",\n",
    "}\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train.drop(['Lab1'], axis=1), y_train=y_train,**automl_settings,estimator_list=[\"rfr\"])\n",
    "pred=automl.predict(X_test.drop(['Lab1'], axis=1))\n",
    "R2=round(sm.r2_score(y_test, pred), 2)\n",
    "r=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "RMSE=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y),2)\n",
    "print(r,R2,RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-cabinet",
   "metadata": {},
   "source": [
    "#  NO2 DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=NO2_Data[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour','Ref_O3']]#'Ref_O3'\n",
    "y=NO2_Data['Ref']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#len(X_test)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-threshold",
   "metadata": {},
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Initialize an AutoML instance\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 200,  # in seconds\n",
    "    \"metric\": 'rmse',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"california.log\",\n",
    "}\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train.drop(['Lab1'], axis=1), y_train=y_train,**automl_settings,estimator_list=[\"xgboost\"])\n",
    "pred=automl.predict(X_test.drop(['Lab1'], axis=1))\n",
    "R2=round(sm.r2_score(y_test, pred), 2)\n",
    "r=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "RMSE=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y),2)\n",
    "print(r,R2,RMSE)\n",
    "# Predict\n",
    "#print(automl.predict(X_test))\n",
    "# Print the best model\n",
    "print(automl.model.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-powder",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel':  ['rbf', 'poly', 'sigmoid']} \n",
    "  \n",
    "grid = GridSearchCV(SVR(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train.drop(['Lab1'], axis=1), y_train)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-winning",
   "metadata": {},
   "source": [
    "# O3 DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "#,'Ref_CO','Ref_NO2','Ref_SO2'\n",
    "X=O3_Data[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]#,'Ref_NO2'\n",
    "y=O3_Data['Ref']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-freight",
   "metadata": {},
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Initialize an AutoML instance\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 200,  # in seconds\n",
    "    \"metric\": 'rmse',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"california.log\",\n",
    "}\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train.drop(['Lab1'], axis=1), y_train=y_train,**automl_settings,estimator_list=[\"xgboost\"])\n",
    "pred=automl.predict(X_test.drop(['Lab1'], axis=1))\n",
    "R2=round(sm.r2_score(y_test, pred), 2)\n",
    "r=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "RMSE=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y),2)\n",
    "print(r,R2,RMSE)\n",
    "# Predict\n",
    "#print(automl.predict(X_test))\n",
    "# Print the best model\n",
    "print(automl.model.estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel':  ['rbf', 'poly', 'sigmoid']} \n",
    "  \n",
    "grid = GridSearchCV(SVR(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train.drop(['Lab1'], axis=1), y_train)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-romance",
   "metadata": {},
   "source": [
    "# SO2 DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Ref=pd.read_csv('Ref.csv')\n",
    "Ref[\"CO\"] = 1000 * Ref[\"CO\"]\n",
    "Ref['Date'] = pd.to_datetime(Ref['Date_Time'])\n",
    "Ref=Ref.set_index('Date')\n",
    "Ref.drop('Date_Time',axis = 1, inplace = True)\n",
    "Ref=Ref.resample('5min').mean()\n",
    "Ref=Ref[76463:137376]\n",
    "Ref_CO=Ref['CO'].to_list()\n",
    "Ref_NO2=Ref['NO2'].to_list()\n",
    "Ref_SO2=Ref['SO2'].to_list()\n",
    "Ref_O3=Ref['O3'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('Conc_SO2.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab2','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_SO2=data\n",
    "Data_so2=data\n",
    "#signal=np.array(WE)-np.array(AE)\n",
    "#Data_SO2['Net Signal']=signal\n",
    "Data_SO2['Month']=Data_SO2.index.month\n",
    "Data_SO2['Day_of_week']=Data_SO2.index.dayofweek\n",
    "Data_SO2['Day']=Data_SO2.index.day\n",
    "Data_SO2['Hour']=Data_SO2.index.hour\n",
    "SO2_Data=Data_SO2\n",
    "SO2_Data=SO2_Data.resample('5min').mean()\n",
    "SO2_Data=SO2_Data[(SO2_Data[SO2_Data.columns] >= 0).all(axis=1)]\n",
    "SO2_Data=SO2_Data.dropna() \n",
    "data = pd.read_csv('SO2.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_SO2=data\n",
    "Data_SO2['Ref']=Ref_SO2\n",
    "WE=Data_SO2['WE'].to_list()\n",
    "AE=Data_SO2['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_SO2['Lab2']=Data_so2['Lab2'].to_list()\n",
    "Data_SO2['Net Signal']=signal\n",
    "Data_SO2['Month']=Data_SO2.index.month\n",
    "Data_SO2['Day_of_week']=Data_SO2.index.dayofweek\n",
    "Data_SO2['Day']=Data_SO2.index.day\n",
    "Data_SO2['Hour']=Data_SO2.index.hour\n",
    "SO2_Data=Data_SO2\n",
    "SO2_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "SO2_Data=SO2_Data[(SO2_Data[SO2_Data.columns] >= 0).all(axis=1)]\n",
    "SO2_Data=SO2_Data.dropna()\n",
    "SO2_Data=SO2_Data.resample('h').mean()\n",
    "SO2_Data=SO2_Data.dropna()\n",
    "SO2_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "#'Ref_CO','Ref_NO2','Ref_O3',\n",
    "X=SO2_Data[['Net Signal','Lab2','Temp','RH','Month','Day_of_week','Hour']]\n",
    "y=SO2_Data['Ref']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-ending",
   "metadata": {},
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Initialize an AutoML instance\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 200,  # in seconds\n",
    "    \"metric\": 'rmse',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"california.log\",\n",
    "}\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train.drop(['Lab2'], axis=1), y_train=y_train,**automl_settings,estimator_list=[\"xgboost\"])\n",
    "pred=automl.predict(X_test.drop(['Lab2'], axis=1))\n",
    "R2=round(sm.r2_score(y_test, pred), 2)\n",
    "r=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "RMSE=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y),2)\n",
    "print(r,R2,RMSE)\n",
    "# Predict\n",
    "#print(automl.predict(X_test))\n",
    "# Print the best model\n",
    "print(automl.model.estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-carolina",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel':  ['rbf', 'poly', 'sigmoid']} \n",
    "  \n",
    "grid = GridSearchCV(SVR(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train.drop(['Lab2'], axis=1), y_train)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
