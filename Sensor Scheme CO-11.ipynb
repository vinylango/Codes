{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "correct-saskatchewan",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exact-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Ref=pd.read_csv('Ref.csv')\n",
    "Ref[\"CO\"] = 1000 * Ref[\"CO\"]\n",
    "Ref['Date'] = pd.to_datetime(Ref['Date_Time'])\n",
    "Ref=Ref.set_index('Date')\n",
    "Ref.drop('Date_Time',axis = 1, inplace = True)\n",
    "Ref=Ref.resample('5min').mean()\n",
    "Ref=Ref[76463:137376]\n",
    "Ref_CO=Ref['CO'].to_list()\n",
    "Ref_NO2=Ref['NO2'].to_list()\n",
    "Ref_SO2=Ref['SO2'].to_list()\n",
    "Ref_O3=Ref['O3'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excellent-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-subscriber",
   "metadata": {},
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('CO.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO['Ref']=Ref_CO\n",
    "WE=Data_CO['WE'].to_list()\n",
    "AE=Data_CO['AE'].to_list()\n",
    "\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "data = pd.read_csv('Conc_CO.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data.resample('5min').mean()\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna() \n",
    "CO_Data.shape\n",
    "CO_Data=CO_Data.resample('h').mean()\n",
    "CO_Data=CO_Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "common-tension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44410, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('CO.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO['Ref']=Ref_CO\n",
    "index_names = Data_CO[ (Data_CO['WE'] >1000)].index\n",
    "Data_CO.drop(index_names, inplace = True)\n",
    "WE=Data_CO['WE'].to_list()\n",
    "AE=Data_CO['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "data = pd.read_csv('Conc_CO.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO.drop(index_names, inplace = True)\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "#CO_Data=CO_Data.sample(frac=1)\n",
    "#CO_Data=CO_Data.resample('h').mean()\n",
    "CO_Data=CO_Data.dropna()\n",
    "#CO_Data= (CO_Data-CO_Data.min())/(CO_Data.max()-CO_Data.min())\n",
    "CO_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contained-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-03 15:50:00</th>\n",
       "      <td>555.780140</td>\n",
       "      <td>29.441429</td>\n",
       "      <td>52.018571</td>\n",
       "      <td>399.25210</td>\n",
       "      <td>168.141429</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03 15:55:00</th>\n",
       "      <td>514.393544</td>\n",
       "      <td>29.401071</td>\n",
       "      <td>52.805119</td>\n",
       "      <td>284.54245</td>\n",
       "      <td>136.740190</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03 16:00:00</th>\n",
       "      <td>592.411938</td>\n",
       "      <td>29.211333</td>\n",
       "      <td>53.102667</td>\n",
       "      <td>261.28890</td>\n",
       "      <td>137.737333</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:45:00</th>\n",
       "      <td>1355.690593</td>\n",
       "      <td>32.782750</td>\n",
       "      <td>36.151417</td>\n",
       "      <td>198.86920</td>\n",
       "      <td>314.136500</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:50:00</th>\n",
       "      <td>189.562971</td>\n",
       "      <td>32.399528</td>\n",
       "      <td>37.143389</td>\n",
       "      <td>157.33180</td>\n",
       "      <td>8.279061</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                            \n",
       "2019-10-03 15:50:00   555.780140  29.441429  52.018571  399.25210  168.141429   \n",
       "2019-10-03 15:55:00   514.393544  29.401071  52.805119  284.54245  136.740190   \n",
       "2019-10-03 16:00:00   592.411938  29.211333  53.102667  261.28890  137.737333   \n",
       "2019-10-07 10:45:00  1355.690593  32.782750  36.151417  198.86920  314.136500   \n",
       "2019-10-07 10:50:00   189.562971  32.399528  37.143389  157.33180    8.279061   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour  \n",
       "Date                                                \n",
       "2019-10-03 15:50:00     10            3    3    15  \n",
       "2019-10-03 15:55:00     10            3    3    15  \n",
       "2019-10-03 16:00:00     10            3    3    16  \n",
       "2019-10-07 10:45:00     10            0    7    10  \n",
       "2019-10-07 10:50:00     10            0    7    10  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_Data1=CO_Data[3:]\n",
    "CO_Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "express-indication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60913"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('O3.txt', header = None,low_memory=False)\n",
    "data.columns=['AE','WE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "Data_O3['Ref']=Ref_O3\n",
    "WE=Data_O3['WE'].to_list()\n",
    "AE=Data_O3['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna()\n",
    "data = pd.read_csv('Conc_O3.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna() \n",
    "O3_Data=O3_Data.resample('5min').mean()\n",
    "O3_Data=O3_Data.dropna()\n",
    "#O3_Data= (O3_Data-CO_Data.min())/(O3_Data.max()-O3_Data.min())\n",
    "O3_Data.head()\n",
    "\n",
    "ref_O3=Data_O3['Ref'].to_list()\n",
    "len(ref_O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "maritime-effects",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_O3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-02 11:55:00</th>\n",
       "      <td>460.448301</td>\n",
       "      <td>26.378438</td>\n",
       "      <td>58.063437</td>\n",
       "      <td>15.230400</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>46.094860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:10:00</th>\n",
       "      <td>1364.583446</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>48.612609</td>\n",
       "      <td>6.665136</td>\n",
       "      <td>37.815652</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>55.810810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:15:00</th>\n",
       "      <td>224.159154</td>\n",
       "      <td>25.765087</td>\n",
       "      <td>48.441408</td>\n",
       "      <td>6.642805</td>\n",
       "      <td>12.275893</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>57.907075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:20:00</th>\n",
       "      <td>82.998996</td>\n",
       "      <td>26.120078</td>\n",
       "      <td>47.716553</td>\n",
       "      <td>2.844210</td>\n",
       "      <td>13.152720</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>58.880540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 15:45:00</th>\n",
       "      <td>566.301152</td>\n",
       "      <td>30.418466</td>\n",
       "      <td>50.153181</td>\n",
       "      <td>10.084125</td>\n",
       "      <td>9.323533</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>40.068225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                            \n",
       "2019-10-02 11:55:00   460.448301  26.378438  58.063437  15.230400    7.850000   \n",
       "2019-10-02 12:10:00  1364.583446  25.500000  48.612609   6.665136   37.815652   \n",
       "2019-10-02 12:15:00   224.159154  25.765087  48.441408   6.642805   12.275893   \n",
       "2019-10-02 12:20:00    82.998996  26.120078  47.716553   2.844210   13.152720   \n",
       "2019-10-02 15:45:00   566.301152  30.418466  50.153181  10.084125    9.323533   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour     Ref_O3  \n",
       "Date                                                           \n",
       "2019-10-02 11:55:00     10            2    2    11  46.094860  \n",
       "2019-10-02 12:10:00     10            2    2    12  55.810810  \n",
       "2019-10-02 12:15:00     10            2    2    12  57.907075  \n",
       "2019-10-02 12:20:00     10            2    2    12  58.880540  \n",
       "2019-10-02 15:45:00     10            2    2    15  40.068225  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('NO2.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_NO2=data\n",
    "Data_NO2['Ref']=Ref_NO2\n",
    "WE=Data_NO2['WE'].to_list()\n",
    "AE=Data_NO2['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_NO2['Net Signal']=signal\n",
    "Data_NO2['Month']=Data_NO2.index.month\n",
    "Data_NO2['Day_of_week']=Data_NO2.index.dayofweek\n",
    "Data_NO2['Day']=Data_NO2.index.day\n",
    "Data_NO2['Hour']=Data_NO2.index.hour\n",
    "NO2_Data=Data_NO2\n",
    "NO2_Data=NO2_Data[(NO2_Data[NO2_Data.columns] >= 0).all(axis=1)]\n",
    "NO2_Data=NO2_Data.dropna()\n",
    "data = pd.read_csv('Conc_NO2.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "subscript = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\") \n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_NO2=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_NO2['Net Signal']=signal\n",
    "Data_NO2['Month']=Data_NO2.index.month\n",
    "Data_NO2['Day_of_week']=Data_NO2.index.dayofweek\n",
    "Data_NO2['Day']=Data_NO2.index.day\n",
    "Data_NO2['Hour']=Data_NO2.index.hour\n",
    "Data_NO2['Ref_O3']=ref_O3\n",
    "NO2_Data=Data_NO2\n",
    "NO2_Data=NO2_Data[(NO2_Data[NO2_Data.columns] >= 0).all(axis=1)]\n",
    "NO2_Data=NO2_Data.dropna()\n",
    "#NO2_Data= (NO2_Data-NO2_Data.min())/(NO2_Data.max()-NO2_Data.min())\n",
    "NO2_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recognized-watch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-02 11:55:00</th>\n",
       "      <td>621.625704</td>\n",
       "      <td>26.378438</td>\n",
       "      <td>58.063437</td>\n",
       "      <td>46.094860</td>\n",
       "      <td>3.605625</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>15.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:10:00</th>\n",
       "      <td>1788.609900</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>48.612609</td>\n",
       "      <td>55.810810</td>\n",
       "      <td>3.528696</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6.665136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:15:00</th>\n",
       "      <td>287.254970</td>\n",
       "      <td>25.765087</td>\n",
       "      <td>48.441408</td>\n",
       "      <td>57.907075</td>\n",
       "      <td>17.781453</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6.642805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:20:00</th>\n",
       "      <td>99.598353</td>\n",
       "      <td>26.120078</td>\n",
       "      <td>47.716553</td>\n",
       "      <td>58.880540</td>\n",
       "      <td>20.285180</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2.844210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:50:00</th>\n",
       "      <td>105.723457</td>\n",
       "      <td>32.399528</td>\n",
       "      <td>37.143389</td>\n",
       "      <td>48.533490</td>\n",
       "      <td>11.862076</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4.344894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 23:35:00</th>\n",
       "      <td>60.523932</td>\n",
       "      <td>16.255137</td>\n",
       "      <td>83.573341</td>\n",
       "      <td>22.222497</td>\n",
       "      <td>7.913030</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>3.480461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 23:40:00</th>\n",
       "      <td>64.665593</td>\n",
       "      <td>16.162835</td>\n",
       "      <td>84.568452</td>\n",
       "      <td>28.185830</td>\n",
       "      <td>7.234113</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>3.467720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 23:45:00</th>\n",
       "      <td>58.131073</td>\n",
       "      <td>16.137381</td>\n",
       "      <td>84.388139</td>\n",
       "      <td>29.609723</td>\n",
       "      <td>8.246587</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>3.357663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 23:50:00</th>\n",
       "      <td>59.385848</td>\n",
       "      <td>16.114102</td>\n",
       "      <td>84.249935</td>\n",
       "      <td>19.007920</td>\n",
       "      <td>8.179632</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>3.306978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30 23:55:00</th>\n",
       "      <td>51.442543</td>\n",
       "      <td>16.114863</td>\n",
       "      <td>84.416501</td>\n",
       "      <td>16.022503</td>\n",
       "      <td>10.341400</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>3.474913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36821 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                            \n",
       "2019-10-02 11:55:00   621.625704  26.378438  58.063437  46.094860    3.605625   \n",
       "2019-10-02 12:10:00  1788.609900  25.500000  48.612609  55.810810    3.528696   \n",
       "2019-10-02 12:15:00   287.254970  25.765087  48.441408  57.907075   17.781453   \n",
       "2019-10-02 12:20:00    99.598353  26.120078  47.716553  58.880540   20.285180   \n",
       "2019-10-07 10:50:00   105.723457  32.399528  37.143389  48.533490   11.862076   \n",
       "...                          ...        ...        ...        ...         ...   \n",
       "2020-04-30 23:35:00    60.523932  16.255137  83.573341  22.222497    7.913030   \n",
       "2020-04-30 23:40:00    64.665593  16.162835  84.568452  28.185830    7.234113   \n",
       "2020-04-30 23:45:00    58.131073  16.137381  84.388139  29.609723    8.246587   \n",
       "2020-04-30 23:50:00    59.385848  16.114102  84.249935  19.007920    8.179632   \n",
       "2020-04-30 23:55:00    51.442543  16.114863  84.416501  16.022503   10.341400   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour    Ref_NO2  \n",
       "Date                                                           \n",
       "2019-10-02 11:55:00     10            2    2    11  15.230400  \n",
       "2019-10-02 12:10:00     10            2    2    12   6.665136  \n",
       "2019-10-02 12:15:00     10            2    2    12   6.642805  \n",
       "2019-10-02 12:20:00     10            2    2    12   2.844210  \n",
       "2019-10-07 10:50:00     10            0    7    10   4.344894  \n",
       "...                    ...          ...  ...   ...        ...  \n",
       "2020-04-30 23:35:00      4            3   30    23   3.480461  \n",
       "2020-04-30 23:40:00      4            3   30    23   3.467720  \n",
       "2020-04-30 23:45:00      4            3   30    23   3.357663  \n",
       "2020-04-30 23:50:00      4            3   30    23   3.306978  \n",
       "2020-04-30 23:55:00      4            3   30    23   3.474913  \n",
       "\n",
       "[36821 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "Ref_O3=10*np.array(Ref_O3)\n",
    "data = pd.read_csv('O3.txt', header = None,low_memory=False)\n",
    "data.columns=['AE','WE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "Data_O3['Ref']=Ref_O3\n",
    "WE=Data_O3['WE'].to_list()\n",
    "AE=Data_O3['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna()\n",
    "data = pd.read_csv('Conc_O3.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "ref_NO2=Data_NO2['Ref'].to_list()\n",
    "Data_O3['Ref_NO2']=ref_NO2\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "#O3_Data= (O3_Data-O3_Data.min())/(O3_Data.max()-O3_Data.min())\n",
    "O3_Data=O3_Data.dropna()\n",
    "\n",
    "O3_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fluid-ebony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42633, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "CO_Data=CO_Data[(np.abs(stats.zscore(CO_Data)) < 3).all(axis=1)]\n",
    "CO_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sweet-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(pred, y_test):\n",
    "    pred=np.array(pred)\n",
    "    y_test=np.array(y_test)\n",
    "    mae=sum(abs(pred-y_test))/len(y_test)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-doctor",
   "metadata": {},
   "source": [
    "#  New Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-witch",
   "metadata": {},
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('CO_2.txt', header = None,low_memory=False)\n",
    "data.columns=['C1','C2','C3','C4','Temp','RH','Ref','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "\n",
    "data=data[['C1','Temp','RH','Ref']]\n",
    "Data_CO=data\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "#Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "CO_Data=CO_Data[:3446]\n",
    "CO_Data=CO_Data.resample('h').mean()\n",
    "CO_Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-trash",
   "metadata": {},
   "source": [
    "Temp1=CO_Data['Temp'].to_list()\n",
    "RH1=CO_Data['RH'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-parameter",
   "metadata": {},
   "source": [
    "CO_Data1.tail()\n",
    "CO_Data1=CO_Data1.resample('h').mean()\n",
    "CO_Data1['Temp1']=Temp1\n",
    "CO_Data1['RH1']=RH1\n",
    "CO_Data1=CO_Data1.dropna()\n",
    "Temp=CO_Data1['Temp'].to_list()\n",
    "RH=CO_Data1['RH'].to_list()\n",
    "Temp2=CO_Data1['Temp1'].to_list()\n",
    "RH2=CO_Data1['RH1'].to_list()\n",
    "\n",
    "ind=[i for i in range(len(Temp))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-grave",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ind, RH, color='green')\n",
    "plt.plot(ind, RH2, color='red')\n",
    "plt.ylabel('RH')\n",
    "plt.xlabel('Hours')\n",
    "plt.legend(['Sensor','Station'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-april",
   "metadata": {},
   "source": [
    "NO2_Data1=NO2_Data.resample('h').mean()\n",
    "NO2_Data1=NO2_Data1[28:]\n",
    "NO2_Data1['Temp1']=Temp1\n",
    "NO2_Data1['RH1']=RH1\n",
    "NO2_Data1=NO2_Data1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-least",
   "metadata": {},
   "source": [
    "O3_Data1=O3_Data.resample('h').mean()\n",
    "O3_Data1=O3_Data1[28:]\n",
    "O3_Data1['Temp1']=Temp1\n",
    "O3_Data1['RH1']=RH1\n",
    "O3_Data1=O3_Data1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-paper",
   "metadata": {},
   "source": [
    "CO_Data=CO_Data1\n",
    "NO2_Data=NO2_Data1\n",
    "O3_Data=O3_Data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vital-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    mbe=np.mean(true-pred)\n",
    "    return mbe\n",
    "def CRMSE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    crmse=np.sqrt(np.mean(((true-np.mean(true))-(pred-np.mean(pred)))**2))\n",
    "    if np.std(pred)>np.std(true):\n",
    "        crmse=crmse\n",
    "    else:\n",
    "        crmse=-crmse\n",
    "    return crmse\n",
    "\n",
    "def sMAE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    smae=np.mean(abs(true-pred)/((abs(true)+abs(pred))/2))\n",
    "    return smae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "focal-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sMAE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    mae=sum(abs(pred-true))/len(pred)\n",
    "    nmae=mae/np.mean(true)\n",
    "    return 1-nmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "limiting-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "p = np.array([ 0.9])\n",
    "df=np.array(range(1,10000))\n",
    "chi = [0]+chi2.isf(p, df).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tight-reputation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.66023432606575"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "#find T critical value\n",
    "scipy.stats.t.ppf(q=.95,df=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "second-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred,true):\n",
    "    pred=list(pred)\n",
    "    true=list(true)\n",
    "    for i in range(len(true)):\n",
    "        if true[i]==0:\n",
    "            true.pop(i)\n",
    "            pred.pop(i)\n",
    "    pred=np.array(pred)\n",
    "    true=np.array(true)\n",
    "    #d=((pred-true)/((true+pred)/2))*100\n",
    "    d=((pred-true)/np.mean(true))*100\n",
    "    n=len(pred)\n",
    "    A=np.sqrt((n-1)/chi[n-1])\n",
    "    cv=np.sqrt(((n*sum(abs(d)**2)-(sum(abs(d)))**2)/(2*n*(n-1))))*A\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "checked-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(pred,true):\n",
    "    pred=list(pred)\n",
    "    true=list(true)\n",
    "    for i in range(len(true)):\n",
    "        if true[i]==0:\n",
    "            true.pop(i)\n",
    "            pred.pop(i)\n",
    "    pred=np.array(pred)\n",
    "    true=np.array(true)\n",
    "    #d=((pred-true)/((true+pred)/2))*100\n",
    "    d=((pred-true)/np.mean(true))*100\n",
    "    n=len(pred)\n",
    "    AB=sum(abs(d))/n\n",
    "    #AS=np.sqrt(((n*sum(abs(d)**2)-(sum(abs(d)))**2)/(n*(n-1))))\n",
    "    AS=np.sqrt(sum((d-AB)**2)/(n-1))\n",
    "    t=scipy.stats.t.ppf(q=.95,df=n-1)\n",
    "    bias=AB+(t*AS/np.sqrt(n))\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fixed-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOA(pred,true):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    frac=sum(abs(true-pred))/sum((abs(pred-np.mean(true))+abs(true-np.mean(true))))\n",
    "    d=1-frac\n",
    "    return d  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "challenging-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REF(pred,y_test,alpha):\n",
    "    import random\n",
    "    cal=np.array(pred)\n",
    "    ref=np.array(y_test.to_list())\n",
    "    ref_mean=np.mean(ref)\n",
    "    cal_mean=np.mean(cal)\n",
    "    prec=np.array([20 for i in range(len(ref))])\n",
    "    u=np.maximum(prec,0.001*ref)\n",
    "    #cal=np.log(cal)\n",
    "    #ref=np.log(ref)\n",
    "    sx_s=(1/len(ref))*sum((ref-ref_mean)**2)\n",
    "    sy_s=(1/len(cal))*sum((cal-cal_mean)**2)\n",
    "    sxy=(1/len(cal))*sum((cal-cal_mean)*(ref-ref_mean))\n",
    "    beta_1=((sy_s-alpha*sx_s)+np.sqrt((sy_s-sx_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    beta_0=cal_mean-beta_1*ref_mean\n",
    "    RSS=sum((cal-beta_0-beta_1*ref)**2-(beta_1**2+alpha)*u**2)\n",
    "    du_s=RSS/(len(cal)-2)\n",
    "    Beta_1=((sy_s-alpha*sx_s-du_s)+np.sqrt((sy_s-alpha*sx_s-du_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    Beta_0=cal_mean-Beta_1*ref_mean\n",
    "    P1=(RSS/(len(cal)-2))\n",
    "    P2=(Beta_1**2+alpha)*u**2+(-2*Beta_1**2+2*Beta_1-1)*u**2\n",
    "    P3=(Beta_0+(Beta_1-1)*ref)**2\n",
    "    P=[]\n",
    "    for i in range(len(P3)):\n",
    "        P.append(P1+P2[i]+P3[i])\n",
    "    for i in range(len(P)):\n",
    "        if P[i]<0:\n",
    "            P[i]=random.randint(1,100)\n",
    "    u_cal=(2*np.sqrt(np.array(P))/cal)*100\n",
    "    #u_cal=((2*np.sqrt((RSS/(len(cal)-2))+(1-(beta_1-1)**2)*(0.08*ref)**2+(Beta_0+(Beta_1-1)*ref)**2))/cal)*100\n",
    "    return u_cal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "portuguese-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REF2(pred,y_test,alpha,LV):\n",
    "    import random\n",
    "    cal=np.array(pred)\n",
    "    ref=np.array(y_test.to_list())\n",
    "    ref_mean=np.mean(ref)\n",
    "    cal_mean=np.mean(cal)\n",
    "    prec=np.array([20 for i in range(len(ref))])\n",
    "    u=0.001*ref\n",
    "    #cal=np.log(cal)\n",
    "    #ref=np.log(ref)\n",
    "    sx_s=(1/len(ref))*sum((ref-ref_mean)**2)\n",
    "    sy_s=(1/len(cal))*sum((cal-cal_mean)**2)\n",
    "    sxy=(1/len(cal))*sum((cal-cal_mean)*(ref-ref_mean))\n",
    "    #beta_1=((sy_s-sx_s)+np.sqrt((sy_s-sx_s)**2+4*sxy**2))/(2*sxy)\n",
    "    beta_1=((sy_s-alpha*sx_s)+np.sqrt((sy_s-sx_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    beta_0=cal_mean-beta_1*ref_mean\n",
    "    RSS=sum((cal-beta_0-beta_1*ref)**2-(beta_1**2+alpha)*(0.001*LV)**2)\n",
    "    du_s=RSS/(len(cal)-2)\n",
    "    #Beta_1=((sy_s-sx_s-du_s)+np.sqrt((sy_s-sx_s-du_s)**2+4*sxy**2))/(2*sxy)\n",
    "    Beta_1=((sy_s-alpha*sx_s-du_s)+np.sqrt((sy_s-alpha*sx_s-du_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    Beta_0=cal_mean-Beta_1*ref_mean\n",
    "    P1=(RSS/(len(cal)-2))\n",
    "    P2=(Beta_1**2+alpha)*(0.001*LV)**2+(-2*Beta_1**2+2*Beta_1-1)*(0.001*LV)**2\n",
    "    P3=(Beta_0+(Beta_1-1)*LV)**2\n",
    "    P=P1+P2+P3\n",
    "    if P<0:\n",
    "        P=random.randint(1,100)\n",
    "    u_cal=(2*np.sqrt(P)/(Beta_0+Beta_1*LV))*100\n",
    "    #u_cal=((2*np.sqrt((RSS/(len(cal)-2))+(1-(beta_1-1)**2)*0.1+(Beta_0+(Beta_1-1)*ref)**2))\n",
    "    #/cal)*100\n",
    "    return u_cal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "comic-plenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "completed-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "attempted-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "attached-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rational-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "featured-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "elegant-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#O3_Data=O3_Data.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "assisted-abuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-03 15:50:00</th>\n",
       "      <td>555.780140</td>\n",
       "      <td>29.441429</td>\n",
       "      <td>52.018571</td>\n",
       "      <td>399.25210</td>\n",
       "      <td>168.141429</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03 15:55:00</th>\n",
       "      <td>514.393544</td>\n",
       "      <td>29.401071</td>\n",
       "      <td>52.805119</td>\n",
       "      <td>284.54245</td>\n",
       "      <td>136.740190</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-03 16:00:00</th>\n",
       "      <td>592.411938</td>\n",
       "      <td>29.211333</td>\n",
       "      <td>53.102667</td>\n",
       "      <td>261.28890</td>\n",
       "      <td>137.737333</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:50:00</th>\n",
       "      <td>189.562971</td>\n",
       "      <td>32.399528</td>\n",
       "      <td>37.143389</td>\n",
       "      <td>157.33180</td>\n",
       "      <td>8.279061</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:55:00</th>\n",
       "      <td>286.614343</td>\n",
       "      <td>32.289000</td>\n",
       "      <td>37.378125</td>\n",
       "      <td>137.17690</td>\n",
       "      <td>37.425916</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                           \n",
       "2019-10-03 15:50:00  555.780140  29.441429  52.018571  399.25210  168.141429   \n",
       "2019-10-03 15:55:00  514.393544  29.401071  52.805119  284.54245  136.740190   \n",
       "2019-10-03 16:00:00  592.411938  29.211333  53.102667  261.28890  137.737333   \n",
       "2019-10-07 10:50:00  189.562971  32.399528  37.143389  157.33180    8.279061   \n",
       "2019-10-07 10:55:00  286.614343  32.289000  37.378125  137.17690   37.425916   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour  \n",
       "Date                                                \n",
       "2019-10-03 15:50:00     10            3    3    15  \n",
       "2019-10-03 15:55:00     10            3    3    15  \n",
       "2019-10-03 16:00:00     10            3    3    16  \n",
       "2019-10-07 10:50:00     10            0    7    10  \n",
       "2019-10-07 10:55:00     10            0    7    10  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=[x for _, x in CO_Data.groupby('Month')]\n",
    "data_oct=df1[4]\n",
    "#data_oct=data_oct.sample(frac=1)\n",
    "data_nov=df1[5]\n",
    "#data_nov=data_nov.sample(frac=1)\n",
    "data_dec=df1[6]\n",
    "#data_dec=data_dec.sample(frac=1)\n",
    "data_jan=df1[0]\n",
    "#data_jan=data_jan.sample(frac=1)\n",
    "data_feb=df1[1]\n",
    "#data_feb=data_feb.sample(frac=1)\n",
    "data_mar=df1[2]\n",
    "#data_mar=data_mar.sample(frac=1)\n",
    "data_apr=df1[3]\n",
    "#data_apr=data_apr.sample(frac=1)\n",
    "data=[data_oct,data_nov,data_dec,data_jan,data_feb,data_mar]\n",
    "data_oct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "viral-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Oct=data_oct.resample('60min').mean()\n",
    "data_Oct=data_Oct.dropna()\n",
    "data_Nov=data_nov.resample('60min').mean()\n",
    "data_Nov=data_Nov.dropna()\n",
    "data_Dec=data_dec.resample('60min').mean()\n",
    "data_Dec=data_Dec.dropna()\n",
    "data_Jan=data_jan.resample('60min').mean()\n",
    "data_Jan=data_Jan.dropna()\n",
    "data_Feb=data_feb.resample('60min').mean()\n",
    "data_Feb=data_Feb.dropna()\n",
    "data_Mar=data_mar.resample('60min').mean()\n",
    "data_Mar=data_Mar.dropna()\n",
    "data_Apr=data_apr.resample('60min').mean()\n",
    "data_Apr=data_Apr.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-video",
   "metadata": {},
   "source": [
    "Temp=data_Jan['RH'].to_list()\n",
    "len(Temp)\n",
    "\n",
    "ind=[i for i in range(len(Temp))]\n",
    "plt.plot(ind,Temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "experienced-acting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:00:00</th>\n",
       "      <td>121.424636</td>\n",
       "      <td>15.130260</td>\n",
       "      <td>53.920912</td>\n",
       "      <td>27.811665</td>\n",
       "      <td>3.875583</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>18.995165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:05:00</th>\n",
       "      <td>97.965370</td>\n",
       "      <td>15.088266</td>\n",
       "      <td>54.620724</td>\n",
       "      <td>28.450277</td>\n",
       "      <td>4.498775</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>21.162180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:10:00</th>\n",
       "      <td>89.752931</td>\n",
       "      <td>13.271542</td>\n",
       "      <td>61.067292</td>\n",
       "      <td>26.522085</td>\n",
       "      <td>1.582542</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>17.001195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:15:00</th>\n",
       "      <td>97.043600</td>\n",
       "      <td>13.187583</td>\n",
       "      <td>61.158583</td>\n",
       "      <td>24.511943</td>\n",
       "      <td>1.419583</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>23.019373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:20:00</th>\n",
       "      <td>95.767470</td>\n",
       "      <td>13.104631</td>\n",
       "      <td>61.276556</td>\n",
       "      <td>25.317920</td>\n",
       "      <td>1.699226</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>21.306785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                           \n",
       "2020-02-14 15:00:00  121.424636  15.130260  53.920912  27.811665    3.875583   \n",
       "2020-02-14 15:05:00   97.965370  15.088266  54.620724  28.450277    4.498775   \n",
       "2020-02-14 16:10:00   89.752931  13.271542  61.067292  26.522085    1.582542   \n",
       "2020-02-14 16:15:00   97.043600  13.187583  61.158583  24.511943    1.419583   \n",
       "2020-02-14 16:20:00   95.767470  13.104631  61.276556  25.317920    1.699226   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour    Ref_NO2  \n",
       "Date                                                           \n",
       "2020-02-14 15:00:00      2            4   14    15  18.995165  \n",
       "2020-02-14 15:05:00      2            4   14    15  21.162180  \n",
       "2020-02-14 16:10:00      2            4   14    16  17.001195  \n",
       "2020-02-14 16:15:00      2            4   14    16  23.019373  \n",
       "2020-02-14 16:20:00      2            4   14    16  21.306785  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=[x for _, x in O3_Data.groupby('Month')]\n",
    "data_oct2=df2[4]\n",
    "#data_oct2=data_oct2.sample(frac=1)\n",
    "data_nov2=df2[5]\n",
    "#data_nov2=data_nov2.sample(frac=1)\n",
    "data_dec2=df2[6]\n",
    "#data_dec2=data_dec2.sample(frac=1)\n",
    "data_jan2=df2[0]\n",
    "#data_jan2=data_jan2.sample(frac=1)\n",
    "data_feb2=df2[1]\n",
    "#data_feb2=data_feb2.sample(frac=1)\n",
    "data_mar2=df2[2]\n",
    "#data_mar2=data_mar2.sample(frac=1)\n",
    "data_apr2=df2[3]\n",
    "#data_apr2=data_apr2.sample(frac=1)\n",
    "data=[data_oct2,data_nov2,data_dec2,data_jan2,data_feb2,data_mar2]\n",
    "data_feb2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "natural-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Oct2=data_oct2.resample('60min').mean()\n",
    "data_Oct2=data_Oct2.dropna()\n",
    "data_Nov2=data_nov2.resample('60min').mean()\n",
    "data_Nov2=data_Nov2.dropna()\n",
    "data_Dec2=data_dec2.resample('60min').mean()\n",
    "data_Dec2=data_Dec2.dropna()\n",
    "data_Jan2=data_jan2.resample('60min').mean()\n",
    "data_Jan2=data_Jan2.dropna()\n",
    "data_Feb2=data_feb2.resample('60min').mean()\n",
    "data_Feb2=data_Feb2.dropna()\n",
    "data_Mar2=data_mar2.resample('60min').mean()\n",
    "data_Mar2=data_Mar2.dropna()\n",
    "data_Apr2=data_apr2.resample('60min').mean()\n",
    "data_Apr2=data_Apr2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "controlled-removal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:00:00</th>\n",
       "      <td>121.424636</td>\n",
       "      <td>15.130260</td>\n",
       "      <td>53.920912</td>\n",
       "      <td>27.811665</td>\n",
       "      <td>3.875583</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>18.995165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:05:00</th>\n",
       "      <td>97.965370</td>\n",
       "      <td>15.088266</td>\n",
       "      <td>54.620724</td>\n",
       "      <td>28.450277</td>\n",
       "      <td>4.498775</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>21.162180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:10:00</th>\n",
       "      <td>89.752931</td>\n",
       "      <td>13.271542</td>\n",
       "      <td>61.067292</td>\n",
       "      <td>26.522085</td>\n",
       "      <td>1.582542</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>17.001195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:15:00</th>\n",
       "      <td>97.043600</td>\n",
       "      <td>13.187583</td>\n",
       "      <td>61.158583</td>\n",
       "      <td>24.511943</td>\n",
       "      <td>1.419583</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>23.019373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:20:00</th>\n",
       "      <td>95.767470</td>\n",
       "      <td>13.104631</td>\n",
       "      <td>61.276556</td>\n",
       "      <td>25.317920</td>\n",
       "      <td>1.699226</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>21.306785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                           \n",
       "2020-02-14 15:00:00  121.424636  15.130260  53.920912  27.811665    3.875583   \n",
       "2020-02-14 15:05:00   97.965370  15.088266  54.620724  28.450277    4.498775   \n",
       "2020-02-14 16:10:00   89.752931  13.271542  61.067292  26.522085    1.582542   \n",
       "2020-02-14 16:15:00   97.043600  13.187583  61.158583  24.511943    1.419583   \n",
       "2020-02-14 16:20:00   95.767470  13.104631  61.276556  25.317920    1.699226   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour    Ref_NO2  \n",
       "Date                                                           \n",
       "2020-02-14 15:00:00      2            4   14    15  18.995165  \n",
       "2020-02-14 15:05:00      2            4   14    15  21.162180  \n",
       "2020-02-14 16:10:00      2            4   14    16  17.001195  \n",
       "2020-02-14 16:15:00      2            4   14    16  23.019373  \n",
       "2020-02-14 16:20:00      2            4   14    16  21.306785  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=[x for _, x in O3_Data.groupby('Month')]\n",
    "data_oct3=df3[4]\n",
    "#data_oct3=data_oct3.sample(frac=1)\n",
    "data_nov3=df3[5]\n",
    "#data_nov3=data_nov3.sample(frac=1)\n",
    "data_dec3=df3[6]\n",
    "#data_dec3=data_dec3.sample(frac=1)\n",
    "data_jan3=df3[0]\n",
    "#data_jan3=data_jan3.sample(frac=1)\n",
    "data_feb3=df3[1]\n",
    "#data_feb3=data_feb3.sample(frac=1)\n",
    "data_mar3=df3[2]\n",
    "#data_mar3=data_mar3.sample(frac=1)\n",
    "data_apr3=df3[3]\n",
    "#data_apr3=data_apr3.sample(frac=1)\n",
    "data=[data_oct3,data_nov3,data_dec3,data_jan3,data_feb3,data_mar3]\n",
    "data_feb3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "black-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Oct3=data_oct3.resample('60min').last()\n",
    "data_Oct3=data_Oct3.dropna()\n",
    "data_Nov3=data_nov3.resample('60min').last()\n",
    "data_Nov3=data_Nov3.dropna()\n",
    "data_Dec3=data_dec3.resample('60min').last()\n",
    "data_Dec3=data_Dec3.dropna()\n",
    "data_Jan3=data_jan3.resample('60min').last()\n",
    "data_Jan3=data_Jan3.dropna()\n",
    "data_Feb3=data_feb3.resample('60min').last()\n",
    "data_Feb3=data_Feb3.dropna()\n",
    "data_Mar3=data_mar3.resample('60min').last()\n",
    "data_Mar3=data_Mar3.dropna()\n",
    "data_Apr3=data_apr3.resample('60min').last()\n",
    "data_Apr3=data_Apr3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "color-mayor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-02 11:00:00</th>\n",
       "      <td>621.625704</td>\n",
       "      <td>26.378438</td>\n",
       "      <td>58.063437</td>\n",
       "      <td>46.094860</td>\n",
       "      <td>3.605625</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:00:00</th>\n",
       "      <td>99.598353</td>\n",
       "      <td>26.120078</td>\n",
       "      <td>47.716553</td>\n",
       "      <td>58.880540</td>\n",
       "      <td>20.285180</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.844210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:00:00</th>\n",
       "      <td>110.669169</td>\n",
       "      <td>32.289000</td>\n",
       "      <td>37.378125</td>\n",
       "      <td>45.984525</td>\n",
       "      <td>11.033542</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.166651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:00:00</th>\n",
       "      <td>145.934737</td>\n",
       "      <td>36.586708</td>\n",
       "      <td>32.431417</td>\n",
       "      <td>47.910815</td>\n",
       "      <td>12.648250</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.230621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 12:00:00</th>\n",
       "      <td>260.566848</td>\n",
       "      <td>34.527578</td>\n",
       "      <td>32.699732</td>\n",
       "      <td>49.135953</td>\n",
       "      <td>3.073537</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.435194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 19:00:00</th>\n",
       "      <td>107.920431</td>\n",
       "      <td>19.151852</td>\n",
       "      <td>72.105362</td>\n",
       "      <td>9.720208</td>\n",
       "      <td>3.490208</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.040065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 20:00:00</th>\n",
       "      <td>94.910725</td>\n",
       "      <td>18.313667</td>\n",
       "      <td>74.531250</td>\n",
       "      <td>3.432125</td>\n",
       "      <td>4.673810</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.918170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 21:00:00</th>\n",
       "      <td>80.634312</td>\n",
       "      <td>17.822078</td>\n",
       "      <td>76.417224</td>\n",
       "      <td>16.595835</td>\n",
       "      <td>5.533771</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.285320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 22:00:00</th>\n",
       "      <td>74.353514</td>\n",
       "      <td>17.293811</td>\n",
       "      <td>78.388130</td>\n",
       "      <td>2.777708</td>\n",
       "      <td>7.845655</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.464620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 23:00:00</th>\n",
       "      <td>69.429447</td>\n",
       "      <td>16.949621</td>\n",
       "      <td>79.744070</td>\n",
       "      <td>2.394000</td>\n",
       "      <td>9.790860</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.807595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                           \n",
       "2019-10-02 11:00:00  621.625704  26.378438  58.063437  46.094860    3.605625   \n",
       "2019-10-02 12:00:00   99.598353  26.120078  47.716553  58.880540   20.285180   \n",
       "2019-10-07 10:00:00  110.669169  32.289000  37.378125  45.984525   11.033542   \n",
       "2019-10-07 11:00:00  145.934737  36.586708  32.431417  47.910815   12.648250   \n",
       "2019-10-07 12:00:00  260.566848  34.527578  32.699732  49.135953    3.073537   \n",
       "...                         ...        ...        ...        ...         ...   \n",
       "2019-10-31 19:00:00  107.920431  19.151852  72.105362   9.720208    3.490208   \n",
       "2019-10-31 20:00:00   94.910725  18.313667  74.531250   3.432125    4.673810   \n",
       "2019-10-31 21:00:00   80.634312  17.822078  76.417224  16.595835    5.533771   \n",
       "2019-10-31 22:00:00   74.353514  17.293811  78.388130   2.777708    7.845655   \n",
       "2019-10-31 23:00:00   69.429447  16.949621  79.744070   2.394000    9.790860   \n",
       "\n",
       "                     Month  Day_of_week   Day  Hour    Ref_NO2  \n",
       "Date                                                            \n",
       "2019-10-02 11:00:00   10.0          2.0   2.0  11.0  15.230400  \n",
       "2019-10-02 12:00:00   10.0          2.0   2.0  12.0   2.844210  \n",
       "2019-10-07 10:00:00   10.0          0.0   7.0  10.0   4.166651  \n",
       "2019-10-07 11:00:00   10.0          0.0   7.0  11.0   8.230621  \n",
       "2019-10-07 12:00:00   10.0          0.0   7.0  12.0   5.435194  \n",
       "...                    ...          ...   ...   ...        ...  \n",
       "2019-10-31 19:00:00   10.0          3.0  31.0  19.0  29.040065  \n",
       "2019-10-31 20:00:00   10.0          3.0  31.0  20.0  35.918170  \n",
       "2019-10-31 21:00:00   10.0          3.0  31.0  21.0  20.285320  \n",
       "2019-10-31 22:00:00   10.0          3.0  31.0  22.0  29.464620  \n",
       "2019-10-31 23:00:00   10.0          3.0  31.0  23.0  25.807595  \n",
       "\n",
       "[490 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Oct3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "round-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "peaceful-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import  XGBRegressor\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "# create an xgboost regression model\n",
    "#n_estimators=10000, max_depth=5, eta=0.01, subsample=0.9,colsample_bytree=0.4,alpha=10\n",
    "model = XGBRegressor(n_estimators=10000, max_depth=5, eta=0.01, subsample=0.9, \n",
    "                     colsample_bytree=0.4,alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "front-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REF(pred,y_test,alpha):\n",
    "    import random\n",
    "    cal=np.array(pred)\n",
    "    ref=np.array(y_test.to_list())\n",
    "    for i in range(len(ref)):\n",
    "        if ref[i]==0:\n",
    "            ref[i]=np.mean(ref)\n",
    "    ref_mean=np.mean(ref)\n",
    "    cal_mean=np.mean(cal)\n",
    "    prec=np.array([20 for i in range(len(ref))])\n",
    "    u=np.maximum(prec,0.001*ref)\n",
    "    u=0.001*ref\n",
    "    #cal=np.log(cal)\n",
    "    #ref=np.log(ref)\n",
    "    sx_s=(1/len(ref))*sum((ref-ref_mean)**2)\n",
    "    sy_s=(1/len(cal))*sum((cal-cal_mean)**2)\n",
    "    sxy=(1/len(cal))*sum((cal-cal_mean)*(ref-ref_mean))\n",
    "    beta_1=((sy_s-alpha*sx_s)+np.sqrt((sy_s-sx_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    beta_0=cal_mean-beta_1*ref_mean\n",
    "    RSS=sum((cal-beta_0-beta_1*ref)**2-(beta_1**2+alpha)*u**2)\n",
    "    du_s=RSS/(len(cal)-2)\n",
    "    Beta_1=((sy_s-alpha*sx_s-du_s)+np.sqrt((sy_s-alpha*sx_s-du_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    Beta_0=cal_mean-Beta_1*ref_mean\n",
    "    P1=(RSS/(len(cal)-2))\n",
    "    #P2=(Beta_1**2+alpha)*u**2+(-2*Beta_1**2+2*Beta_1-1)*u**2\n",
    "    P2=-(u**2)\n",
    "    P3=(Beta_0+(Beta_1-1)*ref)**2\n",
    "    P=[]\n",
    "    for i in range(len(P3)):\n",
    "        P.append(P1+P2[i]+P3[i])\n",
    "    U1=[]\n",
    "    Ref=[]\n",
    "    for i in range(len(P)):\n",
    "        if P[i]>=0:\n",
    "            U1.append(P[i])\n",
    "            Ref.append(ref[i])\n",
    "    #for i in range(len(P)):\n",
    "        #if P[i]<0:\n",
    "           # P[i]=np.mean(P1)\n",
    "    u_cal=(2*np.sqrt(np.array(U1))/np.array(Ref))*100\n",
    "    U_cal=[]\n",
    "    for i in range(len(Ref)):\n",
    "        if Ref[i]==max(Ref):\n",
    "            U_cal.append(u_cal[i])\n",
    "    return U_cal[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-setup",
   "metadata": {},
   "source": [
    "import sklearn.metrics as sm\n",
    "from flaml import AutoML\n",
    "X=data_Oct[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour']]\n",
    "y=data_Oct['Ref']\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size = 0.2,shuffle=True)\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 50,  # in seconds\n",
    "    \"metric\": 'rmse',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"california.log\",\n",
    "}\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train.drop(['Lab1'], axis=1), y_train=y_train,\n",
    "           **automl_settings,estimator_list=[\"rf\"])#,estimator_list=[\"xgboost\"]\n",
    "pred=automl.predict(X_test.drop(['Lab1'], axis=1))\n",
    "R2=round(sm.r2_score(y_test, pred), 2)\n",
    "r=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "RMSE=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y),2)\n",
    "print(r,R2,RMSE)\n",
    "# Predict\n",
    "#print(automl.predict(X_test))\n",
    "# Print the best model\n",
    "print(automl.model.estimator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-indie",
   "metadata": {},
   "source": [
    "plt.plot(y_test,y_test, color='red')\n",
    "plt.scatter(pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-michael",
   "metadata": {},
   "source": [
    "# Mothly schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-interstate",
   "metadata": {},
   "source": [
    "# Oct 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-andrew",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "empirical-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "#from sklearn.metrics import mean_absolute_error as mae\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "  \n",
    " # create regressor object\n",
    "regressor1= RandomForestRegressor(max_features=0.38328515902344024, max_leaf_nodes=2566,\n",
    "                      n_estimators=911, n_jobs=-1)\n",
    "\n",
    "regressor2 = RandomForestRegressor(max_features=0.30888259687906783, max_leaf_nodes=1564,\n",
    "                      n_estimators=194, n_jobs=-1,min_samples_split= 2,min_samples_leaf= 1, \n",
    "                                  random_state = 0,max_depth=None,bootstrap=False)\n",
    "\n",
    "regressor3 = RandomForestRegressor(n_estimators = 500,min_samples_split= 2,min_samples_leaf= 1,max_features= 'sqrt', \n",
    "                                  random_state = 0,max_depth=None,bootstrap=False)\n",
    "# fit the regressor with x and y data\n",
    "\n",
    "#regressor= LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "forbidden-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler2=StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-capability",
   "metadata": {},
   "source": [
    "co_data=O3_Data\n",
    "co_data=co_data.resample('60min').mean()\n",
    "co_data=co_data.dropna()\n",
    "X=co_data[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=co_data['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "model=regressor.fit(X_train,y_train)\n",
    "pred=model.predict(X_test)\n",
    "rmse_r=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "rmse_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-collins",
   "metadata": {},
   "source": [
    "A=['Net Signal','Temp','RH','Month','Day_of_week','Hour']\n",
    "B='Ref'\n",
    "frame1=[data_Oct,data_Nov,data_Dec,data_Jan,data_Feb,data_Mar]#,data_feb\n",
    "Train1=pd.concat(frame1)\n",
    "train1=Train1.sample(frac=1)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(train1)\n",
    "mask = yhat != -1\n",
    "train1= train1[mask]\n",
    "test1=data_Apr\n",
    "yhat = lof.fit_predict(test1)\n",
    "mask = yhat != -1\n",
    "test1= test1[mask]\n",
    "\n",
    "frame2=[data_Oct,data_Nov,data_Dec,data_Jan,data_Feb,data_Apr]#,data_feb\n",
    "Train2=pd.concat(frame2)\n",
    "train2=Train2.sample(frac=1)\n",
    "yhat = lof.fit_predict(train2)\n",
    "mask = yhat != -1\n",
    "train2= train2[mask]\n",
    "test2=data_Mar\n",
    "yhat = lof.fit_predict(test2)\n",
    "mask = yhat != -1\n",
    "test2= test2[mask]\n",
    "\n",
    "frame3=[data_Oct,data_Nov,data_Dec,data_Jan,data_Mar,data_Apr]#,data_feb\n",
    "Train3=pd.concat(frame3)\n",
    "train3=Train3.sample(frac=1)\n",
    "yhat = lof.fit_predict(train3)\n",
    "mask = yhat != -1\n",
    "train3= train3[mask]\n",
    "test3=data_Feb\n",
    "yhat = lof.fit_predict(test3)\n",
    "mask = yhat != -1\n",
    "test3= test3[mask]\n",
    "\n",
    "frame4=[data_Oct,data_Nov,data_Dec,data_Feb,data_Mar,data_Apr]#,data_feb\n",
    "Train4=pd.concat(frame4)\n",
    "train4=Train4.sample(frac=1)\n",
    "yhat = lof.fit_predict(train4)\n",
    "mask = yhat != -1\n",
    "train4= train4[mask]\n",
    "test4=data_Jan\n",
    "yhat = lof.fit_predict(test4)\n",
    "mask = yhat != -1\n",
    "test4= test4[mask]\n",
    "\n",
    "frame5=[data_Oct,data_Nov,data_Jan,data_Feb,data_Mar,data_Apr]#,data_feb\n",
    "Train5=pd.concat(frame5)\n",
    "train5=Train5.sample(frac=1)\n",
    "yhat = lof.fit_predict(train5)\n",
    "mask = yhat != -1\n",
    "train5= train5[mask]\n",
    "test5=data_Dec\n",
    "yhat = lof.fit_predict(test5)\n",
    "mask = yhat != -1\n",
    "test5= test5[mask]\n",
    "\n",
    "frame6=[data_Oct,data_Dec,data_Jan,data_Feb,data_Mar,data_Apr]#,data_feb\n",
    "Train6=pd.concat(frame6)\n",
    "train6=Train6.sample(frac=1)\n",
    "yhat = lof.fit_predict(train6)\n",
    "mask = yhat != -1\n",
    "train6= train6[mask]\n",
    "test6=data_Nov\n",
    "yhat = lof.fit_predict(test6)\n",
    "mask = yhat != -1\n",
    "test6= test6[mask]\n",
    "\n",
    "frame7=[data_Nov,data_Dec,data_Jan,data_Feb,data_Mar,data_Apr]#,data_feb\n",
    "Train7=pd.concat(frame7)\n",
    "train7=Train7.sample(frac=1)\n",
    "yhat = lof.fit_predict(train7)\n",
    "mask = yhat != -1\n",
    "train7= train7[mask]\n",
    "test7=data_Oct\n",
    "yhat = lof.fit_predict(test7)\n",
    "mask = yhat != -1\n",
    "test7= test7[mask]\n",
    "\n",
    "\n",
    "Train=[train1,train2,train3,train4,train5,train6,train7]\n",
    "Test=[test1,test2,test3,test4,test5,test6,test7]\n",
    "train=[Train1,Train2,Train3,Train4,Train5,Train6,Train7]\n",
    "\n",
    "\n",
    "R2=[]\n",
    "RMSE=[]\n",
    "R=[]\n",
    "for i in range(len(Train)):\n",
    "    model=regressor.fit(Train[i][A],Train[i][B])\n",
    "    pred=model.predict(Test[i][A])\n",
    "    r=round(np.corrcoef(Test[i][B], pred)[0, 1],2)\n",
    "    rmse=round(np.sqrt(sm.mean_squared_error(Test[i][B], pred))/np.mean(Test[i][B]),2)\n",
    "    #RMSE2=(round(MAE(y_train2[int(0.02*i*X_train2.shape[0]):], pred2),2)/np.mean(y))\n",
    "    r2=round(sm.r2_score(Test[i][B], pred), 2)\n",
    "    \n",
    "    R2.append(r2)\n",
    "    RMSE.append(rmse)\n",
    "    R.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-journal",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig= plt.figure(figsize=(5.3,3.5))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "x= ['Pred1', 'Pred2', 'Pred3','Pred4','Pred5','Pred6','Pred7']\n",
    "students1=RMSE\n",
    "students2= [48,32,22,18,16]\n",
    "plt.hlines([rmse_r], -0.5, 6.8, color='orange', linewidth=2)\n",
    "x = np.arange(7)\n",
    "wid= 0.7\n",
    "graph1=ax.bar(x,students1,wid, color='#00BFFF', alpha=1)\n",
    "#graph2=ax.bar(x+0.2,students2,wid, color='#00BFFF', alpha=1)\n",
    "#plt.legend(['15min','60min'],title='Data resolution')\n",
    "i= 0\n",
    "for p in graph1:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    plt.text(x+width/2,\n",
    "             y+height+0.02,\n",
    "             str(students1[i]),\n",
    "             ha='center',\n",
    "             weight='bold',fontsize=10, color='#00BFFF', alpha=1)\n",
    "    i+=1\n",
    "\n",
    "ax.set_xticks([0,1,2,3,4,5,6])\n",
    "ax.set_xticklabels(['Pred1', 'Pred2', 'Pred3','Pred4','Pred5','Pred6','Pred7'])\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.yticks(np.arange(0,0.7, step=0.1))\n",
    "\n",
    "#plt.xlabel('Tolerance,Tc (%)', fontsize=19)\n",
    "plt.ylabel('NRMSE', fontsize=19)\n",
    "plt.setp(ax.spines.values(), linewidth=1.4)\n",
    "plt.title(\"O3\",fontsize=18)\n",
    "plt.savefig(\"Pred_O3.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-framing",
   "metadata": {},
   "source": [
    "Ref=O3_Data['Ref'].to_list()\n",
    "Ref_N=O3_Data['Ref_NO2'].to_list()\n",
    "\n",
    "O3_Data.plot.line(y=['Ref', 'Ref_NO2'], figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "useful-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_shape = (7,),kernel_initializer='normal', activation= 'linear'))\n",
    "model.add(Dense(128,kernel_initializer='normal', activation= 'relu'))\n",
    "model.add(Dense(128, kernel_initializer='normal',activation= 'relu'))\n",
    "model.add(Dense(100, kernel_initializer='normal',activation= 'relu'))\n",
    "\n",
    "model.add(Dense(1,kernel_initializer='normal',activation='linear',))\n",
    "sgd = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer = sgd, loss = 'mean_squared_error', metrics= ['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-dream",
   "metadata": {},
   "source": [
    "# Sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-wales",
   "metadata": {},
   "source": [
    "# 1 Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "informational-slovenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] 0.43\n",
      "[4] 0.374\n",
      "[1] 0.38\n",
      "[5] 0.499\n",
      "[2] 0.22\n",
      "[4] 0.376\n",
      "[5] 0.493\n",
      "[5] 0.498\n",
      "[1] 0.382\n",
      "[0] 0.396\n",
      "[1] 0.386\n",
      "[1] 0.378\n",
      "[0] 0.4\n",
      "[3] 0.43\n",
      "[4] 0.372\n",
      "[3] 0.431\n",
      "[2] 0.22\n",
      "[4] 0.373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3e426beb3270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mrmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;31m# Parallel loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n\u001b[0m\u001b[1;32m   1005\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f=1\n",
    "N=50\n",
    "\n",
    "df1=[x for _, x in NO2_Data.groupby('Month')]\n",
    "data_oct=df1[4]\n",
    "#data_oct=data_oct.sample(frac=1)\n",
    "data_nov=df1[5]\n",
    "#data_nov=data_nov.sample(frac=1)\n",
    "data_dec=df1[6]\n",
    "#data_dec=data_dec.sample(frac=1)\n",
    "data_jan=df1[0]\n",
    "#data_jan=data_jan.sample(frac=1)\n",
    "data_feb=df1[1]\n",
    "#data_feb=data_feb.sample(frac=1)\n",
    "data_mar=df1[2]\n",
    "\n",
    "RMSE1=[]\n",
    "U1=[]\n",
    "for i in range(1,9):\n",
    "    data_Oct=data_oct.resample('60min').mean()\n",
    "    data_Oct=data_Oct.dropna()\n",
    "    #data_oct=data_Oct.iloc[::-1]\n",
    "    \n",
    "    data_Oct1=data_Oct[:int(0.1*i*data_Oct.shape[0])]\n",
    "    data_Oct2=data_Oct[int(0.1*i*data_Oct.shape[0]):]\n",
    "    data_Nov=data_nov.resample('60min').mean()\n",
    "    data_Nov=data_Nov.dropna()\n",
    "    #data_nov=data_Nov.iloc[::-1]\n",
    "    \n",
    "    data_Nov1=data_Nov[:int(0.1*i*data_Nov.shape[0])]\n",
    "    data_Nov2=data_Nov[int(0.1*i*data_Nov.shape[0]):]\n",
    "    data_Dec=data_dec.resample('60min').mean()\n",
    "    data_Dec=data_Dec.dropna()\n",
    "    #data_dec=data_Dec.iloc[::-1]\n",
    "    \n",
    "    data_Dec1=data_Dec[:int(0.1*i*data_Dec.shape[0])]\n",
    "    data_Dec2=data_Dec[int(0.1*i*data_Dec.shape[0]):]\n",
    "    data_Jan=data_jan.resample('60min').mean()\n",
    "    data_Jan=data_Jan.dropna()\n",
    "    #data_jan=data_Jan.iloc[::-1]\n",
    "    \n",
    "    data_Jan1=data_Jan[:int(0.1*i*data_Jan.shape[0])]\n",
    "    data_Jan2=data_Jan[int(0.1*i*data_Jan.shape[0]):]\n",
    "    data_Feb=data_feb.resample('60min').mean()\n",
    "    data_Feb=data_Feb.dropna()\n",
    "    #data_feb=data_Feb.iloc[::-1]\n",
    "    \n",
    "    \n",
    "    data_Feb1=data_Feb[:int(0.1*i*data_Feb.shape[0])]\n",
    "    data_Feb2=data_Feb[int(0.1*i*data_Feb.shape[0]):]\n",
    "    data_Mar=data_mar.resample('60min').mean()\n",
    "    data_Mar=data_Mar.dropna()\n",
    "    #data_Mar=data_Mar.iloc[::-1]\n",
    "    \n",
    "    data_Mar1=data_Mar[:int(0.1*i*data_Mar.shape[0])]\n",
    "    data_Mar2=data_Mar[int(0.1*i*data_Mar.shape[0]):]\n",
    "    frame1=[data_Oct1,data_Nov1,data_Dec1,data_Jan1,data_Feb1,data_Mar1]\n",
    "    frame2=[data_Oct2,data_Nov2,data_Dec2,data_Jan2,data_Feb2,data_Mar2]\n",
    "    U=[]\n",
    "    RMSE=[]\n",
    "    Data1=[]\n",
    "    Data2=[]\n",
    "    list = [j for j in range(len(data))]\n",
    "    for k in range(N):\n",
    "        ind=random.sample(list, f)\n",
    "        dat1=[frame1[ind[l]] for l in range(len(ind))]\n",
    "        dat2=[frame2[ind[l]] for l in range(len(ind))]\n",
    "        Data1=pd.concat(dat1)\n",
    "        Data2=pd.concat(dat2)\n",
    "        Data=pd.concat([Data1,Data2])\n",
    "        X=Data[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour','Ref_O3']]\n",
    "        y=Data['Ref']\n",
    "        X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "        model=regressor1.fit(X_train,y_train)\n",
    "        X_test_max=X.loc[y ==max(y)]\n",
    "        y_test_max=y.loc[y ==max(y)]\n",
    "        X_test=pd.concat([X_test,X_test_max])\n",
    "        y_test=pd.concat([y_test,y_test_max])\n",
    "        pred=model.predict(X_test)\n",
    "        rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "        u=round(REF(pred,y_test,1.4),2)\n",
    "        U.append(u)\n",
    "        RMSE.append(rmse)\n",
    "    U1.append(np.round(np.mean(U),2))\n",
    "    RMSE1.append(np.round(np.mean(RMSE),2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "yellow-construction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74.9, 47.83, 39.84, 36.17, 31.79, 30.45, 24.35, 23.08]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "upset-exercise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37, 0.28, 0.27, 0.26, 0.24, 0.24, 0.22, 0.2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-receiver",
   "metadata": {},
   "source": [
    "# 3 Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "requested-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=3\n",
    "N=50\n",
    "\n",
    "df1=[x for _, x in NO2_Data.groupby('Month')]\n",
    "data_oct=df1[4]\n",
    "#data_oct=data_oct.sample(frac=1)\n",
    "data_nov=df1[5]\n",
    "#data_nov=data_nov.sample(frac=1)\n",
    "data_dec=df1[6]\n",
    "#data_dec=data_dec.sample(frac=1)\n",
    "data_jan=df1[0]\n",
    "#data_jan=data_jan.sample(frac=1)\n",
    "data_feb=df1[1]\n",
    "#data_feb=data_feb.sample(frac=1)\n",
    "data_mar=df1[2]\n",
    "\n",
    "RMSE3=[]\n",
    "U3=[]\n",
    "for i in range(1,9):\n",
    "    data_Oct=data_oct.resample('60min').mean()\n",
    "    data_Oct=data_Oct.dropna()\n",
    "    #data_oct=data_Oct.iloc[::-1]\n",
    "    \n",
    "    data_Oct1=data_Oct[:int(0.1*i*data_Oct.shape[0])]\n",
    "    data_Oct2=data_Oct[int(0.1*i*data_Oct.shape[0]):]\n",
    "    data_Nov=data_nov.resample('60min').mean()\n",
    "    data_Nov=data_Nov.dropna()\n",
    "    #data_nov=data_Nov.iloc[::-1]\n",
    "    \n",
    "    data_Nov1=data_Nov[:int(0.1*i*data_Nov.shape[0])]\n",
    "    data_Nov2=data_Nov[int(0.1*i*data_Nov.shape[0]):]\n",
    "    data_Dec=data_dec.resample('60min').mean()\n",
    "    data_Dec=data_Dec.dropna()\n",
    "    #data_dec=data_Dec.iloc[::-1]\n",
    "    \n",
    "    data_Dec1=data_Dec[:int(0.1*i*data_Dec.shape[0])]\n",
    "    data_Dec2=data_Dec[int(0.1*i*data_Dec.shape[0]):]\n",
    "    data_Jan=data_jan.resample('60min').mean()\n",
    "    data_Jan=data_Jan.dropna()\n",
    "    #data_jan=data_Jan.iloc[::-1]\n",
    "    \n",
    "    data_Jan1=data_Jan[:int(0.1*i*data_Jan.shape[0])]\n",
    "    data_Jan2=data_Jan[int(0.1*i*data_Jan.shape[0]):]\n",
    "    data_Feb=data_feb.resample('60min').mean()\n",
    "    data_Feb=data_Feb.dropna()\n",
    "    #data_feb=data_Feb.iloc[::-1]\n",
    "    \n",
    "    \n",
    "    data_Feb1=data_Feb[:int(0.1*i*data_Feb.shape[0])]\n",
    "    data_Feb2=data_Feb[int(0.1*i*data_Feb.shape[0]):]\n",
    "    data_Mar=data_mar.resample('60min').mean()\n",
    "    data_Mar=data_Mar.dropna()\n",
    "    #data_Mar=data_Mar.iloc[::-1]\n",
    "    \n",
    "    data_Mar1=data_Mar[:int(0.1*i*data_Mar.shape[0])]\n",
    "    data_Mar2=data_Mar[int(0.1*i*data_Mar.shape[0]):]\n",
    "    frame1=[data_Oct1,data_Nov1,data_Dec1,data_Jan1,data_Feb1,data_Mar1]\n",
    "    frame2=[data_Oct2,data_Nov2,data_Dec2,data_Jan2,data_Feb2,data_Mar2]\n",
    "    U=[]\n",
    "    RMSE=[]\n",
    "    Data1=[]\n",
    "    Data2=[]\n",
    "    list = [j for j in range(len(data))]\n",
    "    for k in range(N):\n",
    "        ind=random.sample(list, f)\n",
    "        dat1=[frame1[ind[l]] for l in range(len(ind))]\n",
    "        dat2=[frame2[ind[l]] for l in range(len(ind))]\n",
    "        Data1=pd.concat(dat1)\n",
    "        Data2=pd.concat(dat2)\n",
    "        Data=pd.concat([Data1,Data2])\n",
    "        X=Data[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour','Ref_O3']]\n",
    "        y=Data['Ref']\n",
    "        X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "        model=regressor1.fit(X_train,y_train)\n",
    "        X_test_max=X.loc[y ==max(y)]\n",
    "        y_test_max=y.loc[y ==max(y)]\n",
    "        X_test=pd.concat([X_test,X_test_max])\n",
    "        y_test=pd.concat([y_test,y_test_max])\n",
    "        pred=model.predict(X_test)\n",
    "        rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "        u=round(REF(pred,y_test,1.4),2)\n",
    "        U.append(u)\n",
    "        RMSE.append(rmse)\n",
    "    U3.append(np.round(np.mean(U),2))\n",
    "    RMSE3.append(np.round(np.mean(RMSE),2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "alternate-activity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66.08, 43.0, 32.35, 31.32, 28.33, 25.98, 21.82, 19.37]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "intensive-firewall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34, 0.26, 0.23, 0.23, 0.22, 0.21, 0.2, 0.19]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-attempt",
   "metadata": {},
   "source": [
    "#  6 Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fluid-improvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.11\n",
      "51.93\n",
      "51.2\n",
      "52.05\n",
      "52.1\n",
      "51.1\n",
      "51.21\n",
      "51.25\n",
      "50.86\n",
      "52.01\n",
      "50.93\n",
      "51.32\n",
      "51.63\n",
      "51.18\n",
      "52.21\n",
      "51.58\n",
      "51.59\n",
      "51.12\n",
      "51.93\n",
      "51.46\n",
      "51.91\n",
      "51.26\n",
      "51.62\n",
      "51.27\n",
      "51.73\n",
      "51.75\n",
      "51.95\n",
      "51.3\n",
      "51.62\n",
      "50.97\n",
      "51.66\n",
      "51.82\n",
      "51.71\n",
      "52.01\n",
      "51.13\n",
      "51.87\n",
      "53.12\n",
      "51.53\n",
      "51.44\n",
      "51.47\n",
      "50.96\n",
      "51.55\n",
      "51.04\n",
      "51.94\n",
      "51.87\n",
      "51.54\n",
      "52.22\n",
      "51.04\n",
      "51.33\n",
      "51.39\n",
      "34.62\n",
      "34.88\n",
      "34.82\n",
      "34.44\n",
      "35.21\n",
      "34.72\n",
      "34.93\n",
      "35.01\n",
      "34.59\n",
      "35.05\n",
      "35.36\n",
      "34.36\n",
      "34.44\n",
      "34.7\n",
      "35.17\n",
      "35.01\n",
      "34.6\n",
      "35.13\n",
      "34.96\n",
      "34.44\n",
      "35.13\n",
      "34.88\n",
      "35.03\n",
      "34.43\n",
      "34.85\n",
      "34.91\n",
      "34.85\n",
      "34.72\n",
      "34.74\n",
      "34.76\n",
      "34.76\n",
      "35.36\n",
      "34.66\n",
      "34.71\n",
      "34.6\n",
      "34.63\n",
      "34.83\n",
      "34.83\n",
      "34.72\n",
      "35.15\n",
      "34.56\n",
      "34.7\n",
      "34.98\n",
      "35.24\n",
      "34.76\n",
      "34.5\n",
      "34.34\n",
      "34.68\n",
      "35.17\n",
      "34.7\n",
      "24.8\n",
      "25.08\n",
      "24.9\n",
      "25.26\n",
      "25.14\n",
      "24.98\n",
      "25.26\n",
      "25.3\n",
      "24.98\n",
      "25.12\n",
      "25.33\n",
      "25.09\n",
      "25.28\n",
      "25.32\n",
      "25.1\n",
      "25.45\n",
      "25.33\n",
      "24.94\n",
      "25.19\n",
      "24.73\n",
      "25.14\n",
      "24.98\n",
      "24.91\n",
      "25.0\n",
      "24.83\n",
      "25.18\n",
      "25.22\n",
      "24.99\n",
      "25.05\n",
      "24.92\n",
      "25.1\n",
      "24.78\n",
      "25.24\n",
      "25.16\n",
      "24.73\n",
      "24.98\n",
      "25.1\n",
      "25.06\n",
      "25.07\n",
      "24.94\n",
      "25.22\n",
      "25.04\n",
      "25.03\n",
      "25.12\n",
      "24.94\n",
      "25.16\n",
      "24.87\n",
      "25.11\n",
      "25.12\n",
      "24.95\n",
      "24.28\n",
      "24.31\n",
      "24.02\n",
      "24.51\n",
      "23.97\n",
      "24.28\n",
      "24.52\n",
      "24.16\n",
      "24.45\n",
      "24.21\n",
      "24.31\n",
      "24.34\n",
      "24.39\n",
      "23.91\n",
      "24.39\n",
      "23.93\n",
      "24.26\n",
      "24.37\n",
      "24.27\n",
      "24.51\n",
      "24.06\n",
      "24.36\n",
      "24.13\n",
      "24.39\n",
      "24.35\n",
      "24.07\n",
      "24.31\n",
      "24.03\n",
      "24.23\n",
      "24.25\n",
      "24.18\n",
      "24.19\n",
      "24.21\n",
      "24.3\n",
      "24.44\n",
      "24.42\n",
      "24.15\n",
      "24.01\n",
      "24.22\n",
      "24.34\n",
      "24.34\n",
      "24.25\n",
      "24.3\n",
      "24.34\n",
      "24.14\n",
      "24.28\n",
      "24.08\n",
      "24.49\n",
      "24.43\n",
      "24.39\n",
      "20.61\n",
      "20.43\n",
      "20.55\n",
      "20.82\n",
      "20.58\n",
      "20.63\n",
      "20.6\n",
      "20.53\n",
      "20.57\n",
      "20.63\n",
      "20.69\n",
      "20.74\n",
      "20.72\n",
      "20.4\n",
      "20.62\n",
      "20.71\n",
      "20.78\n",
      "20.58\n",
      "20.72\n",
      "20.57\n",
      "21.02\n",
      "20.34\n",
      "20.81\n",
      "20.57\n",
      "20.74\n",
      "20.6\n",
      "20.53\n",
      "20.6\n",
      "20.63\n",
      "20.74\n",
      "20.38\n",
      "20.57\n",
      "20.72\n",
      "20.62\n",
      "20.72\n",
      "20.82\n",
      "20.58\n",
      "20.62\n",
      "20.61\n",
      "20.54\n",
      "20.81\n",
      "20.8\n",
      "20.46\n",
      "20.54\n",
      "20.98\n",
      "20.67\n",
      "20.59\n",
      "20.8\n",
      "20.55\n",
      "20.94\n",
      "20.88\n",
      "20.88\n",
      "21.18\n",
      "20.99\n",
      "21.17\n",
      "21.08\n",
      "20.91\n",
      "20.64\n",
      "20.98\n",
      "20.79\n",
      "21.08\n",
      "20.78\n",
      "20.87\n",
      "20.97\n",
      "21.0\n",
      "20.78\n",
      "20.89\n",
      "20.76\n",
      "21.02\n",
      "20.84\n",
      "21.05\n",
      "20.83\n",
      "21.02\n",
      "20.96\n",
      "21.07\n",
      "20.62\n",
      "20.91\n",
      "20.68\n",
      "21.07\n",
      "20.95\n",
      "20.79\n",
      "21.08\n",
      "20.76\n",
      "20.78\n",
      "20.77\n",
      "21.09\n",
      "20.81\n",
      "20.99\n",
      "21.07\n",
      "20.85\n",
      "20.95\n",
      "20.96\n",
      "21.11\n",
      "20.58\n",
      "21.03\n",
      "20.85\n",
      "20.9\n",
      "20.84\n",
      "20.8\n",
      "20.82\n",
      "16.56\n",
      "16.34\n",
      "16.34\n",
      "16.29\n",
      "16.65\n",
      "16.71\n",
      "16.65\n",
      "16.57\n",
      "16.62\n",
      "16.25\n",
      "16.51\n",
      "16.48\n",
      "16.71\n",
      "16.61\n",
      "16.61\n",
      "16.52\n",
      "16.47\n",
      "16.57\n",
      "16.52\n",
      "16.6\n",
      "16.4\n",
      "16.77\n",
      "16.78\n",
      "16.61\n",
      "16.84\n",
      "16.58\n",
      "16.57\n",
      "16.7\n",
      "16.51\n",
      "16.63\n",
      "16.76\n",
      "16.58\n",
      "16.55\n",
      "16.5\n",
      "16.48\n",
      "16.5\n",
      "16.6\n",
      "16.56\n",
      "16.65\n",
      "16.52\n",
      "16.62\n",
      "16.39\n",
      "16.48\n",
      "16.78\n",
      "16.36\n",
      "16.46\n",
      "16.39\n",
      "16.3\n",
      "16.43\n",
      "16.45\n",
      "15.92\n",
      "15.82\n",
      "16.04\n",
      "15.95\n",
      "15.75\n",
      "16.01\n",
      "15.83\n",
      "15.93\n",
      "15.92\n",
      "15.83\n",
      "15.98\n",
      "15.89\n",
      "15.7\n",
      "15.75\n",
      "15.9\n",
      "15.77\n",
      "15.91\n",
      "16.04\n",
      "15.8\n",
      "16.04\n",
      "15.89\n",
      "16.04\n",
      "15.88\n",
      "16.07\n",
      "15.88\n",
      "16.0\n",
      "15.86\n",
      "16.03\n",
      "15.85\n",
      "15.85\n",
      "16.02\n",
      "15.96\n",
      "15.9\n",
      "15.87\n",
      "15.93\n",
      "16.1\n",
      "15.96\n",
      "16.03\n",
      "16.09\n",
      "15.91\n",
      "16.12\n",
      "15.81\n",
      "15.66\n",
      "16.07\n",
      "15.9\n",
      "15.95\n",
      "15.75\n",
      "16.28\n",
      "15.93\n",
      "15.98\n"
     ]
    }
   ],
   "source": [
    "f=5\n",
    "N=50\n",
    "\n",
    "df1=[x for _, x in NO2_Data.groupby('Month')]\n",
    "data_oct=df1[4]\n",
    "#data_oct=data_oct.sample(frac=1)\n",
    "data_nov=df1[5]\n",
    "#data_nov=data_nov.sample(frac=1)\n",
    "data_dec=df1[6]\n",
    "#data_dec=data_dec.sample(frac=1)\n",
    "data_jan=df1[0]\n",
    "#data_jan=data_jan.sample(frac=1)\n",
    "data_feb=df1[1]\n",
    "#data_feb=data_feb.sample(frac=1)\n",
    "data_mar=df1[2]\n",
    "\n",
    "RMSE6=[]\n",
    "U6=[]\n",
    "for i in range(1,9):\n",
    "    data_Oct=data_oct.resample('60min').mean()\n",
    "    data_Oct=data_Oct.dropna()\n",
    "    #data_oct=data_Oct.iloc[::-1]\n",
    "    \n",
    "    data_Oct1=data_Oct[:int(0.1*i*data_Oct.shape[0])]\n",
    "    data_Oct2=data_Oct[int(0.1*i*data_Oct.shape[0]):]\n",
    "    data_Nov=data_nov.resample('60min').mean()\n",
    "    data_Nov=data_Nov.dropna()\n",
    "    #data_nov=data_Nov.iloc[::-1]\n",
    "    \n",
    "    data_Nov1=data_Nov[:int(0.1*i*data_Nov.shape[0])]\n",
    "    data_Nov2=data_Nov[int(0.1*i*data_Nov.shape[0]):]\n",
    "    data_Dec=data_dec.resample('60min').mean()\n",
    "    data_Dec=data_Dec.dropna()\n",
    "    #data_dec=data_Dec.iloc[::-1]\n",
    "    \n",
    "    data_Dec1=data_Dec[:int(0.1*i*data_Dec.shape[0])]\n",
    "    data_Dec2=data_Dec[int(0.1*i*data_Dec.shape[0]):]\n",
    "    data_Jan=data_jan.resample('60min').mean()\n",
    "    data_Jan=data_Jan.dropna()\n",
    "    #data_jan=data_Jan.iloc[::-1]\n",
    "    \n",
    "    data_Jan1=data_Jan[:int(0.1*i*data_Jan.shape[0])]\n",
    "    data_Jan2=data_Jan[int(0.1*i*data_Jan.shape[0]):]\n",
    "    data_Feb=data_feb.resample('60min').mean()\n",
    "    data_Feb=data_Feb.dropna()\n",
    "    #data_feb=data_Feb.iloc[::-1]\n",
    "    \n",
    "    \n",
    "    data_Feb1=data_Feb[:int(0.1*i*data_Feb.shape[0])]\n",
    "    data_Feb2=data_Feb[int(0.1*i*data_Feb.shape[0]):]\n",
    "    data_Mar=data_mar.resample('60min').mean()\n",
    "    data_Mar=data_Mar.dropna()\n",
    "    #data_Mar=data_Mar.iloc[::-1]\n",
    "    \n",
    "    data_Mar1=data_Mar[:int(0.1*i*data_Mar.shape[0])]\n",
    "    data_Mar2=data_Mar[int(0.1*i*data_Mar.shape[0]):]\n",
    "    frame1=[data_Oct1,data_Nov1,data_Dec1,data_Jan1,data_Feb1]\n",
    "    frame2=[data_Oct2,data_Nov2,data_Dec2,data_Jan2,data_Feb2]\n",
    "    U=[]\n",
    "    RMSE=[]\n",
    "    Data1=[]\n",
    "    Data2=[]\n",
    "    list = [j for j in range(len(frame1))]\n",
    "    for k in range(N):\n",
    "        ind=random.sample(list, f)\n",
    "        dat1=[frame1[ind[l]] for l in range(len(ind))]\n",
    "        dat2=[frame2[ind[l]] for l in range(len(ind))]\n",
    "        Data1=pd.concat(dat1)\n",
    "        Data2=pd.concat(dat2)\n",
    "        Data=pd.concat([Data1,Data2])\n",
    "        X=Data[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour','Ref_O3']]\n",
    "        y=Data['Ref']\n",
    "        X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "        model=regressor1.fit(X_train,y_train)\n",
    "        X_test_max=X.loc[y ==max(y)]\n",
    "        y_test_max=y.loc[y ==max(y)]\n",
    "        X_test=pd.concat([X_test,X_test_max])\n",
    "        y_test=pd.concat([y_test,y_test_max])\n",
    "        pred=model.predict(X_test)\n",
    "        rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "        u=round(REF(pred,y_test,1.4),2)\n",
    "        print(u)\n",
    "        U.append(u)\n",
    "        RMSE.append(rmse)\n",
    "    U6.append(np.round(np.mean(U),2))\n",
    "    RMSE6.append(np.round(np.mean(RMSE),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "powerful-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51.56, 34.81, 25.07, 24.26, 20.65, 20.91, 16.55, 15.93]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "systematic-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27, 0.22, 0.2, 0.2, 0.18, 0.18, 0.16, 0.15]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-fishing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-richmond",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-garlic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "virgin-artist",
   "metadata": {},
   "source": [
    "# Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "external-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing random module\n",
    "import random\n",
    "df1=[x for _, x in CO_Data.groupby('Month')]\n",
    "data_oct=df1[4]\n",
    "#data_oct=data_oct.sample(frac=1)\n",
    "data_nov=df1[5]\n",
    "#data_nov=data_nov.sample(frac=1)\n",
    "data_dec=df1[6]\n",
    "#data_dec=data_dec.sample(frac=1)\n",
    "data_jan=df1[0]\n",
    "#data_jan=data_jan.sample(frac=1)\n",
    "data_feb=df1[1]\n",
    "#data_feb=data_feb.sample(frac=1)\n",
    "data_mar=df1[2]\n",
    "data_Oct=data_oct.resample('60min').mean()\n",
    "data_Oct=data_Oct.dropna()\n",
    "data_oct=data_Oct.iloc[::-1]\n",
    "data_Nov=data_nov.resample('60min').mean()\n",
    "data_Nov=data_Nov.dropna()\n",
    "data_nov=data_Nov.iloc[::-1]\n",
    "data_Dec=data_dec.resample('60min').mean()\n",
    "data_Dec=data_Dec.dropna()\n",
    "data_dec=data_Dec.iloc[::-1]\n",
    "data_Jan=data_jan.resample('60min').mean()\n",
    "data_Jan=data_Jan.dropna()\n",
    "data_jan=data_Jan.iloc[::-1]\n",
    "data_Feb=data_feb.resample('60min').mean()\n",
    "data_Feb=data_Feb.dropna()\n",
    "data_feb=data_Feb.iloc[::-1]\n",
    "data_Mar=data_mar.resample('60min').mean()\n",
    "data_Mar=data_Mar.dropna()\n",
    "data_mar=data_Mar.iloc[::-1]\n",
    "data=[data_Oct,data_Nov,data_Dec,data_Jan]#data_Feb\n",
    "frame1=[data_Dec1,data_Jan1]\n",
    "frame2=[data_Dec1,data_Jan2,]\n",
    "CO_data1=pd.concat(frame1)\n",
    "CO_data2=pd.concat(frame2)\n",
    "CO_data=pd.concat([CO_data1,CO_data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cloudy-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=1\n",
    "N=50\n",
    "U=[]\n",
    "RMSE=[]\n",
    "L_data=[]\n",
    "list = [i for i in range(len(data))]\n",
    "for i in range(1,9):\n",
    "    U1=[]\n",
    "    RMSE1=[]\n",
    "    j=0\n",
    "    while j<len(data):\n",
    "        ind=[j]\n",
    "        dat=[data[ind[k]] for k in range(len(ind))]\n",
    "        Data=pd.concat(dat)\n",
    "        X=Data[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour']]\n",
    "        y=Data['Ref']\n",
    "        X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "        model=regressor1.fit(X_train,y_train)\n",
    "        X_test_max=X.loc[y ==max(y)]\n",
    "        y_test_max=y.loc[y ==max(y)]\n",
    "        X_test=pd.concat([X_test,X_test_max])\n",
    "        y_test=pd.concat([y_test,y_test_max])\n",
    "        pred=model.predict(X_test)\n",
    "        rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "        u=round(REF(pred,y_test,1.4),2)\n",
    "        U1.append(u)\n",
    "        RMSE1.append(rmse)\n",
    "    j=j+1\n",
    "    U.append(np.array(U1))\n",
    "    RMSE.append(np.array(RMSE1))\n",
    "REU=np.array(U)\n",
    "REU_1=sum(REU)/len(REU)\n",
    "NRMSE=np.array(RMSE)\n",
    "NRMSE_1=sum(NRMSE)/len(NRMSE)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "thrown-adventure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74.04   , 55.94625, 96.24125, 84.985  , 71.82625, 51.35125])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REU_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "packed-venezuela",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.525625, 0.617125, 0.758375, 0.60475 , 0.4515  , 0.31625 ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NRMSE_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "elementary-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=3\n",
    "N=50\n",
    "U=[]\n",
    "RMSE=[]\n",
    "L_data=[]\n",
    "list = [i for i in range(len(data))]\n",
    "for i in range(N):\n",
    "    ind=random.sample(list, f)\n",
    "    dat=[data[ind[i]] for i in range(len(ind))]\n",
    "    L_data.append(dat)\n",
    "Data=[pd.concat(L_data[i]) for i in range(len(L_data))]\n",
    "for j in range(len(Data)):\n",
    "    U1=[]\n",
    "    RMSE1=[]\n",
    "    for i in range(1,9):\n",
    "        X=Data[j][['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour','Ref_O3']]\n",
    "        y=Data[j]['Ref']\n",
    "        X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "        model=regressor1.fit(X_train,y_train)\n",
    "        X_test_max=X.loc[y ==max(y)]\n",
    "        y_test_max=y.loc[y ==max(y)]\n",
    "        X_test=pd.concat([X_test,X_test_max])\n",
    "        y_test=pd.concat([y_test,y_test_max])\n",
    "        pred=model.predict(X_test)\n",
    "        rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "        u=round(REF(pred,y_test,1.4),2)\n",
    "        U1.append(u)\n",
    "        RMSE1.append(rmse)\n",
    "    U.append(np.array(U1))\n",
    "    RMSE.append(np.array(RMSE1))\n",
    "REU=np.array(U)\n",
    "REU_3=sum(REU)/len(REU)\n",
    "NRMSE=np.array(RMSE)\n",
    "NRMSE_3=sum(NRMSE)/len(NRMSE)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "infinite-raleigh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([54.7422, 50.8156, 46.754 , 43.2864, 36.0142, 33.8986, 32.3864,\n",
       "       25.7888])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REU_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "indian-ideal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31926, 0.3037 , 0.28908, 0.27644, 0.2589 , 0.25152, 0.23922,\n",
       "       0.22122])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NRMSE_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "civil-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=6\n",
    "N=50\n",
    "U=[]\n",
    "RMSE=[]\n",
    "L_data=[]\n",
    "list = [i for i in range(len(data))]\n",
    "for i in range(N):\n",
    "    ind=random.sample(list, f)\n",
    "    dat=[data[ind[i]] for i in range(len(ind))]\n",
    "    L_data.append(dat)\n",
    "Data=[pd.concat(L_data[i]) for i in range(len(L_data))]\n",
    "for j in range(len(Data)):\n",
    "    U1=[]\n",
    "    RMSE1=[]\n",
    "    for i in range(1,9):\n",
    "        X=Data[j][['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Hour','Ref_O3']]\n",
    "        y=Data[j]['Ref']\n",
    "        X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "        model=regressor1.fit(X_train,y_train)\n",
    "        X_test_max=X.loc[y ==max(y)]\n",
    "        y_test_max=y.loc[y ==max(y)]\n",
    "        X_test=pd.concat([X_test,X_test_max])\n",
    "        y_test=pd.concat([y_test,y_test_max])\n",
    "        pred=model.predict(X_test)\n",
    "        rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "        u=round(REF(pred,y_test,1.4),2)\n",
    "        U1.append(u)\n",
    "        RMSE1.append(rmse)\n",
    "    U.append(np.array(U1))\n",
    "    RMSE.append(np.array(RMSE1))\n",
    "REU=np.array(U)\n",
    "REU_6=sum(REU)/len(REU)\n",
    "NRMSE=np.array(RMSE)\n",
    "NRMSE_6=sum(NRMSE)/len(NRMSE)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "alleged-makeup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51.4472, 45.7908, 39.0024, 34.9292, 33.1632, 31.684 , 29.1972,\n",
       "       26.3578])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REU_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fixed-joyce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30472, 0.2753 , 0.25358, 0.24242, 0.23782, 0.2373 , 0.23206,\n",
       "       0.2255 ])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NRMSE_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-redhead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-orleans",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-messaging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fitted-identifier",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Ref_NO2'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-e49a3f27fe1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mCO_data2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mCO_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCO_data1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCO_data2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCO_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Net Signal'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Temp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RH'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Month'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Day_of_week'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ref_NO2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCO_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3811\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3813\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6115\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6175\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6176\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6178\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Ref_NO2'] not in index\""
     ]
    }
   ],
   "source": [
    "RMSE_32=[]\n",
    "for i in range(1,9):\n",
    "    data_Oct=data_oct.resample('60min').mean()\n",
    "    data_Oct=data_Oct.dropna()\n",
    "    data_Oct1=data_Oct[:int(0.1*i*data_Oct.shape[0])]\n",
    "    data_Oct2=data_Oct[int(0.1*i*data_Oct.shape[0]):]\n",
    "    data_Nov=data_nov.resample('60min').mean()\n",
    "    data_Nov=data_Nov.dropna()\n",
    "    data_Nov1=data_Nov[:int(0.1*i*data_Nov.shape[0])]\n",
    "    data_Nov2=data_Nov[int(0.1*i*data_Nov.shape[0]):]\n",
    "    data_Dec=data_dec.resample('60min').mean()\n",
    "    data_Dec=data_Dec.dropna()\n",
    "    data_Dec1=data_Dec[:int(0.1*i*data_Dec.shape[0])]\n",
    "    data_Dec2=data_Dec[int(0.1*i*data_Dec.shape[0]):]\n",
    "    data_Jan=data_jan.resample('60min').mean()\n",
    "    data_Jan=data_Jan.dropna()\n",
    "    data_Jan1=data_Jan[:int(0.1*i*data_Jan.shape[0])]\n",
    "    data_Jan2=data_Jan[int(0.1*i*data_Jan.shape[0]):]\n",
    "    data_Feb=data_feb.resample('60min').mean()\n",
    "    data_Feb=data_Feb.dropna()\n",
    "    data_Feb1=data_Feb[:int(0.1*i*data_Feb.shape[0])]\n",
    "    data_Feb2=data_Feb[int(0.1*i*data_Feb.shape[0]):]\n",
    "    data_Mar=data_mar.resample('60min').mean()\n",
    "    data_Mar=data_Mar.dropna()\n",
    "    data_Mar1=data_Mar[:int(0.1*i*data_Mar.shape[0])]\n",
    "    data_Mar2=data_Mar[int(0.1*i*data_Mar.shape[0]):]\n",
    "    frame1=[data_Dec1,data_Jan1]\n",
    "    frame2=[data_Dec1,data_Jan2,]\n",
    "    CO_data1=pd.concat(frame1)\n",
    "    CO_data2=pd.concat(frame2)\n",
    "    CO_data=pd.concat([CO_data1,CO_data2])\n",
    "    X=CO_data[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "    y=CO_data['Ref']\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "    model=regressor.fit(X_train,y_train)\n",
    "    pred=model.predict(X_test)\n",
    "    rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "    RMSE_32.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_33=[]\n",
    "for i in range(1,9):\n",
    "    data_Oct=data_oct.resample('60min').mean()\n",
    "    data_Oct=data_Oct.dropna()\n",
    "    data_Oct1=data_Oct[:int(0.1*i*data_Oct.shape[0])]\n",
    "    data_Oct2=data_Oct[int(0.1*i*data_Oct.shape[0]):]\n",
    "    data_Nov=data_nov.resample('60min').mean()\n",
    "    data_Nov=data_Nov.dropna()\n",
    "    data_Nov1=data_Nov[:int(0.1*i*data_Nov.shape[0])]\n",
    "    data_Nov2=data_Nov[int(0.1*i*data_Nov.shape[0]):]\n",
    "    data_Dec=data_dec.resample('60min').mean()\n",
    "    data_Dec=data_Dec.dropna()\n",
    "    data_Dec1=data_Dec[:int(0.1*i*data_Dec.shape[0])]\n",
    "    data_Dec2=data_Dec[int(0.1*i*data_Dec.shape[0]):]\n",
    "    data_Jan=data_jan.resample('60min').mean()\n",
    "    data_Jan=data_Jan.dropna()\n",
    "    data_Jan1=data_Jan[:int(0.1*i*data_Jan.shape[0])]\n",
    "    data_Jan2=data_Jan[int(0.1*i*data_Jan.shape[0]):]\n",
    "    data_Feb=data_feb.resample('60min').mean()\n",
    "    data_Feb=data_Feb.dropna()\n",
    "    data_Feb1=data_Feb[:int(0.1*i*data_Feb.shape[0])]\n",
    "    data_Feb2=data_Feb[int(0.1*i*data_Feb.shape[0]):]\n",
    "    data_Mar=data_mar.resample('60min').mean()\n",
    "    data_Mar=data_Mar.dropna()\n",
    "    data_Mar1=data_Mar[:int(0.1*i*data_Mar.shape[0])]\n",
    "    data_Mar2=data_Mar[int(0.1*i*data_Mar.shape[0]):]\n",
    "    frame1=[data_Feb1,data_Mar1]\n",
    "    frame2=[data_Feb2,data_Mar2]\n",
    "    CO_data1=pd.concat(frame1)\n",
    "    CO_data2=pd.concat(frame2)\n",
    "    CO_data=pd.concat([CO_data1,CO_data2])\n",
    "    X=CO_data[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "    y=CO_data['Ref']\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=1-0.1*i,shuffle=False)\n",
    "    model=regressor.fit(X_train,y_train)\n",
    "    pred=model.predict(X_test)\n",
    "    rmse=(round(np.sqrt(sm.mean_squared_error(y_test,pred))/np.mean(y),3))\n",
    "    RMSE_33.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_3=np.round((np.array(RMSE_32)+np.array(RMSE_31)+np.array(RMSE_31))/3,3)\n",
    "RMSE_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-wrestling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-billy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-politics",
   "metadata": {},
   "source": [
    "# October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Oct[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Oct['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "#frame1=[data_oct3,data_nov3]\n",
    "#fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "#Fall=fall.resample('h').mean()\n",
    "#Fall=Fall.dropna()\n",
    "#X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -2\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200]\n",
    "X_train3=X_Data[:350]\n",
    "X_train4=X_Data[:550]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200]\n",
    "y_train3=y_Data[:350]\n",
    "y_train4=y_Data[:550]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_o=[]\n",
    "B3_o=[]\n",
    "B4_o=[]\n",
    "\n",
    "P2_o=[]\n",
    "P3_o=[]\n",
    "P4_o=[]\n",
    "\n",
    "model2=regressor.fit(X_train2[:int(0.4*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.4*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(0.4*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "B2_o.append(b)\n",
    "P2_o.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=X_train4[:int(0.1*2*X_train4.shape[0])]\n",
    "B=X_train4[int(0.1*2*X_train4.shape[0]):]\n",
    "len(A),len(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "er2_o=[]\n",
    "er3_o=[]\n",
    "er4_o=[]\n",
    "MPC2=[]\n",
    "MPC3=[]\n",
    "MPC4=[]\n",
    "RMSE2_o=[]\n",
    "RMSE3_o=[]\n",
    "RMSE4_o=[]\n",
    "R2_o=[]\n",
    "R3_o=[]\n",
    "R4_o=[]\n",
    "U2_o=[]\n",
    "U3_o=[]\n",
    "U4_o=[]\n",
    "KL=[1,2,3,4,5,6,7,8]\n",
    "B2_o=[]\n",
    "B3_o=[]\n",
    "B4_o=[]\n",
    "P2_o=[]\n",
    "P3_o=[]\n",
    "P4_o=[]\n",
    "for i in KL:\n",
    "    model2=regressor.fit(X_train2[:int(0.1*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.1*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.1*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.1*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.1*i*X_train2.shape[0]):], \n",
    "                                               pred2)),1)/np.mean(y))\n",
    "    #RMSE2=(round(MAE(y_train2[int(0.02*i*X_train2.shape[0]):], pred2),2)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.1*i*X_train2.shape[0]):], pred2), 2)\n",
    "    u2=REF(pred2,y_train2[int(0.1*i*X_train2.shape[0]):],1.4)\n",
    "    b2=np.round(bias(pred2,y_train2[int(0.1*i*X_train2.shape[0]):]))\n",
    "    p2=np.round(precision(pred2,y_train2[int(0.1*i*X_train2.shape[0]):]))\n",
    "    B2_o.append(b2)\n",
    "    P2_o.append(p2)\n",
    "    \n",
    "    U2_o.append(u2)\n",
    "    RMSE2_o.append(RMSE2)\n",
    "    R2_o.append(r2)\n",
    "    er2_o.append(rmse2)\n",
    "    \n",
    "for i in KL:\n",
    "    model3=regressor.fit(X_train3[:int(0.1*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.1*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.1*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.1*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    \n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.1*i*X_train3.shape[0]):], \n",
    "                                               pred3)),1)/np.mean(y))\n",
    "    #RMSE3=(round(MAE(y_train3[int(0.02*i*X_train3.shape[0]):], pred3),2)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.1*i*X_train3.shape[0]):], pred3), 2)\n",
    "    u3=REF(pred3,y_train3[int(0.1*i*X_train3.shape[0]):],1.4)\n",
    "    U3_o.append(u3)\n",
    "    RMSE3_o.append(RMSE3)\n",
    "    R3_o.append(r3)\n",
    "    er3_o.append(rmse3)\n",
    "for i in KL:\n",
    "    model4=regressor.fit(X_train4[:int(0.1*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.1*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.1*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.1*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.1*i*X_train4.shape[0]):], \n",
    "                                               pred4)),1)/np.mean(y))\n",
    "    #RMSE4=(round(MAE(y_train4[int(0.02*i*X_train4.shape[0]):], pred4),2)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.1*i*X_train4.shape[0]):], pred4), 2)\n",
    "    u4=REF(pred4,y_train4[int(0.1*i*X_train4.shape[0]):],1.4)\n",
    "    b4=np.round(bias(pred4,y_train4[int(0.1*i*X_train4.shape[0]):]))\n",
    "    p4=np.round(precision(pred4,y_train4[int(0.1*i*X_train4.shape[0]):]))\n",
    "    B4_o.append(b4)\n",
    "    P4_o.append(p4)\n",
    "    U4_o.append(u4)\n",
    "    RMSE4_o.append(RMSE4)\n",
    "    R4_o.append(r4)\n",
    "    er4_o.append(rmse4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "B4_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "P4_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "U4_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE4_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE3_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE4_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[er2_o,er3_o,er4_o]\n",
    "B=[np.mean(er2_o),np.mean(er3_o),np.mean(er4_o)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        er_o=A[i]\n",
    "\n",
    "A=[R2_o,R3_o,R4_o]\n",
    "B=[np.mean(R2_o),np.mean(R3_o),np.mean(R4_o)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        R_o=A[i]\n",
    "A=[RMSE2_o,RMSE3_o,RMSE4_o]\n",
    "B=[np.mean(RMSE2_o),np.mean(RMSE3_o),np.mean(RMSE4_o)]\n",
    "for i in range(3):\n",
    "    if B[i]==min(B):\n",
    "        RMSE_o=A[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-qatar",
   "metadata": {},
   "source": [
    "# Nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Nov['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "#X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "#frame1=[data_dec3,data_jan3]#,data_feb\n",
    "#winter=pd.concat(frame1)\n",
    "#Winter=winter.resample('h').mean()\n",
    "#Winter=Winter.dropna()\n",
    "#X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Winter['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -2\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200]\n",
    "X_train3=X_Data[:350]\n",
    "X_train4=X_Data[:550]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200]\n",
    "y_train3=y_Data[:350]\n",
    "y_train4=y_Data[:550]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "er2_n=[]\n",
    "er3_n=[]\n",
    "er4_n=[]\n",
    "RMSE2_n=[]\n",
    "RMSE3_n=[]\n",
    "RMSE4_n=[]\n",
    "R2_n=[]\n",
    "R3_n=[]\n",
    "R4_n=[]\n",
    "U4_n=[]\n",
    "for i in KL:\n",
    "    model2=regressor.fit(X_train2[:int(0.1*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.1*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.1*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.1*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.1*i*X_train2.shape[0]):],\n",
    "                                               pred2)),1)/np.mean(y))\n",
    "    #RMSE2=(round(MAE(y_train2[int(0.02*i*X_train2.shape[0]):], pred2),2)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.1*i*X_train2.shape[0]):], pred2), 2)\n",
    "    RMSE2_n.append(RMSE2)\n",
    "    R2_n.append(r2)\n",
    "    er2_n.append(rmse2)\n",
    "for i in KL:\n",
    "    model3=regressor.fit(X_train3[:int(0.1*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.1*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.1*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.1*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.1*i*X_train3.shape[0]):], \n",
    "                                               pred3)),1)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.1*i*X_train3.shape[0]):], pred3), 2)\n",
    "    #RMSE3=(round(MAE(y_train3[int(0.02*i*X_train3.shape[0]):], pred3),2)/np.mean(y))\n",
    "    RMSE3_n.append(RMSE3)\n",
    "    R3_n.append(r3)\n",
    "    er3_n.append(rmse3)\n",
    "for i in KL:\n",
    "    model4=regressor.fit(X_train4[:int(0.1*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.1*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.1*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.1*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.1*i*X_train4.shape[0]):], \n",
    "                                               pred4)),1)/np.mean(y))\n",
    "    #RMSE4=(round(MAE(y_train4[int(0.02*i*X_train4.shape[0]):], pred4),2)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.1*i*X_train4.shape[0]):], pred4), 2)\n",
    "    u4=REF(pred4,y_train4[int(0.1*i*X_train4.shape[0]):],1.4)\n",
    "    U4_n.append(u4)\n",
    "    RMSE4_n.append(RMSE4)\n",
    "    R4_n.append(r4)\n",
    "    er4_n.append(rmse4)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "U4_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[er2_n,er3_n,er4_n]\n",
    "B=[np.mean(er2_n),np.mean(er3_n),np.mean(er4_n)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        er_n=A[i]\n",
    "\n",
    "A=[R2_n,R3_n,R4_n]\n",
    "B=[np.mean(R2_n),np.mean(R3_n),np.mean(R4_n)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        R_n=A[i]\n",
    "A=[RMSE2_n,RMSE3_n,RMSE4_n]\n",
    "B=[np.mean(RMSE2_n),np.mean(RMSE3_n),np.mean(RMSE4_n)]\n",
    "for i in range(3):\n",
    "    if B[i]==min(B):\n",
    "        RMSE_n=A[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-company",
   "metadata": {},
   "source": [
    "# Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Dec['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "#frame1=[data_mar3,data_apr3]\n",
    "#spring=pd.concat(frame1)\n",
    "#Spring=spring.resample('h').mean()\n",
    "#Spring=Spring.dropna()\n",
    "#X=Spring[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Spring['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -2\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200]\n",
    "X_train3=X_Data[:350]\n",
    "X_train4=X_Data[:550]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200]\n",
    "y_train3=y_Data[:350]\n",
    "y_train4=y_Data[:550]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "er2_d=[]\n",
    "er3_d=[]\n",
    "er4_d=[]\n",
    "RMSE2_d=[]\n",
    "RMSE3_d=[]\n",
    "RMSE4_d=[]\n",
    "R2_d=[]\n",
    "R3_d=[]\n",
    "R4_d=[]\n",
    "U4_d=[]\n",
    "for i in KL:\n",
    "    model2=regressor.fit(X_train2[:int(0.1*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.1*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.1*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.1*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.1*i*X_train2.shape[0]):], \n",
    "                                               pred2)),1)/np.mean(y))\n",
    "    #RMSE2=(round(MAE(y_train2[int(0.02*i*X_train2.shape[0]):], pred2),2)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.1*i*X_train2.shape[0]):], pred2), 2)\n",
    "    RMSE2_d.append(RMSE2)\n",
    "    R2_d.append(r2)\n",
    "    er2_d.append(rmse2)\n",
    "for i in KL:\n",
    "    model3=regressor.fit(X_train3[:int(0.1*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.1*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.1*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.1*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.1*i*X_train3.shape[0]):], \n",
    "                                               pred3)),1)/np.mean(y))\n",
    "    #RMSE3=(round(MAE(y_train3[int(0.02*i*X_train3.shape[0]):], pred3),2)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.1*i*X_train3.shape[0]):], pred3), 2)\n",
    "    RMSE3_d.append(RMSE3)\n",
    "    R3_d.append(r3)\n",
    "    er3_d.append(rmse3)\n",
    "for i in KL:\n",
    "    model4=regressor.fit(X_train4[:int(0.1*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.1*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.1*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.1*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.1*i*X_train4.shape[0]):], \n",
    "                                               pred4)),1)/np.mean(y))\n",
    "    #RMSE4=(round(MAE(y_train4[int(0.02*i*X_train4.shape[0]):], pred4),2)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.1*i*X_train4.shape[0]):], pred4), 2)\n",
    "    u4=REF(pred4,y_train4[int(0.1*i*X_train4.shape[0]):],1.4)\n",
    "    U4_d.append(u4)\n",
    "    RMSE4_d.append(RMSE4)\n",
    "    R4_d.append(r4)\n",
    "    er4_d.append(rmse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[er2_d,er3_d,er4_d]\n",
    "B=[np.mean(er2_d),np.mean(er3_d),np.mean(er4_d)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        er_d=A[i]\n",
    "\n",
    "A=[R2_d,R3_d,R4_d]\n",
    "B=[np.mean(R2_d),np.mean(R3_d),np.mean(R4_d)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        R_d=A[i]\n",
    "A=[RMSE2_d,RMSE3_d,RMSE4_d]\n",
    "B=[np.mean(RMSE2_d),np.mean(RMSE3_d),np.mean(RMSE4_d)]\n",
    "for i in range(3):\n",
    "    if B[i]==min(B):\n",
    "        RMSE_d=A[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-bradley",
   "metadata": {},
   "source": [
    "# Jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Jan[['Net Signal','Temp','RH','Month','Day_of_week','Hour']]\n",
    "y=data_Jan['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200]\n",
    "X_train3=X_Data[:350]\n",
    "X_train4=X_Data[:550]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200]\n",
    "y_train3=y_Data[:350]\n",
    "y_train4=y_Data[:550]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "er2_j=[]\n",
    "er3_j=[]\n",
    "er4_j=[]\n",
    "RMSE2_j=[]\n",
    "RMSE3_j=[]\n",
    "RMSE4_j=[]\n",
    "R2_j=[]\n",
    "R3_j=[]\n",
    "R4_j=[]\n",
    "U4_j=[]\n",
    "for i in KL:\n",
    "    model2=regressor.fit(X_train2[:int(0.1*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.1*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.1*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.1*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.1*i*X_train2.shape[0]):], \n",
    "                                               pred2)),1)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.1*i*X_train2.shape[0]):], pred2), 2)\n",
    "    RMSE2_j.append(RMSE2)\n",
    "    R2_j.append(r2)\n",
    "    er2_j.append(rmse2)\n",
    "for i in KL:\n",
    "    model3=regressor.fit(X_train3[:int(0.1*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.1*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.1*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.1*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.1*i*X_train3.shape[0]):], \n",
    "                                               pred3)),1)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.1*i*X_train3.shape[0]):], pred3), 2)\n",
    "    RMSE3_j.append(RMSE3)\n",
    "    R3_j.append(r3)\n",
    "    er3_j.append(rmse3)\n",
    "for i in KL:\n",
    "    model4=regressor.fit(X_train4[:int(0.1*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.1*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.1*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.1*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.1*i*X_train4.shape[0]):],\n",
    "                                               pred4)),1)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.1*i*X_train4.shape[0]):], pred4), 2)\n",
    "    u4=REF(pred4,y_train4[int(0.1*i*X_train4.shape[0]):],1.4)\n",
    "    U4_j.append(u4)\n",
    "    RMSE4_j.append(RMSE4)\n",
    "    R4_j.append(r4)\n",
    "    er4_j.append(rmse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[er2_j,er3_j,er4_j]\n",
    "B=[np.mean(er2_j),np.mean(er3_j),np.mean(er4_j)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        er_j=A[i]\n",
    "\n",
    "A=[R2_j,R3_j,R4_j]\n",
    "B=[np.mean(R2_j),np.mean(R3_j),np.mean(R4_j)]\n",
    "for i in range(3):\n",
    "    if B[i]==max(B):\n",
    "        R_j=A[i]\n",
    "A=[RMSE2_j,RMSE3_j,RMSE4_j]\n",
    "B=[np.mean(RMSE2_j),np.mean(RMSE3_j),np.mean(RMSE4_j)]\n",
    "for i in range(3):\n",
    "    if B[i]==min(B):\n",
    "        RMSE_j=A[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[er4_o[i],er4_n[i],er4_d[i],er4_j[i],er2_o[i],er2_n[i],er2_d[i],er2_j[i],\n",
    "         er3_o[i],er3_n[i],er3_d[i],er3_j[i]] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[R4_o[i],R4_n[i],R4_d[i],R4_j[i],R2_o[i],R2_n[i],R2_d[i],R2_j[i],\n",
    "         R3_o[i],R3_n[i],R3_d[i],R3_j[i]] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3 =[[RMSE2_o[i],RMSE2_n[i],RMSE2_d[i],RMSE2_j[i],RMSE3_o[i],RMSE3_n[i],RMSE3_d[i],RMSE3_j[i],\n",
    "         #RMSE4_o[i],RMSE4_n[i],RMSE4_d[i],RMSE4_j[i]] for i in range(40)]\n",
    "data3 =[[RMSE4_o[i],RMSE4_n[i],RMSE4_d[i],RMSE4_j[i]] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=[[U4_o[i],U4_n[i],U4_d[i],U4_j[i]] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean3=[]\n",
    "for i in range(len(data3)):\n",
    "    corr3=np.mean(data3[i])\n",
    "    Corr_mean3.append(np.round(corr3,2))\n",
    "    \n",
    "Corr_mean3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['10','20', '30', '40', '50', '60','70','80']\n",
    "students =Corr_mean3 \n",
    "graph=ax.bar(langs,students, color='#6495ED')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.yticks(np.arange(0,0.5, step=0.1))\n",
    "plt.ylabel('NRMSE', fontsize=20)\n",
    "plt.xlabel('Fraction of training data (%)', fontsize=20)\n",
    "plt.setp(ax.spines.values(), linewidth=1.4)\n",
    "#plt.title(r\"$O_3$\",fontsize=18)\n",
    "plt.savefig(\"CS_RMSE.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_mean=[]\n",
    "for i in range(len(data4)):\n",
    "    corr3=np.mean(data4[i])\n",
    "    U_mean.append(np.round(corr3))\n",
    "    \n",
    "U_mean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs= ['10','20', '30', '40', '50', '60','70','80']\n",
    "students=U_mean \n",
    "graph=ax.bar(langs,students, color='#6495ED')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.yticks(np.arange(0,80, step=10))\n",
    "plt.ylabel('REU (%)', fontsize=20)\n",
    "plt.xlabel('Fraction of training data (%)', fontsize=20)\n",
    "plt.setp(ax.spines.values(), linewidth=1.4)\n",
    "#plt.title(r\"$O_3$\",fontsize=18)\n",
    "plt.savefig(\"CS_REU.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diff3=abs(((Corr_mean3[4]-np.array(Corr_mean3)))*100)\n",
    "A=np.array(Diff3)\n",
    "A=np.round(A)[:-1]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_mean=[]\n",
    "for i in range(len(data)):\n",
    "    u=np.mean(data4[i])\n",
    "    u_mean.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "Corr_mean3=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    corr2=np.mean(data2[i])\n",
    "    corr3=np.mean(data3[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "    Corr_mean3.append(corr3)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff3=abs(((Corr_mean3[39]-np.array(Corr_mean3))/Corr_mean3[39])*100)\n",
    "print(Diff3)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "Diff3=list(Diff3[:5]+5.5*(moving_avg(Diff3,6)[0]-Diff3[4]))+list(moving_avg(Diff3,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(6.5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[10 for i in range(len(Y_Test))]\n",
    "y4=[20 for i in range(len(Y_Test))]\n",
    "y6=[30 for i in range(len(Y_Test))]\n",
    "y8=[40 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.hlines([2], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([4], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([6], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([8], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([10], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "m1,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"d\",markersize=9, alpha=1)\n",
    "m2,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "#plt.plot(Diff2[:-1],A[:-1], color='#CD5B45',marker=\"d\",markersize=13, alpha=0.9)\n",
    "m3,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"d\",markersize=9, alpha=1)\n",
    "m4,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "lgnd = ax.legend([ (m1,m2),(m3,m4)], ['r','RMSE'],loc = 2, bbox_to_anchor = (0.4,1), fontsize=13)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0,15)\n",
    "ax.set_xlim(right=40)\n",
    "ax.set_ylim(bottom=-0.2)\n",
    "plt.yticks(np.arange(0,15, step=2))\n",
    "plt.xticks(np.arange(0,40, step=5))\n",
    "ax.set_xticks([0,5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['0','10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.ylabel('Change in performance (%)',fontsize=20)\n",
    "plt.yticks([2,4,6,8,10,12,14])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "#plt.xlabel('Tolerance, Tc (%)',fontsize=20)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.text(0.2,2, 'Tc=2%',fontsize=17)\n",
    "plt.text(0.2,4, 'Tc=4%',fontsize=17)\n",
    "plt.text(0.2,6, 'Tc=6%',fontsize=17)\n",
    "plt.text(0.2,8, 'Tc=8%',fontsize=17)\n",
    "plt.text(0.2,10, 'Tc=10%',fontsize=17)\n",
    "textstr = 'CO-Monthly'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.735, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.savefig(\"CS_CO_M1.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array(Diff3)\n",
    "A=np.round(A,3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "Corr_mean3=[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    corr2=np.mean(data2[i])\n",
    "    corr3=np.mean(data3[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "    Corr_mean3.append(corr3)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>2 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=2 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1[1],Corr_mean1[1], marker=\"d\",s=100,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2[-1],Corr_mean2[-1], marker=\"d\",s=100,color='teal', alpha=0.9)\n",
    "plt.legend(['>Tc','≤Tc'],loc = 2, bbox_to_anchor = (0.64,0.29), fontsize=16)\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,0.91, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=2%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M2_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>4 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=4 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,0.91, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=4%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M4_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>6 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=6 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,0.91, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr ='CO (Tc=6%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M6_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>8 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=8 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,0.91, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=8%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M8_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>10 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=10 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,0.91, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=10%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M10_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-intranet",
   "metadata": {},
   "source": [
    "Data=[]\n",
    "for i in range(len(data)):\n",
    "    data_mean, data_std = np.mean(data[i]), np.std(data[i])\n",
    "    cut_off = data_std * 1.9\n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "    outliers_removed = [x for x in data[i] if x > lower and x < upper]\n",
    "    Data.append(outliers_removed )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-combination",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# Creating dataset\n",
    "np.random.seed(10)\n",
    "#data_1 =[Av1[10],Av3[10],Av5[10]]\n",
    "#data_2 =[Av1[20],Av3[20],Av5[20]]\n",
    "#data_3 =[Av1[30],Av3[30],Av5[30]]\n",
    "#data_4 =[Av1[40],Av3[40],Av5[40]]\n",
    "#data = [[er4_o[i],er4_n[i],er4_d[i],er4_j[i],er2_o[i],er2_n[i],er2_d[i],er2_j[i],\n",
    "         #er3_o[i],er3_n[i],er3_d[i],er3_j[i]] for i in range(18)]\n",
    "#data2= [[Av2[i],Av4[i],Av6[i]] for i in range(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-ceiling",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "data=Data\n",
    "\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "colors= ['#00688B' for i in range(40)]\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "for median in bp['medians']:\n",
    "    median.set(color ='green',\n",
    "               linewidth = 1)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([2,4,6,8,10,12,14,16])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'])\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.savefig(\"CS_M_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-symbol",
   "metadata": {},
   "source": [
    "from scipy.signal import savgol_filter\n",
    "er21= savgol_filter(er2, 3, 2)\n",
    "er31= savgol_filter(er3, 3, 2)\n",
    "er41= savgol_filter(er4, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-following",
   "metadata": {},
   "source": [
    "perc=[5*i for i in range(1,19)]\n",
    "#plt.plot(perc, er1, color='#0000FF',linewidth=2.5)\n",
    "plt.plot(perc, er2, color='#00FFFF',linewidth=2.5)\n",
    "plt.plot(perc, er31, color='#E3CF57',linewidth=2.5)\n",
    "plt.plot(perc, er41, color='#00008B',linewidth=2.5)\n",
    "#plt.plot(perc, er51, color='#CD3333',linewidth=2.5)\n",
    "#plt.plot(perc, er61, color='#9932CC',linewidth=2.5)\n",
    "plt.ylabel('Pearson correlation (r)', fontsize=18)\n",
    "plt.xlabel('Training data (%)', fontsize=18)\n",
    "plt.xlim([0, 80])\n",
    "plt.ylim([0.7, 1])\n",
    "legend=plt.legend(['1000','1500','2000','2500','3000'],fontsize=12,loc = 2, bbox_to_anchor = (0.65,0.6))\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(r\"$O_3$\", fontsize=14)\n",
    "legend.set_title(\"No. data points\", prop = {'size':13})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-reaction",
   "metadata": {},
   "source": [
    "# CO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-poultry",
   "metadata": {},
   "source": [
    "#X=data_Oct3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Oct3['Ref']\n",
    "frame1=[data_oct,data_nov]\n",
    "fall=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall.resample('h').mean()\n",
    "Fall=Fall.dropna()\n",
    "X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Fall['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=51)\n",
    "#Fall=Fall.sample(frac=1)\n",
    "P1=[]\n",
    "B1=[]\n",
    "corr1=[]\n",
    "RMSE1=[]\n",
    "for i in range(1,19):\n",
    "    regressor.fit(X_train[:int(0.05*i*X_train2.shape[0])], y_train[:int(0.05*i*X_train2.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.05*i*X_train.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.05*i*X_train.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.05*i*X_train.shape[0]):])\n",
    "    rmse1=round(np.corrcoef(y_train[int(0.05*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    P1.append(p1)\n",
    "    B1.append(b1)\n",
    "    corr1.append(rmse1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Oct3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Oct3['Ref']\n",
    "frame1=[data_oct,data_nov]\n",
    "fall=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall.resample('60min').mean()\n",
    "Fall=Fall.dropna()\n",
    "X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Fall['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "#Fall=Fall.sample(frac=1)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "P11=[]\n",
    "B11=[]\n",
    "corr11=[]\n",
    "RMSE11=[]\n",
    "R11=[]\n",
    "U11=[]\n",
    "for i in KL:\n",
    "    regressor.fit(X_train[:int(0.1*i*X_train.shape[0])], y_train[:int(0.1*i*X_train.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.1*i*X_train.shape[0]):])\n",
    "    #p1=precision(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    #b1=bias(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    rmse11=round(np.corrcoef(y_train[int(0.1*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    r=round(sm.r2_score(y_train[int(0.1*i*X_train.shape[0]):], pred), 2)\n",
    "    R11.append(r)\n",
    "    RMSE=(round(np.sqrt(sm.mean_squared_error(y_train[int(0.1*i*X_train.shape[0]):], pred)),1)/\n",
    "          np.mean(y))\n",
    "    #RMSE=(round(MAE(y_train[int(0.02*i*X_train.shape[0]):], pred),2)/np.mean(y))\n",
    "    u11=REF(pred,y_train[int(0.1*i*X_train.shape[0]):],1.4)\n",
    "    U11.append(u11)\n",
    "    RMSE11.append(RMSE)\n",
    "    \n",
    "    #P11.append(p1)\n",
    "    #B11.append(b1)\n",
    "    corr11.append(rmse11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "U11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-abuse",
   "metadata": {},
   "source": [
    "#X=data_Nov3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Nov3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=71)\n",
    "frame1=[data_dec,data_jan]#,data_feb\n",
    "winter=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter=winter.resample('h').mean()\n",
    "Winter=Winter.dropna()\n",
    "#Winter=Winter.sample(frac=1)\n",
    "X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Winter['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=81)\n",
    "P2=[]\n",
    "B2=[]\n",
    "corr2=[]\n",
    "for i in range(1,19):\n",
    "    regressor.fit(X_train[:int(0.05*i*X_train2.shape[0])], y_train[:int(0.05*i*X_train2.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train[int(0.05*i*X_train2.shape[0]):], pred)[0, 1],2)\n",
    "    P2.append(p1)\n",
    "    B2.append(b1)\n",
    "    corr2.append(rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Nov3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Nov3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,random_state=91)\n",
    "frame1=[data_dec,data_jan]#,data_feb\n",
    "winter=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter=winter.resample('60min').mean()\n",
    "Winter=Winter.dropna()\n",
    "#Winter=Winter.sample(frac=1)\n",
    "X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Winter['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "P21=[]\n",
    "B21=[]\n",
    "corr21=[]\n",
    "R21=[]\n",
    "RMSE21=[]\n",
    "U21=[]\n",
    "for i in KL:\n",
    "    regressor.fit(X_train[:int(0.1*i*X_train.shape[0])], y_train[:int(0.1*i*X_train.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.1*i*X_train.shape[0]):])\n",
    "    #p1=precision(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    #b1=bias(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    rmse21=round(np.corrcoef(y_train[int(0.1*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    r=round(sm.r2_score(y_train[int(0.1*i*X_train.shape[0]):], pred), 2)\n",
    "    RMSE=(round(np.sqrt(sm.mean_squared_error(y_train[int(0.1*i*X_train.shape[0]):], pred)),1)/np.mean(y))\n",
    "    #RMSE=(round(MAE(y_train[int(0.02*i*X_train.shape[0]):], pred),2)/np.mean(y))\n",
    "    u21=REF(pred,y_train[int(0.1*i*X_train.shape[0]):],1.4)\n",
    "    U21.append(u21)\n",
    "    RMSE21.append(RMSE)\n",
    "    R21.append(r)\n",
    "    corr21.append(rmse21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "U21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-collectible",
   "metadata": {},
   "source": [
    "#X=data_Dec3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Dec3['Ref']\n",
    "frame1=[data_mar,data_apr]\n",
    "spring=pd.concat(frame1)\n",
    "Spring=spring.resample('h').mean()\n",
    "Spring=Spring.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Spring['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=111)\n",
    "P3=[]\n",
    "B3=[]\n",
    "corr3=[]\n",
    "for i in range(1,19):\n",
    "    regressor.fit(X_train[:int(0.05*i*X_train2.shape[0])], y_train[:int(0.05*i*X_train2.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train[int(0.05*i*X_train2.shape[0]):], pred)[0, 1],2)\n",
    "    P3.append(p1)\n",
    "    B3.append(b1)\n",
    "    corr3.append(rmse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Dec3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Dec3['Ref']\n",
    "frame1=[data_mar,data_apr]\n",
    "spring=pd.concat(frame1)\n",
    "Spring=spring.resample('60min').mean()\n",
    "Spring=Spring.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Spring['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "P31=[]\n",
    "B31=[]\n",
    "corr31=[]\n",
    "R31=[]\n",
    "RMSE31=[]\n",
    "U31=[]\n",
    "for i in KL:\n",
    "    regressor.fit(X_train[:int(0.1*i*X_train.shape[0])], y_train[:int(0.1*i*X_train.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.1*i*X_train.shape[0]):])\n",
    "    #p1=precision(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    #b1=bias(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    rmse31=round(np.corrcoef(y_train[int(0.1*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    r=round(sm.r2_score(y_train[int(0.1*i*X_train.shape[0]):], pred), 2)\n",
    "    RMSE=(round(np.sqrt(sm.mean_squared_error(y_train[int(0.1*i*X_train.shape[0]):], pred)),1)/np.mean(y))\n",
    "    #RMSE=(round(MAE(y_train[int(0.02*i*X_train.shape[0]):], pred),2)/np.mean(y))\n",
    "    u31=REF(pred,y_train[int(0.1*i*X_train.shape[0]):],1.4)\n",
    "    U31.append(u31)\n",
    "    RMSE31.append(RMSE)\n",
    "    R31.append(r)\n",
    "    \n",
    "    #P31.append(p1)\n",
    "    #B31.append(b1)\n",
    "    corr31.append(rmse31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "U31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-gnome",
   "metadata": {},
   "source": [
    "plt.plot(ind,D,marker='d',markersize=10, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = [[corr11[i],corr21[i],corr31[i]] for i in range(8)]\n",
    "Data2=[[R11[i],R21[i],R31[i]] for i in range(8)]\n",
    "Data3=[[RMSE11[i],RMSE21[i],RMSE31[i]] for i in range(8)]\n",
    "Data4=[[U11[i],U21[i],U31[i]] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean3=[]\n",
    "for i in range(len(Data3)):\n",
    "    corr3=np.mean(Data3[i])\n",
    "    Corr_mean3.append(np.round(corr3,2))\n",
    "    \n",
    "Corr_mean3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_mean=[]\n",
    "for i in range(len(Data4)):\n",
    "    corr3=np.mean(Data4[i])\n",
    "    U_mean.append(np.round(corr3))\n",
    "    \n",
    "U_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "Corr_mean3=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    corr2=np.mean(Data2[i])\n",
    "    corr3=np.mean(Data3[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "    Corr_mean3.append(corr3)\n",
    "np.round(Corr_mean3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-anxiety",
   "metadata": {},
   "source": [
    "# 6months Calibration Scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Dec3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Dec3['Ref']\n",
    "frame1=[data_oct,data_nov,data_dec,data_jan,data_feb,data_mar]\n",
    "spring=pd.concat(frame1)\n",
    "Spring=spring.resample('60min').mean()\n",
    "Spring=Spring.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Spring['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "P31=[]\n",
    "B31=[]\n",
    "corr31=[]\n",
    "R31=[]\n",
    "RMSE41=[]\n",
    "U41=[]\n",
    "for i in KL:\n",
    "    regressor.fit(X_train[:int(0.1*i*X_train.shape[0])], y_train[:int(0.1*i*X_train.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.1*i*X_train.shape[0]):])\n",
    "    #p1=precision(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    #b1=bias(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    rmse31=round(np.corrcoef(y_train[int(0.1*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    r=round(sm.r2_score(y_train[int(0.1*i*X_train.shape[0]):], pred), 2)\n",
    "    RMSE=(round(np.sqrt(sm.mean_squared_error(y_train[int(0.1*i*X_train.shape[0]):], pred)),1)/np.mean(y))\n",
    "    #RMSE=(round(MAE(y_train[int(0.02*i*X_train.shape[0]):], pred),2)/np.mean(y))\n",
    "    u31=REF(pred,y_train[int(0.1*i*X_train.shape[0]):],1.4)\n",
    "    U41.append(u31)\n",
    "    RMSE41.append(RMSE)\n",
    "    R31.append(r)\n",
    "    \n",
    "    #P31.append(p1)\n",
    "    #B31.append(b1)\n",
    "    corr31.append(rmse31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "U41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "Corr_mean3=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    corr2=np.mean(Data2[i])\n",
    "    corr3=np.mean(Data3[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "    Corr_mean3.append(corr3)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff3=abs(((Corr_mean3[39]-np.array(Corr_mean3)))*100)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "Diff3=list(Diff3[:5]+4.5*(moving_avg(Diff3,6)[0]-Diff3[4]))+list(moving_avg(Diff3,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(6.5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.hlines([2], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([4], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([6], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([8], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([10], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "m1,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"d\",markersize=9, alpha=1)\n",
    "m2,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "#plt.plot(Diff2[:-1],A[:-1], color='#CD5B45',marker=\"d\",markersize=13, alpha=0.9)\n",
    "m3,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"d\",markersize=9, alpha=1)\n",
    "m4,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0,15)\n",
    "ax.set_xlim(right=40)\n",
    "ax.set_ylim(bottom=-0.2)\n",
    "plt.yticks(np.arange(0,15, step=2))\n",
    "plt.xticks(np.arange(0,40, step=5))\n",
    "ax.set_xticks([0,5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['0','10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.ylabel('Change in performance (%)',fontsize=20)\n",
    "plt.yticks([2,4,6,8,10,12,14])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "#plt.xlabel('Tolerance, Tc (%)',fontsize=20)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "#plt.text(0.2,2, 'Tc=2',fontsize=14)\n",
    "#plt.text(0.2,4, 'Tc=4',fontsize=14)\n",
    "#plt.text(0.2,6, 'Tc=6',fontsize=14)\n",
    "#plt.text(0.2,8, 'Tc=8',fontsize=14)\n",
    "#plt.text(0.2,10, 'Tc=10',fontsize=14)\n",
    "textstr = 'CO-Seasonal'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.713, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.savefig(\"CS_CO_S1.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array(Diff3)\n",
    "A=np.round(A,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    corr2=np.mean(Data2[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff3=abs(((max(Corr_mean3)-np.array(Corr_mean3))/min(Corr_mean3))*100)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=50,color='darkgoldenrod', alpha=1)\n",
    "plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=50,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.68,0.8), fontsize=16)\n",
    "plt.fill_between(np.array(Y_Test), y1, y2, color='teal', alpha=0.4)\n",
    "plt.fill_between(np.array(Y_Test), y1, y4, color='teal', alpha=0.35)\n",
    "plt.fill_between(np.array(Y_Test), y1, y6, color='teal', alpha=0.3)\n",
    "plt.fill_between(np.array(Y_Test), y1, y8, color='teal', alpha=0.25)\n",
    "plt.fill_between(np.array(Y_Test), y1, y10, color='teal', alpha=0.2)\n",
    "plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=120,color='darkgoldenrod', alpha=1)\n",
    "plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=120,color='#CD5B45', alpha=0.9)\n",
    "plt.scatter(A[:-1],Diff[:-1], s=3,color='black')\n",
    "plt.scatter(A[:-1],Diff2[:-1], s=3,color='black')\n",
    "plt.plot(A[:-1],Diff[:-1], color='darkgoldenrod', alpha=1)\n",
    "plt.plot(A[:-1],Diff2[:-1], color='#CD5B45', alpha=0.9)\n",
    "#plt.scatter(A,Diff3, marker=\"d\",s=500,color='orange', alpha=1)\n",
    "#plt.scatter(A,Diff3, s=5,color='black')\n",
    "#plt.plot(A,Diff2,color='darkred', alpha=0.9)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,15)\n",
    "plt.ylim(0, 20)\n",
    "ax.set_xlim(right=15.5)\n",
    "ax.set_ylim(bottom=-0.5)\n",
    "plt.xticks(np.arange(0,16, step=2))\n",
    "plt.yticks(np.arange(0,20, step=5))\n",
    "ax.set_xticks([5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.yticks([4,8,12,16,20])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Difference (%)',fontsize=20)\n",
    "plt.text(0.2,1.5, 'Tc=2',fontsize=14)\n",
    "plt.text(0.2,3.5, 'Tc=4',fontsize=14)\n",
    "plt.text(0.2,5.5, 'Tc=6',fontsize=14)\n",
    "plt.text(0.2,7.5, 'Tc=8',fontsize=14)\n",
    "plt.text(0.2,9.5, 'Tc=10',fontsize=14)\n",
    "textstr = 'CO-Seasonal'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.73, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.savefig(\"CS_CO_S.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=(max(Corr_mean)-np.array(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>2 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=2 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(ind1[1],Corr_mean1[1], marker=\"d\",s=100,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2[-1],Corr_mean2[-1], marker=\"d\",s=100,color='teal', alpha=0.9)\n",
    "plt.legend(['>Tc','≤Tc'],loc = 2, bbox_to_anchor = (0.64,0.29), fontsize=16)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.5,0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,.95, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=2%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S2_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    Corr_mean.append(corr)\n",
    "Diff=abs(((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100)\n",
    "A=[i for i in range(16)if i%2==0]\n",
    "Diff2=[Diff[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,17,1)]\n",
    "y1=[2 for i in range(len(Y_Test))]\n",
    "y2=[0 for i in range(len(Y_Test))]\n",
    "fig= plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(111)\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.fill_between(np.array(Y_Test), y1, y2, color='teal', alpha=0.4)\n",
    "plt.fill_between(np.array(Y_Test), y1, y4, color='teal', alpha=0.35)\n",
    "plt.fill_between(np.array(Y_Test), y1, y6, color='teal', alpha=0.3)\n",
    "plt.fill_between(np.array(Y_Test), y1, y8, color='teal', alpha=0.25)\n",
    "plt.fill_between(np.array(Y_Test), y1, y10, color='teal', alpha=0.2)\n",
    "plt.scatter(A,Diff2, marker=\"d\",s=500,color='darkgoldenrod', alpha=1)\n",
    "plt.scatter(A,Diff2, s=5,color='black')\n",
    "#plt.plot(A,Diff2,color='darkred', alpha=0.9)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,16)\n",
    "plt.ylim(0, 30)\n",
    "ax.set_xlim(right=16)\n",
    "ax.set_ylim(bottom=-0.5)\n",
    "plt.xticks(np.arange(0,16, step=2))\n",
    "plt.yticks(np.arange(0,20, step=5))\n",
    "ax.set_xticks([2,4,6,8,10,12,14,16])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70',''],fontsize=16)\n",
    "plt.yticks([4,8,12,16,20,24,28])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Difference in correlation (%)',fontsize=20)\n",
    "plt.text(0.2,1.5, 'Tc=2',fontsize=14)\n",
    "plt.text(0.2,3.5, 'Tc=4',fontsize=14)\n",
    "plt.text(0.2,5.5, 'Tc=6',fontsize=14)\n",
    "plt.text(0.2,7.5, 'Tc=8',fontsize=14)\n",
    "plt.text(0.2,9.5, 'Tc=10',fontsize=14)\n",
    "textstr = 'CO-Seasonal'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.74, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.savefig(\"CS_CO_S.pdf\",format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=(max(Corr_mean)-np.array(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>4 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=4 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.5,0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,.95, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=4%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S4_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=(max(Corr_mean)-np.array(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>6 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=6 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.5,0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,.95, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=6%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S6_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=(max(Corr_mean)-np.array(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>8 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=8 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.5,0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,.95, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=8%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S8_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_mean=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=(max(Corr_mean)-np.array(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>10 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=10 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.5,0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,.95, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'CO (Tc=10%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S10_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "data=Data\n",
    "\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "colors= ['#00688B' for i in range(40)]\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "for median in bp['medians']:\n",
    "    median.set(color ='green',\n",
    "               linewidth = 1)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([2,4,6,8,10,12,14,16])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'])\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.savefig(\"CS_S_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-sympathy",
   "metadata": {},
   "source": [
    "# Bias and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-pharmacology",
   "metadata": {},
   "source": [
    "# October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Oct[['Net Signal','Temp','RH','Month','Day_of_week','Hour']]\n",
    "#y=data_Oct['Ref']\n",
    "frame1=[data_oct,data_nov]\n",
    "fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall.resample('60min').mean()\n",
    "Fall=Fall.dropna()\n",
    "X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Fall['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "fr=0.18\n",
    "\n",
    "B2_o=[]\n",
    "B3_o=[]\n",
    "B4_o=[]\n",
    "\n",
    "P2_o=[]\n",
    "P3_o=[]\n",
    "P4_o=[]\n",
    "model2=regressor.fit(X_train2[:int(fr*X_train2.shape[0])], \n",
    "                         y_train2[:int(fr*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(fr*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(fr*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(fr*X_train2.shape[0]):], pred2)\n",
    "B2_o.append(b)\n",
    "P2_o.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(fr*X_train3.shape[0])], \n",
    "                         y_train3[:int(fr*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(fr*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(fr*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(fr*X_train3.shape[0]):], pred3)\n",
    "B3_o.append(b)\n",
    "P3_o.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(fr*X_train4.shape[0])], \n",
    "                         y_train4[:int(fr*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(fr*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(fr*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(fr*X_train4.shape[0]):], pred4)\n",
    "B4_o.append(b)\n",
    "P4_o.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-assembly",
   "metadata": {},
   "source": [
    "# November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Nov[['Net Signal','Temp','RH','Month','Day_of_week','Hour']]\n",
    "#y=data_Nov['Ref']\n",
    "frame1=[data_dec,data_jan,data_feb]\n",
    "fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall.resample('60min').mean()\n",
    "Fall=Fall.dropna()\n",
    "X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Fall['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_n=[]\n",
    "B3_n=[]\n",
    "B4_n=[]\n",
    "\n",
    "P2_n=[]\n",
    "P3_n=[]\n",
    "P4_n=[]\n",
    "model2=regressor.fit(X_train2[:int(fr*X_train2.shape[0])], \n",
    "                         y_train2[:int(fr*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(fr*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(fr*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(fr*X_train2.shape[0]):], pred2)\n",
    "B2_n.append(b)\n",
    "P2_n.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(fr*X_train3.shape[0])], \n",
    "                         y_train3[:int(fr*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(fr*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(fr*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(fr*X_train3.shape[0]):], pred3)\n",
    "B3_n.append(b)\n",
    "P3_n.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(fr*X_train4.shape[0])], \n",
    "                         y_train4[:int(fr*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(fr*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(fr*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(fr*X_train4.shape[0]):], pred4)\n",
    "B4_n.append(b)\n",
    "P4_n.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-prototype",
   "metadata": {},
   "source": [
    "# December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Dec[['Net Signal','Temp','RH','Month','Day_of_week','Hour']]\n",
    "#y=data_Dec['Ref']\n",
    "frame1=[data_mar,data_apr]\n",
    "fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall.resample('60min').mean()\n",
    "Fall=Fall.dropna()\n",
    "X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Fall['Ref']\n",
    "X=scaler2.fit_transform(X)\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -2\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_d=[]\n",
    "B3_d=[]\n",
    "B4_d=[]\n",
    "\n",
    "P2_d=[]\n",
    "P3_d=[]\n",
    "P4_d=[]\n",
    "model2=regressor.fit(X_train2[:int(fr*X_train2.shape[0])], \n",
    "                         y_train2[:int(fr*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(fr*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(fr*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(fr*X_train2.shape[0]):], pred2)\n",
    "B2_d.append(b)\n",
    "P2_d.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(fr*X_train3.shape[0])], \n",
    "                         y_train3[:int(fr*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(fr*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(fr*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(fr*X_train3.shape[0]):], pred3)\n",
    "B3_d.append(b)\n",
    "P3_d.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(fr*X_train4.shape[0])], \n",
    "                         y_train4[:int(fr*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(fr*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(fr*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(fr*X_train4.shape[0]):], pred4)\n",
    "B4_d.append(b)\n",
    "P4_d.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-memphis",
   "metadata": {},
   "source": [
    "X=data_Jan[['Net Signal','Temp','RH','Month','Day_of_week','Hour']]\n",
    "y=data_Jan['Ref']\n",
    "#frame1=[data_mar,data_apr]\n",
    "#fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "#Fall=fall.resample('h').mean()\n",
    "#Fall=Fall.dropna()\n",
    "#X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "#y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_j=[]\n",
    "B3_j=[]\n",
    "B4_j=[]\n",
    "\n",
    "P2_j=[]\n",
    "P3_j=[]\n",
    "P4_j=[]\n",
    "model2=regressor.fit(X_train2[:int(0.4*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.4*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(0.4*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "B2_j.append(b)\n",
    "P2_j.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(0.4*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.4*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(0.4*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "B3_j.append(b)\n",
    "P3_j.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(0.4*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.4*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(0.4*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "B4_j.append(b)\n",
    "P4_j.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-negotiation",
   "metadata": {},
   "source": [
    "X=data_Jan[['Net Signal','Temp','RH','Month','Day_of_week','Hour']]\n",
    "y=data_Jan['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "regressor.fit(X_train[:int(0.4*X_train2.shape[0])], y_train[:int(0.4*X_train2.shape[0])])\n",
    "pred=regressor.predict(X_train[int(0.05*X_train2.shape[0]):])\n",
    "P4=precision(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "B4=bias(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "(P4,B4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-kidney",
   "metadata": {},
   "source": [
    "X=data_Jan3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Jan3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "regressor.fit(X_train[:int(0.4*X_train2.shape[0])], y_train[:int(0.4*X_train2.shape[0])])\n",
    "pred=regressor.predict(X_train[int(0.05*X_train2.shape[0]):])\n",
    "P41=precision(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "B41=bias(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "(P41,B41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=[B2_o[0],B3_o[0],B4_o[0],B2_n[0],B3_n[0],B4_n[0],B2_d[0],B3_d[0],B4_d[0]]\n",
    "B=np.array(B)\n",
    "B=np.round(B,2)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "P=[P2_o[0],P3_o[0],P4_o[0],P2_n[0],P3_n[0],P4_n[0],P2_d[0],P3_d[0],P4_d[0]]\n",
    "P=np.array(P)\n",
    "P=np.round(P,2)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=[B,P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# Creating dataset\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "data=Data\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "plt.axhline(y= 10, color = 'green',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 25, color = 'purple',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 30, color = 'orange',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 50, color = 'dodgerblue',label='RC', linestyle = ':',linewidth=2 )\n",
    "#plt.legend(['RC','SGS','IS','HA & CSP'],ncol=4, loc='lower left', fontsize=10)\n",
    "\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "colors= ['#6495ED','#8B3E2F']\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "\n",
    "#plt.xlabel('Training Data (%)',fontsize=20)\n",
    "plt.ylabel('Error (%)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "#plt.xticks(np.arange(0,8 , step=1))\n",
    "plt.yticks(np.arange(0,61, step=10)) \n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "plt.axhline(y= 10, color = 'green',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 25, color = 'purple',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 30, color = 'orange',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 50, color = 'dodgerblue',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.grid(linestyle='-.',linewidth=0)\n",
    "ax.set_xticks([1,2])\n",
    "ax.set_xticklabels(['Bias','Precision'])\n",
    "#plt.legend(['Bias','Precision'] ,fontsize=16)\n",
    "#plt.title(r\"$CO$\",fontsize=16 )\n",
    "plt.savefig(\"BP_S_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "data=Data\n",
    "\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "plt.axhline(y= 10, color = 'green',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 25, color = 'purple',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 30, color = 'orange',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 50, color = 'dodgerblue',label='RC', linestyle = ':',linewidth=2 )\n",
    "#plt.legend(['RC','SGS','IS','HA & CSP'],ncol=4, loc='lower left', fontsize=10)\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "#colors= ['#00688B' for i in range(2)]\n",
    "colors= ['#6495ED','#8B3E2F']\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "for median in bp['medians']:\n",
    "    median.set(color ='green',\n",
    "               linewidth = 1)\n",
    "\n",
    "plt.ylabel('Error (%)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "#plt.xticks(np.arange(0,5 , step=2))\n",
    "plt.yticks(np.arange(0,61, step=10))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,2])\n",
    "ax.set_xticklabels(['Bias','Precision'])\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.savefig(\"BP_S_CO.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,21)]\n",
    "Rmse1_rf=[]\n",
    "RMSE1_rf=[]\n",
    "REU1=[]\n",
    "REU2=[]\n",
    "lv=2000\n",
    "L_y1=[]\n",
    "A1=[]\n",
    "M1=[]\n",
    "Bias=[]\n",
    "L=[]\n",
    "KK=[]\n",
    "D1=[]\n",
    "Features1=[]\n",
    "P1=[]\n",
    "for i in range(1,10):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    y_test=y_train[48*i:]\n",
    "    #model=stacked_averaged_models.fit(X_train[:48*i].values, y_train[:48*i])\n",
    "    #pred =model.predict(X_train[48*i:].values)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    kk=round(np.corrcoef(y_train[48*i:], pred)[0, 1],2)\n",
    "    KK.append(kk)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    ind=[]\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.3,lv)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    #U=np.sqrt(np.mean((reu)**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    #b=(np.mean(pred)-np.mean(y_train[48*i:]))/np.mean(y_train[48*i:])\n",
    "    #bias=(abs(b)*np.mean(y_train[48*i:])/(1.67*U))\n",
    "    l=1-(U/np.std(y_train[48*i:]))**2\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    features=regressor.feature_importances_\n",
    "    Features1.append(features)\n",
    "    D1.append(d)\n",
    "    L.append(l)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M1.append(p)\n",
    "    #Bias.append(bias)\n",
    "    \n",
    "    \n",
    "    for i in range(len(reu)):\n",
    "        if reu[i]<30:\n",
    "            ind.append(i)\n",
    "    Rmse1_rf.append(RMSE)\n",
    "    RMSE1_rf.append(rmse)\n",
    "    REU1.append(reu)\n",
    "    L_y1.append(k)\n",
    "    REU2.append(round((len(ind)/len(reu))*100,2))\n",
    "    A1.append(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train[:480], y_train[:480])\n",
    "pred=regressor.predict(X_train[480:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv=max\n",
    "A=[i for i in np.arange(1,3,0.1)]\n",
    "K=[]\n",
    "for i in A:\n",
    "    lv=max(y_train[480:])\n",
    "    k=REF2(pred,y_train[480:],i,lv)\n",
    "    K.append(k)\n",
    "for i in range(len(K)):\n",
    "    if K[i]==min(K):\n",
    "        print(A[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(A,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Oct2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Oct2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,21) ]\n",
    "Rmse1_rf2=[]\n",
    "RMSE1_rf2=[]\n",
    "M12=[]\n",
    "for i in range(1,10):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M12.append(p)\n",
    "    Rmse1_rf2.append(mse)\n",
    "    RMSE1_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE1_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "M12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Oct3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Oct3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,21) ]\n",
    "Rmse1_rf3=[]\n",
    "RMSE1_rf3=[]\n",
    "M13=[]\n",
    "for i in range(1,9):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M13.append(p)\n",
    "    Rmse1_rf3.append(mse)\n",
    "    RMSE1_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE1_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "M13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-blues",
   "metadata": {},
   "source": [
    "# Nov 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-transmission",
   "metadata": {},
   "source": [
    "   #   RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov[['Net Signal','Temp1','RH1','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Nov['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum2=sum(A[:100])\n",
    "mean2=np.std(y)/np.mean(y)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf=[]\n",
    "RMSE2_rf=[]\n",
    "REU2=[]\n",
    "L_y2=[]\n",
    "A2=[]\n",
    "M2=[]\n",
    "D2=[]\n",
    "Features2=[]\n",
    "for i in range(1,11):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    reu=REF(pred,y_train[48*i:],1.3)\n",
    "    U=np.sqrt(np.mean((reu)**2))\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    #U=np.sqrt(np.mean((reu)**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    features=regressor.feature_importances_\n",
    "    Features2.append(features)\n",
    "    D2.append(d)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M2.append(p)\n",
    "    Rmse2_rf.append(RMSE)\n",
    "    RMSE2_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.3,lv)\n",
    "    A2.append(re)\n",
    "    REU2.append(reu)\n",
    "    L_y2.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE2_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Nov2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum2=sum(A[:100])\n",
    "mean2=np.std(y)/np.mean(y)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf2=[]\n",
    "RMSE2_rf2=[]\n",
    "M22=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M22.append(p)\n",
    "    Rmse2_rf2.append(mse)\n",
    "    RMSE2_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE2_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "M22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Nov3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf3=[]\n",
    "RMSE2_rf3=[]\n",
    "M23=[]\n",
    "for i in range(1,10):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M23.append(p)\n",
    "    Rmse2_rf3.append(mse)\n",
    "    RMSE2_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE2_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "M23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-conservation",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Dec['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum3=sum(A[:100])\n",
    "mean3=np.std(y)/np.mean(y)\n",
    "Rmse3_rf=[]\n",
    "RMSE3_rf=[]\n",
    "REU3=[]\n",
    "L_y3=[]\n",
    "A3=[]\n",
    "M3=[]\n",
    "for i in range(1,10):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    #lv=max(y_train[48*i:])\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    reu=REF(pred,y_train[48*i:],1.6)\n",
    "    #U=np.sqrt(np.mean((reu)**2))\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M3.append(p)\n",
    "    Rmse3_rf.append(RMSE)\n",
    "    RMSE3_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.3,lv)\n",
    "    A3.append(re)\n",
    "    REU3.append(reu)\n",
    "    L_y3.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE3_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[i for i in range(len(pred))]\n",
    "plt.plot(ind,pred)\n",
    "plt.plot(ind,y_train[432:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Dec2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum3=sum(A[:100])\n",
    "mean3=np.std(y)/np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf2=[]\n",
    "RMSE3_rf2=[]\n",
    "M32=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M32.append(p)\n",
    "    Rmse3_rf2.append(mse)\n",
    "    RMSE3_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE3_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "M32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Dec3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf3=[]\n",
    "RMSE3_rf3=[]\n",
    "M33=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M33.append(p)\n",
    "    Rmse3_rf3.append(mse)\n",
    "    RMSE3_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE3_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "M33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-seeker",
   "metadata": {},
   "source": [
    "# Jan 2020 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-emission",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import KFold\n",
    "X=data_Jan[['Net Signal','Temp1','RH1','Month','Day_of_week','Hour']]\n",
    "y=data_Jan['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=False)\n",
    "X_Train=[]\n",
    "X_Test=[]\n",
    "\n",
    "for i,j in kf5.split(X_train):\n",
    "    X_Train.append(X_train.iloc[i])\n",
    "    X_Test.append(X_train.iloc[j]) \n",
    "    \n",
    "y_Train=[]\n",
    "y_Test=[]\n",
    "\n",
    "for k,l in kf5.split(y_train):\n",
    "    y_Train.append(y_train.iloc[k])\n",
    "    y_Test.append(y_train.iloc[l]) \n",
    "#X_train=preprocessing.scale(X_train)\n",
    "#X_train=preprocessing.normalize(X_train)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum4=sum(A[:1000])\n",
    "mean4=np.std(y)/np.mean(y)\n",
    "Rmse4_rf=[]\n",
    "RMSE4_rf=[]\n",
    "REU4=[]\n",
    "L_y4=[]\n",
    "A4=[]\n",
    "M4=[]\n",
    "D4=[]\n",
    "P4=[]\n",
    "Features=[]\n",
    "for i in range(1,10):\n",
    "    #lv=max(y_train[48*i:])\n",
    "    k=y_Train[0][48*i:].to_list()\n",
    "    #regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    model=regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=model.predict(X_train[48*i:])\n",
    "    #mse=round(sMAE(y_Train[0][48*i:], pred),2)\n",
    "    #rmse=round(sm.r2_score(y_Train[0][48*i:], pred), 2)\n",
    "    #U=np.sqrt(np.mean((0.25*np.array(y_Train[0][48*i:]))**2))\n",
    "    #RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_Train[0][48*i:]))**2))\n",
    "    #m=RMSE/(1*U)\n",
    "    #d=IOA(y_Train[0][48*i:], pred)\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    #features=regressor.feature_importances_\n",
    "    #Features.append(features)\n",
    "    #D4.append(d)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M4.append(p)\n",
    "   \n",
    "    Rmse4_rf.append(RMSE)\n",
    "    #RMSE4_rf.append(rmse)\n",
    "    #reu=REF(pred,y_Train[0][48*i:],1)\n",
    "    #re=REF2(pred,y_Train[0][48*i:],20,lv)\n",
    "    #A4.append(re)\n",
    "    #REU4.append(reu)\n",
    "    #L_y4.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Jan2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Jan2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum4=sum(A[:100])\n",
    "mean4=np.std(y)/np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf2=[]\n",
    "RMSE4_rf2=[]\n",
    "M42=[]\n",
    "D42=[]\n",
    "for i in range(1,9):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #mse= mape=round(mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    D42.append(d)\n",
    "    M42.append(p)\n",
    "    Rmse4_rf2.append(RMSE)\n",
    "    RMSE4_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "M42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Jan3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Jan3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf3=[]\n",
    "RMSE4_rf3=[]\n",
    "M43=[]\n",
    "for i in range(1,10):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #mse= mape=round(mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M43.append(p)\n",
    "    Rmse4_rf3.append(mse)\n",
    "    RMSE4_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE4_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "M43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[[Rmse1_rf[i],Rmse2_rf[:9][i],Rmse3_rf[:9][i],Rmse4_rf[:9][i]] for i in range(9)]\n",
    "AV=[]\n",
    "for i in range(9):\n",
    "    AV.append(np.mean(A[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,10)]\n",
    "#plt.plot(Day,Rmse1_rf, color='red')\n",
    "#plt.plot(Day,Rmse2_rf[:9], color='blue')\n",
    "#plt.plot(Day,Rmse3_rf[:9], color='teal')\n",
    "plt.plot(Day,AV, color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-nursing",
   "metadata": {},
   "source": [
    "# Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-beads",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-conflict",
   "metadata": {},
   "source": [
    "X=data_Feb[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Feb['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-wells",
   "metadata": {},
   "source": [
    "Rmse5_rf2=[]\n",
    "RMSE5_rf2=[]\n",
    "M52=[]\n",
    "for i in range(1,7):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.15*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1.67*U)\n",
    "    M52.append(m)\n",
    "    Rmse5_rf2.append(mse)\n",
    "    RMSE5_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-commercial",
   "metadata": {},
   "source": [
    "A=y.to_list()\n",
    "Ext_feb=[]\n",
    "for i in range(len(A)):\n",
    "    if A[i]>3*np.mean(A):\n",
    "        Ext_feb.append(i)\n",
    "N_Ext_feb=len(Ext_feb)\n",
    "N_Ext_feb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-occasion",
   "metadata": {},
   "source": [
    "mean_feb=np.mean(y)\n",
    "N_feb=y.shape[0]\n",
    "Mean_Rmse_feb=np.mean(Rmse5_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-promise",
   "metadata": {},
   "source": [
    "# March 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-alberta",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-eleven",
   "metadata": {},
   "source": [
    "X=data_Mar[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Mar['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.0001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum6=sum(A[:100])\n",
    "mean6=np.std(y)/np.mean(y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-makeup",
   "metadata": {},
   "source": [
    "Rmse6_rf=[]\n",
    "RMSE6_rf=[]\n",
    "REU6=[]\n",
    "L_y6=[]\n",
    "A6=[]\n",
    "M5=[]\n",
    "S=[]\n",
    "D6=[]\n",
    "for i in range(1,11):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    #lv=max(y_train[48*i:])\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    s=(np.mean(y_train[48*i:]))/(np.array(y_train[48*i:]))\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    D6.append(d)\n",
    "    M5.append(m)\n",
    "    S.append(s)\n",
    "    Rmse6_rf.append(mse)\n",
    "    RMSE6_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.2,lv)\n",
    "    A6.append(re)\n",
    "    REU6.append(reu)\n",
    "    L_y6.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-regulation",
   "metadata": {},
   "source": [
    "REU_d1=list(REU1[0])+list(REU2[0])+list(REU3[0])+list(REU4[0])+list(REU6[0])\n",
    "REU_d2=list(REU1[1])+list(REU2[1])+list(REU3[1])+list(REU4[1])+list(REU6[1])\n",
    "REU_d3=list(REU1[2])+list(REU2[2])+list(REU3[2])+list(REU4[2])+list(REU6[2])\n",
    "REU_d4=list(REU1[3])+list(REU2[3])+list(REU3[3])+list(REU4[3])+list(REU6[3])\n",
    "REU_d5=list(REU1[4])+list(REU2[4])+list(REU3[4])+list(REU4[4])+list(REU6[4])\n",
    "REU_d6=list(REU1[5])+list(REU2[5])+list(REU3[5])+list(REU4[5])+list(REU6[5])\n",
    "REU_d7=list(REU1[6])+list(REU2[6])+list(REU3[6])+list(REU4[6])+list(REU6[6])\n",
    "REU_d8=list(REU1[7])+list(REU2[7])+list(REU3[7])+list(REU4[7])+list(REU6[7])\n",
    "REU_d9=list(REU1[8])+list(REU2[8])+list(REU3[8])+list(REU4[8])+list(REU6[8])\n",
    "REU_d10=list(REU1[9])+list(REU2[9])+list(REU3[9])+list(REU4[9])+list(REU6[9])\n",
    "\n",
    "REU=[REU_d1,REU_d2,REU_d3,REU_d4,REU_d5,REU_d6,REU_d7,REU_d8,REU_d9,REU_d10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-attribute",
   "metadata": {},
   "source": [
    "L_d1=list(L_y1[0])+list(L_y2[0])+list(L_y3[0])+list(L_y4[0])+list(L_y6[0])\n",
    "L_d2=list(L_y1[1])+list(L_y2[1])+list(L_y3[1])+list(L_y4[1])+list(L_y6[1])\n",
    "L_d3=list(L_y1[2])+list(L_y2[2])+list(L_y3[2])+list(L_y4[2])+list(L_y6[2])\n",
    "L_d4=list(L_y1[3])+list(L_y2[3])+list(L_y3[3])+list(L_y4[3])+list(L_y6[3])\n",
    "L_d5=list(L_y1[4])+list(L_y2[4])+list(L_y3[4])+list(L_y4[4])+list(L_y6[4])\n",
    "L_d6=list(L_y1[5])+list(L_y2[5])+list(L_y3[5])+list(L_y4[5])+list(L_y6[5])\n",
    "L_d7=list(L_y1[6])+list(L_y2[6])+list(L_y3[6])+list(L_y4[6])+list(L_y6[6])\n",
    "L_d8=list(L_y1[7])+list(L_y2[7])+list(L_y3[7])+list(L_y4[7])+list(L_y6[7])\n",
    "L_d9=list(L_y1[8])+list(L_y2[8])+list(L_y3[8])+list(L_y4[8])+list(L_y6[8])\n",
    "L_d10=list(L_y1[9])+list(L_y2[9])+list(L_y3[9])+list(L_y4[9])+list(L_y6[9])\n",
    "L=[L_d1,L_d2,L_d3,L_d4,L_d5,L_d6,L_d7,L_d8,L_d9,L_d10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-economy",
   "metadata": {},
   "source": [
    "import numpy.polynomial.polynomial as poly\n",
    "u_cal=REU\n",
    "L_y=L\n",
    "for i in range(len(REU)):\n",
    "    fig= plt.figure(figsize=(7,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax.patch.set_facecolor('lightblue')\n",
    "    #ax.patch.set_alpha(0.1)\n",
    "    ind=[]\n",
    "    U_cal=[]\n",
    "    Ref=[]\n",
    "    dqo=[25 for i in range(len(Ref))]\n",
    "    plt.scatter(L_y[i],u_cal[i], color='#4B0082')\n",
    "    plt.axhline(y=25, color='black', linestyle='-.',linewidth=3)\n",
    "    plt.ylabel('Relative Expanded Uncertainty(%)', fontsize=16)\n",
    "    plt.xlabel('Reference CO concentration(ppb)',fontsize=16)\n",
    "    plt.grid(linestyle='-.',linewidth=0.1)\n",
    "    #plt.title('LAB',fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-geography",
   "metadata": {},
   "source": [
    "X=data_Mar2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Mar2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum6=sum(A[:100])\n",
    "mean6=np.std(y)/np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-effort",
   "metadata": {},
   "source": [
    "Rmse6_rf2=[]\n",
    "RMSE6_rf2=[]\n",
    "M52=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.15*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1.67*U)\n",
    "    M52.append(m)\n",
    "    Rmse6_rf2.append(mse)\n",
    "    RMSE6_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-input",
   "metadata": {},
   "source": [
    "# April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-locator",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-technique",
   "metadata": {},
   "source": [
    "X=data_Apr[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Apr['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=True)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum7=sum(A[:100])\n",
    "mean7=np.std(y)/np.mean(y)\n",
    "Rmse10_rf2=[]\n",
    "RMSE10_rf2=[]\n",
    "M52=[]\n",
    "for i in range(1,5):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.15*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1.67*U)\n",
    "    M52.append(m)\n",
    "    Rmse5_rf2.append(mse)\n",
    "    RMSE5_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-planning",
   "metadata": {},
   "source": [
    "A=y.to_list()\n",
    "Ext_apr=[]\n",
    "for i in range(len(A)):\n",
    "    if A[i]>3*np.mean(A):\n",
    "        Ext_apr.append(i)\n",
    "N_Ext_apr=len(Ext_apr)\n",
    "N_Ext_apr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-cathedral",
   "metadata": {},
   "source": [
    "mean_apr=np.mean(y)\n",
    "N_apr=y.shape[0]\n",
    "Mean_Rmse_apr=np.mean(Rmse10_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-quest",
   "metadata": {},
   "source": [
    "Mean_Rmse=[Mean_Rmse_oct,Mean_Rmse_nov,Mean_Rmse_dec,Mean_Rmse_jan,Mean_Rmse_feb,Mean_Rmse_mar,Mean_Rmse_apr]\n",
    "Mean_conc=[mean_oct,mean_nov,mean_dec,mean_jan,mean_feb,mean_mar,mean_apr]\n",
    "N=[N_oct,N_nov,N_dec,N_jan,N_feb,N_mar,N_apr]\n",
    "N_Ext=[N_Ext_oct,N_Ext_nov,N_Ext_dec,N_Ext_jan,N_Ext_feb,N_Ext_mar,N_Ext_apr]\n",
    "\n",
    "Rmse=[]\n",
    "for i in range(10):\n",
    "    A=[Rmse1_rf[i],Rmse2_rf[i],Rmse3_rf[i],Rmse4_rf[i],Rmse5_rf[i],Rmse6_rf[i],Rmse10_rf[i]]\n",
    "    Rmse.append(A)\n",
    "RMSE=Rmse[0]+Rmse[1]+Rmse[2]+Rmse[3]+Rmse[4]+Rmse[5]+Rmse[6]+Rmse[7]\n",
    "Conc=Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc\n",
    "N_dp=N+N+N+N+N+N+N+N\n",
    "N_ext=N_Ext+N_Ext+N_Ext+N_Ext+N_Ext+N_Ext+N_Ext+N_Ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-tonight",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "Day_1_rf=[RMSE1_rf[0],RMSE2_rf[0],RMSE3_rf[0],RMSE4_rf[0],RMSE6_rf[0]]\n",
    "Day_2_rf=[RMSE1_rf[1],RMSE2_rf[1],RMSE3_rf[1],RMSE4_rf[1],RMSE6_rf[1]]\n",
    "Day_3_rf=[RMSE1_rf[2],RMSE2_rf[2],RMSE3_rf[2],RMSE4_rf[2],RMSE6_rf[2]]\n",
    "Day_4_rf=[RMSE1_rf[3],RMSE2_rf[3],RMSE3_rf[3],RMSE4_rf[3],RMSE6_rf[3]]\n",
    "Day_5_rf=[RMSE1_rf[4],RMSE2_rf[4],RMSE3_rf[4],RMSE4_rf[4],RMSE6_rf[4]]\n",
    "Day_6_rf=[RMSE1_rf[5],RMSE2_rf[5],RMSE3_rf[5],RMSE4_rf[5],RMSE6_rf[5]]\n",
    "Day_7_rf=[RMSE1_rf[6],RMSE2_rf[6],RMSE3_rf[6],RMSE4_rf[6],RMSE6_rf[6]]\n",
    "Day_8_rf=[RMSE1_rf[7],RMSE2_rf[7],RMSE3_rf[7],RMSE4_rf[7],RMSE6_rf[7]]\n",
    "Day_9_rf=[RMSE1_rf[8],RMSE2_rf[8],RMSE3_rf[8],RMSE4_rf[8],RMSE6_rf[8]]\n",
    "Day_10_rf=[RMSE1_rf[9],RMSE2_rf[9],RMSE3_rf[9],RMSE4_rf[9],RMSE6_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-factory",
   "metadata": {},
   "source": [
    "Day_1_ann=[RMSE1_ann[0],RMSE2_ann[0],RMSE3_ann[0],RMSE4_ann[0],RMSE6_ann[0]]\n",
    "Day_2_ann=[RMSE1_ann[1],RMSE2_ann[1],RMSE3_ann[1],RMSE4_ann[1],RMSE6_ann[1]]\n",
    "Day_3_ann=[RMSE1_ann[2],RMSE2_ann[2],RMSE3_ann[2],RMSE4_ann[2],RMSE6_ann[2]]\n",
    "Day_4_ann=[RMSE1_ann[3],RMSE2_ann[3],RMSE3_ann[3],RMSE4_ann[3],RMSE6_ann[3]]\n",
    "Day_5_ann=[RMSE1_ann[4],RMSE2_ann[4],RMSE3_ann[4],RMSE4_ann[4],RMSE6_ann[4]]\n",
    "Day_6_ann=[RMSE1_ann[5],RMSE2_ann[5],RMSE3_ann[5],RMSE4_ann[5],RMSE6_ann[5]]\n",
    "Day_7_ann=[RMSE1_ann[6],RMSE2_ann[6],RMSE3_ann[6],RMSE4_ann[6],RMSE6_ann[6]]\n",
    "Day_8_ann=[RMSE1_ann[7],RMSE2_ann[7],RMSE3_ann[7],RMSE4_ann[7],RMSE6_ann[7]]\n",
    "Day_9_ann=[RMSE1_ann[8],RMSE2_ann[8],RMSE3_ann[8],RMSE4_ann[8],RMSE6_ann[8]]\n",
    "Day_10_ann=[RMSE1_ann[9],RMSE2_ann[9],RMSE3_ann[9],RMSE4_ann[9],RMSE6_ann[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-librarian",
   "metadata": {},
   "source": [
    "Day_1_RF=[Rmse1_rf[0],Rmse2_rf[0],Rmse3_rf[0],Rmse4_rf[0],Rmse6_rf[0]]\n",
    "Day_2_RF=[Rmse1_rf[1],Rmse2_rf[1],Rmse3_rf[1],Rmse4_rf[1],Rmse6_rf[1]]\n",
    "Day_3_RF=[Rmse1_rf[2],Rmse2_rf[2],Rmse3_rf[2],Rmse4_rf[2],Rmse6_rf[2]]\n",
    "Day_4_RF=[Rmse1_rf[3],Rmse2_rf[3],Rmse3_rf[3],Rmse4_rf[3],Rmse6_rf[3]]\n",
    "Day_5_RF=[Rmse1_rf[4],Rmse2_rf[4],Rmse3_rf[4],Rmse4_rf[4],Rmse6_rf[4]]\n",
    "Day_6_RF=[Rmse1_rf[5],Rmse2_rf[5],Rmse3_rf[5],Rmse4_rf[5],Rmse6_rf[5]]\n",
    "Day_7_RF=[Rmse1_rf[6],Rmse2_rf[6],Rmse3_rf[6],Rmse4_rf[6],Rmse6_rf[6]]\n",
    "Day_8_RF=[Rmse1_rf[7],Rmse2_rf[7],Rmse3_rf[7],Rmse4_rf[7],Rmse6_rf[7]]\n",
    "Day_9_RF=[Rmse1_rf[8],Rmse2_rf[8],Rmse3_rf[8],Rmse4_rf[8],Rmse6_rf[8]]\n",
    "Day_10_RF=[Rmse1_rf[9],Rmse2_rf[9],Rmse3_rf[9],Rmse4_rf[9],Rmse6_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-blind",
   "metadata": {},
   "source": [
    "Day_1_RF2=[Rmse1_rf2[0],Rmse2_rf2[0],Rmse3_rf2[0],Rmse4_rf2[0],Rmse6_rf2[0]]\n",
    "Day_2_RF2=[Rmse1_rf2[1],Rmse2_rf2[1],Rmse3_rf2[1],Rmse4_rf2[1],Rmse6_rf2[1]]\n",
    "Day_3_RF2=[Rmse1_rf2[2],Rmse2_rf2[2],Rmse3_rf2[2],Rmse4_rf2[2],Rmse6_rf2[2]]\n",
    "Day_4_RF2=[Rmse1_rf2[3],Rmse2_rf2[3],Rmse3_rf2[3],Rmse4_rf2[3],Rmse6_rf2[3]]\n",
    "Day_5_RF2=[Rmse1_rf2[4],Rmse2_rf2[4],Rmse3_rf2[4],Rmse4_rf2[4],Rmse6_rf2[4]]\n",
    "Day_6_RF2=[Rmse1_rf2[5],Rmse2_rf2[5],Rmse3_rf2[5],Rmse4_rf2[5],Rmse6_rf2[5]]\n",
    "Day_7_RF2=[Rmse1_rf2[6],Rmse2_rf2[6],Rmse3_rf2[6],Rmse4_rf2[6],Rmse6_rf2[6]]\n",
    "Day_8_RF2=[Rmse1_rf2[7],Rmse2_rf2[7],Rmse3_rf2[7],Rmse4_rf2[7],Rmse6_rf2[7]]\n",
    "Day_9_RF2=[Rmse1_rf2[8],Rmse2_rf2[8],Rmse3_rf2[8],Rmse4_rf2[8],Rmse6_rf2[8]]\n",
    "Day_10_RF2=[Rmse1_rf2[9],Rmse2_rf2[9],Rmse3_rf2[9],Rmse4_rf2[9],Rmse6_rf2[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-reaction",
   "metadata": {},
   "source": [
    "Mean=(np.array(Rmse1_rf)+np.array(Rmse2_rf)+np.array(Rmse3_rf)+np.array(Rmse4_rf)+np.array(Rmse6_rf))/5\n",
    "Mean=list(Mean)+list(Mean)+list(Mean)+list(Mean)\n",
    "Mean=sorted(Mean)\n",
    "\n",
    "Oct=sorted(Rmse1_rf+Rmse1_rf+Rmse1_rf+Rmse1_rf)\n",
    "Nov=sorted(Rmse2_rf+Rmse2_rf+Rmse2_rf+Rmse2_rf)\n",
    "Dec=sorted(Rmse3_rf+Rmse3_rf+Rmse3_rf+Rmse3_rf)\n",
    "Jan=sorted(Rmse4_rf+Rmse4_rf+Rmse4_rf+Rmse4_rf)\n",
    "Mar=sorted(Rmse6_rf+Rmse6_rf+Rmse6_rf+Rmse6_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MPI=[M1[0],M2[0],M3[0],M4[0]]#M5[0]]\n",
    "Day_2_MPI=[M1[1],M2[1],M3[1],M4[1]]#M5[1]]\n",
    "Day_3_MPI=[M1[2],M2[2],M3[2],M4[2]]#M5[2]]\n",
    "Day_4_MPI=[M1[3],M2[3],M3[3],M4[3]]#M5[3]]\n",
    "Day_5_MPI=[M1[4],M2[4],M3[4],M4[4]]#M5[4]]\n",
    "Day_6_MPI=[M1[5],M2[5],M3[5],M4[5]]#M5[5]]\n",
    "Day_7_MPI=[M1[6],M2[6],M3[6],M4[6]]#M5[6]]\n",
    "Day_8_MPI=[M1[7],M2[7],M3[7],M4[7]]#M5[7]]\n",
    "Day_9_MPI=[M1[8],M2[8],M3[8],M4[8]]#M5[8]]\n",
    "#Day_10_MPI=[M1[9],M2[9],M3[9],M4[9]]#M5[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MPI2=[M12[0],M22[0],M32[0],M42[0]]#,M52[0]]\n",
    "Day_2_MPI2=[M12[1],M22[1],M32[1],M42[1]]#,M52[1]]\n",
    "Day_3_MPI2=[M12[2],M22[2],M32[2],M42[2]]#,M52[2]]\n",
    "Day_4_MPI2=[M12[3],M22[3],M32[3],M42[3]]#,M52[3]]\n",
    "Day_5_MPI2=[M12[4],M22[4],M32[4],M42[4]]#,M52[4]]\n",
    "Day_6_MPI2=[M12[5],M22[5],M32[5],M42[5]]#,M52[5]]\n",
    "Day_7_MPI2=[M12[6],M22[6],M32[6],M42[6]]#,M52[6]]\n",
    "Day_8_MPI2=[M12[7],M22[7],M32[7],M42[7]]#,M52[7]]\n",
    "#Day_9_MPI2=[M12[8],M22[8],M32[8],M42[8]]#,M52[8]]\n",
    "#Day_10_MPI2=[M12[9],M22[9],M32[9],M42[9]]#,M52[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MPI3=[M13[0],M23[0],M33[0],M43[0]]#,M52[0]]\n",
    "Day_2_MPI3=[M13[1],M23[1],M33[1],M43[1]]#,M52[1]]\n",
    "Day_3_MPI3=[M13[2],M23[2],M33[2],M43[2]]#,M52[2]]\n",
    "Day_4_MPI3=[M13[3],M23[3],M33[3],M43[3]]#,M52[3]]\n",
    "Day_5_MPI3=[M13[4],M23[4],M33[4],M43[4]]#,M52[4]]\n",
    "Day_6_MPI3=[M13[5],M23[5],M33[5],M43[5]]#,M52[5]]\n",
    "Day_7_MPI3=[M13[6],M23[6],M33[6],M43[6]]#,M52[6]]\n",
    "Day_8_MPI3=[M13[7],M23[7],M33[7],M43[7]]#,M52[7]]\n",
    "#Day_9_MPI3=[M13[8],M23[8],M33[8],M43[8]]#,M52[8]]\n",
    "#Day_10_MPI3=[M13[9],M23[9],M33[9],M43[9]]#,M52[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-italic",
   "metadata": {},
   "source": [
    "Day_1_ANN=[Rmse1_ann[0],Rmse2_ann[0],Rmse3_ann[0],Rmse4_ann[0],Rmse6_ann[0]]\n",
    "Day_2_ANN=[Rmse1_ann[1],Rmse2_ann[1],Rmse3_ann[1],Rmse4_ann[1],Rmse6_ann[1]]\n",
    "Day_3_ANN=[Rmse1_ann[2],Rmse2_ann[2],Rmse3_ann[2],Rmse4_ann[2],Rmse6_ann[2]]\n",
    "Day_4_ANN=[Rmse1_ann[3],Rmse2_ann[3],Rmse3_ann[3],Rmse4_ann[3],Rmse6_ann[3]]\n",
    "Day_5_ANN=[Rmse1_ann[4],Rmse2_ann[4],Rmse3_ann[4],Rmse4_ann[4],Rmse6_ann[4]]\n",
    "Day_6_ANN=[Rmse1_ann[5],Rmse2_ann[5],Rmse3_ann[5],Rmse4_ann[5],Rmse6_ann[5]]\n",
    "Day_7_ANN=[Rmse1_ann[6],Rmse2_ann[6],Rmse3_ann[6],Rmse4_ann[6],Rmse6_ann[6]]\n",
    "Day_8_ANN=[Rmse1_ann[7],Rmse2_ann[7],Rmse3_ann[7],Rmse4_ann[7],Rmse6_ann[7]]\n",
    "Day_9_ANN=[Rmse1_ann[8],Rmse2_ann[8],Rmse3_ann[8],Rmse4_ann[8],Rmse6_ann[8]]\n",
    "Day_10_ANN=[Rmse1_ann[9],Rmse2_ann[9],Rmse3_ann[9],Rmse4_ann[9],Rmse6_ann[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_P=Day_1_rf+Day_2_rf+Day_3_rf+Day_4_rf+Day_5_rf+Day_6_rf+Day_7_rf+Day_8_rf+Day_9_rf+Day_10_rf\n",
    "#ANN_P=Day_1_ann+Day_2_ann+Day_3_ann+Day_4_ann+Day_5_ann+Day_6_ann+Day_7_ann+Day_8_ann+Day_9_ann+Day_10_ann\n",
    "#RF_R=Day_1_RF+Day_2_RF+Day_3_RF+Day_4_RF+Day_5_RF+Day_6_RF+Day_7_RF+Day_8_RF+Day_9_RF+Day_10_RF\n",
    "#RF_R2=Day_1_RF2+Day_2_RF2+Day_3_RF2+Day_4_RF2+Day_5_RF2+Day_6_RF2+Day_7_RF2+Day_8_RF2+Day_9_RF2+Day_10_RF2\n",
    "MPI=Day_1_MPI+Day_2_MPI+Day_3_MPI+Day_4_MPI+Day_5_MPI+Day_6_MPI+Day_7_MPI+Day_8_MPI#+Day_9_MPI+Day_10_MPI\n",
    "MPI2=Day_1_MPI2+Day_2_MPI2+Day_3_MPI2+Day_4_MPI2+Day_5_MPI2+Day_6_MPI2+Day_7_MPI2+Day_8_MPI2#+Day_9_MPI2+Day_10_MPI2\n",
    "MPI3=Day_1_MPI3+Day_2_MPI3+Day_3_MPI3+Day_4_MPI3+Day_5_MPI3+Day_6_MPI3+Day_7_MPI3+Day_8_MPI3#+Day_9_MPI3+Day_10_MPI3\n",
    "#ANN_R=Day_1_ANN+Day_2_ANN+Day_3_ANN+Day_4_ANN+Day_5_ANN+Day_6_ANN+Day_7_ANN+Day_8_ANN+Day_9_ANN+Day_10_ANN\n",
    "x0=['0' for i in range(4)]\n",
    "x1=['2' for i in range(4)]\n",
    "x2=['4' for i in range(4)]\n",
    "x3=['6' for i in range(4)]\n",
    "x4=['8' for i in range(4)]\n",
    "x5=['10' for i in range(4)]\n",
    "x6=['12' for i in range(4)]\n",
    "x7=['14' for i in range(4)]\n",
    "x8=['16' for i in range(4)]\n",
    "x9=['18' for i in range(4)]\n",
    "x10=['20' for i in range(4)]\n",
    "x11=['22' for i in range(4)]\n",
    "Reg=[10 for i in range(48)]\n",
    "Reg=[10 for i in range(48)]\n",
    "Spatial=[25 for i in range(60) ]\n",
    "Intervention=[30 for i in range(60) ]\n",
    "Hs_and_sp=[50 for i in range(60) ]\n",
    "reg=[0.9 for i in range(60)]\n",
    "spatial=[0.75 for i in range(60) ]\n",
    "intervention=[0.7 for i in range(60) ]\n",
    "\n",
    "x=x1+x2+x3+x4+x5+x6+x7+x8#+x9+x10\n",
    "X=x0+x1+x2+x3+x4+x5+x6+x7+x8+x9#+x10+x11\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio\n",
    "fig = go.Figure() \n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([1, 3, 2, 3, 1])\n",
    "# Defining x axis\n",
    "x = x\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=MPI,\n",
    "    x=x,\n",
    "    #name=r'$CO$',\n",
    "    marker_color='darkblue',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=MPI2,\n",
    "    x=x,\n",
    "    #name=r'$NO_2$',\n",
    "    marker_color='Teal',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "     #to x-axis\n",
    "    y=MPI3,\n",
    "    x=x,\n",
    "    #name=r'$O_3$',\n",
    "    marker_color='darkgoldenrod',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "\n",
    "\n",
    "#x = x\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "   # y=RF_R2,\n",
    "    #x=x,\n",
    "    #name='R^2(NO2)',\n",
    "    #marker_color='teal',\n",
    "    #showlegend=True\n",
    "   \n",
    "\n",
    "#))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    #defining y axis in corresponding\n",
    "   # to x-axis\n",
    "   # y=RF_R,\n",
    "    #x=x,\n",
    "    #name='SMAE',\n",
    "    #marker_color='#CD6600',\n",
    "    #showlegend=True\n",
    "   \n",
    "#))\n",
    "#fig.add_trace(go.Box(\n",
    "   #y=ANN_R,\n",
    "    #x=x,\n",
    "    #name='XGBoost(NMAE)',\n",
    "    #marker_color='deeppink',\n",
    "    #showlegend=True\n",
    "\n",
    "#))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Reg, \n",
    "                name=\"RC\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'green', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Spatial, \n",
    "                name=\"SGS\",\n",
    "                mode = 'lines',\n",
    "                marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'purple', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                    \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Intervention, \n",
    "                name=\"IS/IM\",\n",
    "                mode = 'lines',\n",
    "                marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'orange', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Hs_and_sp, \n",
    "                name=\"HA/SP\",\n",
    "                    \n",
    "                mode = 'lines',\n",
    "                marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'dodgerblue', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "               \n",
    "                        ))\n",
    "\n",
    "  \n",
    "fig.update_layout(autosize=True,\n",
    "                 #title={'text': r\"$NO_2$\",\n",
    "        #'y':0.78,\n",
    "        #'x':0.5,\n",
    "        #'xanchor': 'center',\n",
    "        #'yanchor': 'top'\n",
    "                       #}, \n",
    "    width=1000,\n",
    "    height=500,\n",
    "                  \n",
    "  legend=dict( yanchor=\"bottom\",\n",
    "    y=0.865,\n",
    "    x=0.01,\n",
    "    orientation=\"h\"\n",
    "),\n",
    "    # group together boxes of the different\n",
    "    # traces for each value of x\n",
    "    boxmode='group',\n",
    "                  plot_bgcolor='rgba(0.0,0.0,0.0,0.0)'\n",
    "                 \n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days of training\",tickfont = dict(size=16),\n",
    "                 titlefont = dict(size=24),linewidth=1.4, linecolor='black',tick0 = 2,\n",
    "        dtick = 2,\n",
    "                 mirror=True)\n",
    "fig.update_yaxes(title_text=\"Precision (%)\",tickfont = dict(size=16),range=[0.4,1],\n",
    "                 titlefont = dict(size=24),linewidth=1.4, linecolor='black',tick0 = 0.2,\n",
    "        dtick =0.2,\n",
    "                 mirror=True)\n",
    "fig.show()\n",
    "chart_studio.plotly.sign_in('vinylango', 'gybbJVWfRSUoTcRRSa6J')\n",
    "chart_studio.plotly.image.save_as(fig, filename='models_boxplot.png')\n",
    "#Image('models_boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-factory",
   "metadata": {},
   "source": [
    "Metric1=['RF' for i in range(len(RF))]\n",
    "Metric2=['XGBoost' for i in range(len(ANN))]\n",
    "Model=Metric1+Metric2\n",
    "Training=x+x\n",
    "Values=RF+ANN\n",
    "len(Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-chaos",
   "metadata": {},
   "source": [
    "#Violin plot which also show the density of the distribution\n",
    "import plotly.express as px\n",
    "Metric1=['RF' for i in range(len(RF))]\n",
    "Metric2=['XGBoost' for i in range(len(ANN))]\n",
    "Model=Metric1+Metric2\n",
    "Training=x+x\n",
    "Values=RF+ANN\n",
    "lst=[[Training[i],Values[i],Model[i]] for i in range(len(Model))]\n",
    "df = pd.DataFrame(lst, columns =['Training Days', 'Pearson correlation (r)','Model'])\n",
    "\n",
    "#fig = px.violin( df,y=\"Performance\", x=\"Calibration Model\", color='Metric', box=True,points=\"all\",\n",
    "          #hover_data=df.columns)\n",
    "fig = px.violin( df,y=\"Pearson correlation (r)\", x=\"Training Days\", color='Model', box=True,\n",
    "          hover_data=df.columns)\n",
    "\n",
    "\n",
    "fig.update_layout(autosize=False,\n",
    "    width=900,\n",
    "    height=500)\n",
    "fig.show()\n",
    "#chart_studio.plotly.sign_in('vinylango', 'gybbJVWfRSUoTcRRSa6J')\n",
    "#chart_studio.plotly.image.save_as(fig, filename='models_violinplots.png')\n",
    "#Image('models_violinplots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-anatomy",
   "metadata": {},
   "source": [
    "# Seasonal Calibration Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-house",
   "metadata": {},
   "source": [
    "# Fall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-anger",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_oct,data_nov]\n",
    "fall=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall.resample('h').mean()\n",
    "Fall=Fall.dropna()\n",
    "#Fall=Fall.sample(frac=1)\n",
    "X=Fall[['Net Signal','Temp1','RH1','Month','Day_of_week','Day','Hour']]\n",
    "y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean1=np.std(y)\n",
    "Rmse7_rf=[]\n",
    "RMSE7_rf=[]\n",
    "REU7=[]\n",
    "L_y7=[]\n",
    "A7=[]\n",
    "M7=[]\n",
    "B7=[]\n",
    "for i in range(1,9):\n",
    "    k=y_train[120*i:].to_list()\n",
    "    lv=max(y_train[120*i:])\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M7.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B7.append(b)\n",
    "    Rmse7_rf.append(mse)\n",
    "    RMSE7_rf.append(rmse)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse7_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE7_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "M7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_oct2,data_nov2]\n",
    "fall2=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall2=fall2.resample('h').mean()\n",
    "Fall2=Fall2.dropna()\n",
    "#Fall=Fall.sample(frac=1)\n",
    "X=Fall2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=Fall2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean1=np.std(y)\n",
    "Rmse7_rf2=[]\n",
    "RMSE7_rf2=[]\n",
    "M72=[]\n",
    "B72=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M72.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B72.append(b)\n",
    "    Rmse7_rf2.append(mse)\n",
    "    RMSE7_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse7_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE7_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "M72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_oct3,data_nov3]\n",
    "fall3=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall3=fall3.resample('h').mean()\n",
    "Fall3=Fall3.dropna()\n",
    "#Fall=Fall.sample(frac=1)\n",
    "X=Fall3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Fall3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean1=np.std(y)\n",
    "Rmse7_rf3=[]\n",
    "RMSE7_rf3=[]\n",
    "M73=[]\n",
    "B73=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M73.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B73.append(b)\n",
    "    Rmse7_rf3.append(mse)\n",
    "    RMSE7_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse7_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE7_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "M73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-maldives",
   "metadata": {},
   "source": [
    "# Winter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-distribution",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_dec,data_jan]#,data_feb\n",
    "winter=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter=winter.resample('h').mean()\n",
    "Winter=Winter.dropna()\n",
    "#Winter=Winter.sample(frac=1)\n",
    "X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Winter['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean2=np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf=[]\n",
    "RMSE8_rf=[]\n",
    "REU8=[]\n",
    "L_y8=[]\n",
    "A8=[]\n",
    "M8=[]\n",
    "B8=[]\n",
    "for i in range(1,8):\n",
    "    k=y_train[120*i:].to_list()\n",
    "    lv=max(y_train[120*i:])\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M8.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B8.append(b)\n",
    "    Rmse8_rf.append(mse)\n",
    "    RMSE8_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[120*i:],1)\n",
    "    re=REF2(pred,y_train[120*i:],1,lv)\n",
    "    A8.append(re)\n",
    "    REU8.append(reu)\n",
    "    L_y8.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE8_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "M8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_dec2,data_jan2,data_feb2]\n",
    "winter2=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter2=winter2.resample('h').mean()\n",
    "Winter2=Winter2.dropna()\n",
    "X=Winter2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=Winter2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean2=np.std(y)\n",
    "Rmse8_rf2=[]\n",
    "RMSE8_rf2=[]\n",
    "M82=[]\n",
    "B82=[]\n",
    "for i in range(1,8):\n",
    "    \n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M82.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B82.append(b)\n",
    "    Rmse8_rf2.append(mse)\n",
    "    RMSE8_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE8_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "M82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_dec3,data_jan3,data_feb3]\n",
    "winter3=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter3=winter3.resample('h').mean()\n",
    "Winter3=Winter3.dropna()\n",
    "X=Winter3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Winter3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean2=np.std(y)\n",
    "Rmse8_rf3=[]\n",
    "RMSE8_rf3=[]\n",
    "M83=[]\n",
    "B83=[]\n",
    "for i in range(1,9):\n",
    "    \n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M83.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B83.append(b)\n",
    "    Rmse8_rf3.append(mse)\n",
    "    RMSE8_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE8_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "M83"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-general",
   "metadata": {},
   "source": [
    "# Spring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-consequence",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_mar,data_apr]\n",
    "spring=pd.concat(frame1)\n",
    "Spring=spring.resample('h').mean()\n",
    "Spring=Spring.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring[['Net Signal','Temp1','RH1','Month','Day_of_week','Day','Hour']]\n",
    "y=Spring['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean3=np.std(y)\n",
    "Rmse9_rf=[]\n",
    "RMSE9_rf=[]\n",
    "REU9=[]\n",
    "L_y9=[]\n",
    "A9=[]\n",
    "M9=[]\n",
    "B9=[]\n",
    "for i in range(1,8):\n",
    "    k=y_train[120*i:].to_list()\n",
    "    lv=20000\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    Rmse9_rf.append(mse)\n",
    "    RMSE9_rf.append(rmse)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M9.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B9.append(b)\n",
    "    reu=REF(pred,y_train[120*i:],1)\n",
    "    re=REF2(pred,y_train[120*i:],1,lv)\n",
    "    A9.append(re)\n",
    "    REU9.append(reu)\n",
    "    L_y9.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse9_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE9_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "M9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_mar2,data_apr2]\n",
    "spring2=pd.concat(frame1)\n",
    "Spring2=spring2.resample('h').mean()\n",
    "Spring2=Spring2.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=Spring2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean3=np.std(y)\n",
    "Rmse9_rf2=[]\n",
    "RMSE9_rf2=[]\n",
    "M92=[]\n",
    "B92=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M92.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B92.append(b)\n",
    "    Rmse9_rf2.append(mse)\n",
    "    RMSE9_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse9_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE9_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "M92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_mar3,data_apr3]\n",
    "spring3=pd.concat(frame1)\n",
    "Spring3=spring3.resample('h').mean()\n",
    "Spring3=Spring3.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Spring3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean3=np.std(y)\n",
    "Rmse9_rf3=[]\n",
    "RMSE9_rf3=[]\n",
    "M93=[]\n",
    "B93=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M93.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B93.append(b)\n",
    "    Rmse9_rf3.append(mse)\n",
    "    RMSE9_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    " Rmse9_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-product",
   "metadata": {},
   "outputs": [],
   "source": [
    " RMSE9_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-village",
   "metadata": {},
   "outputs": [],
   "source": [
    " M93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-digit",
   "metadata": {},
   "source": [
    "#import chart_studio\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "Day_1_rf=[RMSE7_rf[0],RMSE8_rf[0],RMSE9_rf[0]]\n",
    "Day_2_rf=[RMSE7_rf[1],RMSE8_rf[1],RMSE9_rf[1]]\n",
    "Day_3_rf=[RMSE7_rf[2],RMSE8_rf[2],RMSE9_rf[2]]\n",
    "Day_4_rf=[RMSE7_rf[3],RMSE8_rf[3],RMSE9_rf[3]]\n",
    "Day_5_rf=[RMSE7_rf[4],RMSE8_rf[4],RMSE9_rf[4]]\n",
    "Day_6_rf=[RMSE7_rf[5],RMSE8_rf[5],RMSE9_rf[5]]\n",
    "Day_7_rf=[RMSE7_rf[6],RMSE8_rf[6],RMSE9_rf[6]]\n",
    "Day_8_rf=[RMSE7_rf[7],RMSE8_rf[7],RMSE9_rf[7]]\n",
    "#Day_9_rf=[RMSE7_rf[8],RMSE8_rf[8],RMSE9_rf[8]]\n",
    "#Day_10_rf=[RMSE7_rf[9],RMSE8_rf[9],RMSE9_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-sharp",
   "metadata": {},
   "source": [
    "Day_1_ann=[RMSE7_ann[0],RMSE8_ann[0],RMSE9_ann[0]]\n",
    "Day_2_ann=[RMSE7_ann[1],RMSE8_ann[1],RMSE9_ann[1]]\n",
    "Day_3_ann=[RMSE7_ann[2],RMSE8_ann[2],RMSE9_ann[2]]\n",
    "Day_4_ann=[RMSE7_ann[3],RMSE8_ann[3],RMSE9_ann[3]]\n",
    "Day_5_ann=[RMSE7_ann[4],RMSE8_ann[4],RMSE9_ann[4]]\n",
    "Day_6_ann=[RMSE7_ann[5],RMSE8_ann[5],RMSE9_ann[5]]\n",
    "Day_7_ann=[RMSE7_ann[6],RMSE8_ann[6],RMSE9_ann[6]]\n",
    "Day_8_ann=[RMSE7_ann[7],RMSE8_ann[7],RMSE9_ann[7]]\n",
    "Day_9_ann=[RMSE7_ann[8],RMSE8_ann[8],RMSE9_ann[8]]\n",
    "Day_10_ann=[RMSE7_ann[9],RMSE8_ann[9],RMSE9_ann[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-approach",
   "metadata": {},
   "source": [
    "Day_1_RF=[Rmse7_rf[0],Rmse8_rf[0],Rmse9_rf[0]]\n",
    "Day_2_RF=[Rmse7_rf[1],Rmse8_rf[1],Rmse9_rf[1]]\n",
    "Day_3_RF=[Rmse7_rf[2],Rmse8_rf[2],Rmse9_rf[2]]\n",
    "Day_4_RF=[Rmse7_rf[3],Rmse8_rf[3],Rmse9_rf[3]]\n",
    "Day_5_RF=[Rmse7_rf[4],Rmse8_rf[4],Rmse9_rf[4]]\n",
    "Day_6_RF=[Rmse7_rf[5],Rmse8_rf[5],Rmse9_rf[5]]\n",
    "Day_7_RF=[Rmse7_rf[6],Rmse8_rf[6],Rmse9_rf[6]]\n",
    "Day_8_RF=[Rmse7_rf[7],Rmse8_rf[7],Rmse9_rf[7]]\n",
    "#Day_9_RF=[Rmse7_rf[8],Rmse8_rf[8],Rmse9_rf[8]]\n",
    "#Day_10_RF=[Rmse7_rf[9],Rmse8_rf[9],Rmse9_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-luther",
   "metadata": {},
   "source": [
    "Day_1_RF2=[Rmse7_rf2[0],Rmse8_rf2[0],Rmse9_rf2[0]]\n",
    "Day_2_RF2=[Rmse7_rf2[1],Rmse8_rf2[1],Rmse9_rf2[1]]\n",
    "Day_3_RF2=[Rmse7_rf2[2],Rmse8_rf2[2],Rmse9_rf2[2]]\n",
    "Day_4_RF2=[Rmse7_rf2[3],Rmse8_rf2[3],Rmse9_rf2[3]]\n",
    "Day_5_RF2=[Rmse7_rf2[4],Rmse8_rf2[4],Rmse9_rf2[4]]\n",
    "Day_6_RF2=[Rmse7_rf2[5],Rmse8_rf2[5],Rmse9_rf2[5]]\n",
    "Day_7_RF2=[Rmse7_rf2[6],Rmse8_rf2[6],Rmse9_rf2[6]]\n",
    "Day_8_RF2=[Rmse7_rf2[7],Rmse8_rf2[7],Rmse9_rf2[7]]\n",
    "#Day_9_RF=[Rmse7_rf[8],Rmse8_rf[8],Rmse9_rf[8]]\n",
    "#Day_10_RF=[Rmse7_rf[9],Rmse8_rf[9],Rmse9_rf[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import chart_studio\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "Day_1_MQI=[M7[0],M8[0],M9[0]]\n",
    "Day_2_MQI=[M7[1],M8[1],M9[1]]\n",
    "Day_3_MQI=[M7[2],M8[2],M9[2]]\n",
    "Day_4_MQI=[M7[3],M8[3],M9[3]]\n",
    "Day_5_MQI=[M7[4],M8[4],M9[4]]\n",
    "Day_6_MQI=[M7[5],M8[5],M9[5]]\n",
    "Day_7_MQI=[M7[6],M8[6],M9[6]]\n",
    "#Day_8_MQI=[M7[7],M8[7],M9[7]]\n",
    "\n",
    "Day_1_BQI=[B7[0],B8[0],B9[0]]\n",
    "Day_2_BQI=[B7[1],B8[1],B9[1]]\n",
    "Day_3_BQI=[B7[2],B8[2],B9[2]]\n",
    "Day_4_BQI=[B7[3],B8[3],B9[3]]\n",
    "Day_5_BQI=[B7[4],B8[4],B9[4]]\n",
    "Day_6_BQI=[B7[5],B8[5],B9[5]]\n",
    "Day_7_BQI=[B7[6],B8[6],B9[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MQI2=[M72[0],M82[0],M92[0]]\n",
    "Day_2_MQI2=[M72[1],M82[1],M92[1]]\n",
    "Day_3_MQI2=[M72[2],M82[2],M92[2]]\n",
    "Day_4_MQI2=[M72[3],M82[3],M92[3]]\n",
    "Day_5_MQI2=[M72[4],M82[4],M92[4]]\n",
    "Day_6_MQI2=[M72[5],M82[5],M92[5]]\n",
    "Day_7_MQI2=[M72[6],M82[6],M92[6]]\n",
    "#Day_8_MQI2=[M72[7],M92[7],M92[7]]\n",
    "\n",
    "Day_1_BQI2=[B72[0],B82[0],B92[0]]\n",
    "Day_2_BQI2=[B72[1],B82[1],B92[1]]\n",
    "Day_3_BQI2=[B72[2],B82[2],B92[2]]\n",
    "Day_4_BQI2=[B72[3],B82[3],B92[3]]\n",
    "Day_5_BQI2=[B72[4],B82[4],B92[4]]\n",
    "Day_6_BQI2=[B72[5],B82[5],B92[5]]\n",
    "Day_7_BQI2=[B72[6],B82[6],B92[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MQI3=[M73[0],M83[0],M93[0]]\n",
    "Day_2_MQI3=[M73[1],M83[1],M93[1]]\n",
    "Day_3_MQI3=[M73[2],M83[2],M93[2]]\n",
    "Day_4_MQI3=[M73[3],M83[3],M93[3]]\n",
    "Day_5_MQI3=[M73[4],M83[4],M93[4]]\n",
    "Day_6_MQI3=[M73[5],M83[5],M93[5]]\n",
    "Day_7_MQI3=[M73[6],M83[6],M93[6]]\n",
    "#Day_8_MQI2=[M72[7],M92[7],M92[7]]\n",
    "\n",
    "Day_1_BQI3=[B73[0],B83[0],B93[0]]\n",
    "Day_2_BQI3=[B73[1],B83[1],B93[1]]\n",
    "Day_3_BQI3=[B73[2],B83[2],B93[2]]\n",
    "Day_4_BQI3=[B73[3],B83[3],B93[3]]\n",
    "Day_5_BQI3=[B73[4],B83[4],B93[4]]\n",
    "Day_6_BQI3=[B73[5],B83[5],B93[5]]\n",
    "Day_7_BQI3=[B73[6],B83[6],B93[6]]\n",
    "#Day_8_MQI2=[M72[7],M92[7],M92[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-opera",
   "metadata": {},
   "source": [
    "Day_1_ANN=[Rmse7_ann[0],Rmse8_ann[0],Rmse9_ann[0]]\n",
    "Day_2_ANN=[Rmse7_ann[1],Rmse8_ann[1],Rmse9_ann[1]]\n",
    "Day_3_ANN=[Rmse7_ann[2],Rmse8_ann[2],Rmse9_ann[2]]\n",
    "Day_4_ANN=[Rmse7_ann[3],Rmse8_ann[3],Rmse9_ann[3]]\n",
    "Day_5_ANN=[Rmse7_ann[4],Rmse8_ann[4],Rmse9_ann[4]]\n",
    "Day_6_ANN=[Rmse7_ann[5],Rmse8_ann[5],Rmse9_ann[5]]\n",
    "Day_7_ANN=[Rmse7_ann[6],Rmse8_ann[6],Rmse9_ann[6]]\n",
    "Day_8_ANN=[Rmse7_ann[7],Rmse8_ann[7],Rmse9_ann[7]]\n",
    "Day_9_ANN=[Rmse7_ann[8],Rmse8_ann[8],Rmse9_ann[8]]\n",
    "Day_10_ANN=[Rmse7_ann[9],Rmse8_ann[9],Rmse9_ann[9]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_P=Day_1_rf+Day_2_rf+Day_3_rf+Day_4_rf+Day_5_rf+Day_6_rf+Day_7_rf+Day_8_rf\n",
    "#XGBoost_P=Day_1_ann+Day_2_ann+Day_3_ann+Day_4_ann+Day_5_ann+Day_6_ann+Day_7_ann+Day_8_ann+Day_9_ann+Day_10_ann\n",
    "#RF_R=Day_1_RF+Day_2_RF+Day_3_RF+Day_4_RF+Day_5_RF+Day_6_RF+Day_7_RF+Day_8_RF\n",
    "#RF_R2=Day_1_RF2+Day_2_RF2+Day_3_RF2+Day_4_RF2+Day_5_RF2+Day_6_RF2+Day_7_RF2+Day_8_RF2\n",
    "#XGBoost_R=Day_1_ANN+Day_2_ANN+Day_3_ANN+Day_4_ANN+Day_5_ANN+Day_6_ANN+Day_7_ANN+Day_8_ANN+Day_9_ANN+Day_10_ANN\n",
    "MQI=Day_1_MQI+Day_2_MQI+Day_3_MQI+Day_4_MQI+Day_5_MQI+Day_6_MQI+Day_7_MQI#+Day_8_MQI\n",
    "MQI2=Day_1_MQI2+Day_2_MQI2+Day_3_MQI2+Day_4_MQI2+Day_5_MQI2+Day_6_MQI2+Day_7_MQI2#+Day_8_MQI2\n",
    "MQI3=Day_1_MQI3+Day_2_MQI3+Day_3_MQI3+Day_4_MQI3+Day_5_MQI3+Day_6_MQI3+Day_7_MQI3\n",
    "\n",
    "BQI=Day_1_BQI+Day_2_BQI+Day_3_BQI+Day_4_BQI+Day_5_BQI+Day_6_BQI+Day_7_BQI#+Day_8_MQI\n",
    "BQI2=Day_1_BQI2+Day_2_BQI2+Day_3_BQI2+Day_4_BQI2+Day_5_BQI2+Day_6_BQI2+Day_7_BQI2#+Day_8_MQI2\n",
    "BQI3=Day_1_BQI3+Day_2_BQI3+Day_3_BQI3+Day_4_BQI3+Day_5_BQI3+Day_6_BQI3+Day_7_BQI3\n",
    "x0=['0' for i in range(3)]\n",
    "x1=['5' for i in range(3)]\n",
    "x2=['10' for i in range(3)]\n",
    "x3=['15' for i in range(3)]\n",
    "x4=['20' for i in range(3)]\n",
    "x5=['25' for i in range(3)]\n",
    "x6=['30' for i in range(3)]\n",
    "x7=['35' for i in range(3)]\n",
    "x8=['40' for i in range(3)]\n",
    "x9=['45' for i in range(3)]\n",
    "\n",
    "Reg=[10 for i in range(30)]\n",
    "Spatial=[25 for i in range(30) ]\n",
    "Intervention=[30 for i in range(30) ]\n",
    "Hs_and_sp=[50 for i in range(30) ]\n",
    "\n",
    "\n",
    "x=x1+x2+x3+x4+x5+x6+x7\n",
    "X=x0+x1+x2+x3+x4+x5+x6+x7+x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio\n",
    "fig = go.Figure() \n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([1, 3, 2, 3, 1])\n",
    "# Defining x axis\n",
    "x = x\n",
    "\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=MQI,\n",
    "    x=x,\n",
    "    name='Accuracy',\n",
    "    marker_color='#8470FF',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    #y=MQI3,\n",
    "    #x=x,\n",
    "    #name=r'$O_3$',\n",
    "    #marker_color='#FF8000',\n",
    "    #showlegend=False\n",
    "   \n",
    "#))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=BQI,\n",
    "    x=x,\n",
    "    name='Bias',\n",
    "    marker_color='#FF8000',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    #defining y axis in corresponding\n",
    "   # to x-axis\n",
    "    #y=RF_R,\n",
    "    #x=x,\n",
    "    #name='SMAE',\n",
    "    #marker_color='olive',\n",
    "   # showlegend=True\n",
    "   \n",
    "#))\n",
    "#fig.add_trace(go.Box(\n",
    "   #y=ANN_R,\n",
    "    #x=x,\n",
    "    #name='XGBoost(NMAE)',\n",
    "    #marker_color='orangered',\n",
    "    #showlegend=True\n",
    "\n",
    "\n",
    "#))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(autosize=True,\n",
    "                 title={'text':r\"$O_3$\",\n",
    "        'y':0.77,\n",
    "        'x':0.5,\n",
    "        #'xanchor': 'center',\n",
    "        #'yanchor': 'top'\n",
    "                       }, \n",
    "    width=430,\n",
    "    height=400,\n",
    "                  \n",
    "  legend=dict( yanchor=\"bottom\",\n",
    "    y=0.5,\n",
    "    x=0.6,\n",
    "    \n",
    "    orientation=\"v\"\n",
    "),\n",
    "             \n",
    "    # group together boxes of the different\n",
    "    # traces for each value of x\n",
    "    boxmode='group',\n",
    "                  plot_bgcolor='rgba(0,0,0,0)'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days of training\",tickfont = dict(size=16),\n",
    "                 titlefont = dict(size=24),linewidth=1.4, linecolor='black',tick0 = 5,\n",
    "        dtick = 5,mirror=True)\n",
    "fig.update_yaxes(title_text=\"Performance(%)\",range=[0,100],tickfont = dict(size=16),titlefont = dict(size=24),\n",
    "                 linewidth=1.4, linecolor='black',tick0 = 20,\n",
    "        dtick = 20,mirror=True)\n",
    "fig.show()\n",
    "chart_studio.plotly.sign_in('vinylango', 'gybbJVWfRSUoTcRRSa6J')\n",
    "chart_studio.plotly.image.save_as(fig, filename='models_boxplot.png')\n",
    "#Image('models_boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-intro",
   "metadata": {},
   "source": [
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Reg, \n",
    "                name=\"RC\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'green', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Spatial, \n",
    "                name=\"SGS\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'purple', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                    \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Intervention, \n",
    "                name=\"IS/IM\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'orange', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Hs_and_sp, \n",
    "                name=\"HA/SP\",\n",
    "                    \n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'dodgerblue', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "               \n",
    "                        ))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-elite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-traveler",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
