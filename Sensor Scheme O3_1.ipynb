{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "correct-saskatchewan",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1600,
   "id": "exact-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Ref=pd.read_csv('Ref.csv')\n",
    "Ref[\"CO\"] = 1000 * Ref[\"CO\"]\n",
    "Ref['Date'] = pd.to_datetime(Ref['Date_Time'])\n",
    "Ref=Ref.set_index('Date')\n",
    "Ref.drop('Date_Time',axis = 1, inplace = True)\n",
    "Ref=Ref.resample('5min').mean()\n",
    "Ref=Ref[76463:137376]\n",
    "Ref_CO=Ref['CO'].to_list()\n",
    "Ref_NO2=Ref['NO2'].to_list()\n",
    "Ref_SO2=Ref['SO2'].to_list()\n",
    "Ref_O3=Ref['O3'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-subscriber",
   "metadata": {},
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('CO.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO['Ref']=Ref_CO\n",
    "WE=Data_CO['WE'].to_list()\n",
    "AE=Data_CO['AE'].to_list()\n",
    "\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "data = pd.read_csv('Conc_CO.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data.resample('5min').mean()\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna() \n",
    "CO_Data.shape\n",
    "CO_Data=CO_Data.resample('h').mean()\n",
    "CO_Data=CO_Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1603,
   "id": "common-tension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44410, 9)"
      ]
     },
     "execution_count": 1603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('CO.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO['Ref']=Ref_CO\n",
    "index_names = Data_CO[ (Data_CO['WE'] >1000)].index\n",
    "Data_CO.drop(index_names, inplace = True)\n",
    "WE=Data_CO['WE'].to_list()\n",
    "AE=Data_CO['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "data = pd.read_csv('Conc_CO.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_CO=data\n",
    "Data_CO.drop(index_names, inplace = True)\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_CO['Net Signal']=signal\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "#CO_Data=CO_Data.sample(frac=1)\n",
    "#CO_Data=CO_Data.resample('h').mean()\n",
    "CO_Data=CO_Data.dropna()\n",
    "CO_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1564,
   "id": "contained-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-03 16:00:00</th>\n",
       "      <td>592.411938</td>\n",
       "      <td>29.211333</td>\n",
       "      <td>53.102667</td>\n",
       "      <td>261.288900</td>\n",
       "      <td>137.737333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:00:00</th>\n",
       "      <td>610.622636</td>\n",
       "      <td>32.490426</td>\n",
       "      <td>36.890977</td>\n",
       "      <td>164.459300</td>\n",
       "      <td>119.947159</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:00:00</th>\n",
       "      <td>307.626618</td>\n",
       "      <td>34.926112</td>\n",
       "      <td>35.013036</td>\n",
       "      <td>211.526835</td>\n",
       "      <td>56.895186</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 12:00:00</th>\n",
       "      <td>384.296094</td>\n",
       "      <td>35.954278</td>\n",
       "      <td>32.022579</td>\n",
       "      <td>224.669739</td>\n",
       "      <td>69.308199</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 13:00:00</th>\n",
       "      <td>510.094653</td>\n",
       "      <td>34.204720</td>\n",
       "      <td>32.974775</td>\n",
       "      <td>234.974403</td>\n",
       "      <td>103.319737</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lab1       Temp         RH         Ref  Net Signal  \\\n",
       "Date                                                                            \n",
       "2019-10-03 16:00:00  592.411938  29.211333  53.102667  261.288900  137.737333   \n",
       "2019-10-07 10:00:00  610.622636  32.490426  36.890977  164.459300  119.947159   \n",
       "2019-10-07 11:00:00  307.626618  34.926112  35.013036  211.526835   56.895186   \n",
       "2019-10-07 12:00:00  384.296094  35.954278  32.022579  224.669739   69.308199   \n",
       "2019-10-07 13:00:00  510.094653  34.204720  32.974775  234.974403  103.319737   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour  \n",
       "Date                                                \n",
       "2019-10-03 16:00:00   10.0          3.0  3.0  16.0  \n",
       "2019-10-07 10:00:00   10.0          0.0  7.0  10.0  \n",
       "2019-10-07 11:00:00   10.0          0.0  7.0  11.0  \n",
       "2019-10-07 12:00:00   10.0          0.0  7.0  12.0  \n",
       "2019-10-07 13:00:00   10.0          0.0  7.0  13.0  "
      ]
     },
     "execution_count": 1564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_Data1=CO_Data[3:]\n",
    "CO_Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1565,
   "id": "express-indication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60913"
      ]
     },
     "execution_count": 1565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('O3.txt', header = None,low_memory=False)\n",
    "data.columns=['AE','WE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "Data_O3['Ref']=Ref_O3\n",
    "WE=Data_O3['WE'].to_list()\n",
    "AE=Data_O3['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna()\n",
    "data = pd.read_csv('Conc_O3.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna() \n",
    "O3_Data=O3_Data.resample('h').mean()\n",
    "O3_Data=O3_Data.dropna()\n",
    "O3_Data.head()\n",
    "\n",
    "ref_O3=Data_O3['Ref'].to_list()\n",
    "len(ref_O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "id": "maritime-effects",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_O3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-02 11:55:00</th>\n",
       "      <td>460.448301</td>\n",
       "      <td>26.378438</td>\n",
       "      <td>58.063437</td>\n",
       "      <td>15.230400</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>46.094860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:10:00</th>\n",
       "      <td>1364.583446</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>48.612609</td>\n",
       "      <td>6.665136</td>\n",
       "      <td>37.815652</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>55.810810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:15:00</th>\n",
       "      <td>224.159154</td>\n",
       "      <td>25.765087</td>\n",
       "      <td>48.441408</td>\n",
       "      <td>6.642805</td>\n",
       "      <td>12.275893</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>57.907075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:20:00</th>\n",
       "      <td>82.998996</td>\n",
       "      <td>26.120078</td>\n",
       "      <td>47.716553</td>\n",
       "      <td>2.844210</td>\n",
       "      <td>13.152720</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>58.880540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 15:45:00</th>\n",
       "      <td>566.301152</td>\n",
       "      <td>30.418466</td>\n",
       "      <td>50.153181</td>\n",
       "      <td>10.084125</td>\n",
       "      <td>9.323533</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>40.068225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                            \n",
       "2019-10-02 11:55:00   460.448301  26.378438  58.063437  15.230400    7.850000   \n",
       "2019-10-02 12:10:00  1364.583446  25.500000  48.612609   6.665136   37.815652   \n",
       "2019-10-02 12:15:00   224.159154  25.765087  48.441408   6.642805   12.275893   \n",
       "2019-10-02 12:20:00    82.998996  26.120078  47.716553   2.844210   13.152720   \n",
       "2019-10-02 15:45:00   566.301152  30.418466  50.153181  10.084125    9.323533   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour     Ref_O3  \n",
       "Date                                                           \n",
       "2019-10-02 11:55:00     10            2    2    11  46.094860  \n",
       "2019-10-02 12:10:00     10            2    2    12  55.810810  \n",
       "2019-10-02 12:15:00     10            2    2    12  57.907075  \n",
       "2019-10-02 12:20:00     10            2    2    12  58.880540  \n",
       "2019-10-02 15:45:00     10            2    2    15  40.068225  "
      ]
     },
     "execution_count": 1566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('NO2.txt', header = None,low_memory=False)\n",
    "data.columns=['WE','AE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_NO2=data\n",
    "Data_NO2['Ref']=Ref_NO2\n",
    "WE=Data_NO2['WE'].to_list()\n",
    "AE=Data_NO2['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_NO2['Net Signal']=signal\n",
    "Data_NO2['Month']=Data_NO2.index.month\n",
    "Data_NO2['Day_of_week']=Data_NO2.index.dayofweek\n",
    "Data_NO2['Day']=Data_NO2.index.day\n",
    "Data_NO2['Hour']=Data_NO2.index.hour\n",
    "NO2_Data=Data_NO2\n",
    "NO2_Data=NO2_Data[(NO2_Data[NO2_Data.columns] >= 0).all(axis=1)]\n",
    "NO2_Data=NO2_Data.dropna()\n",
    "data = pd.read_csv('Conc_NO2.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "subscript = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\") \n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_NO2=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_NO2['Net Signal']=signal\n",
    "Data_NO2['Month']=Data_NO2.index.month\n",
    "Data_NO2['Day_of_week']=Data_NO2.index.dayofweek\n",
    "Data_NO2['Day']=Data_NO2.index.day\n",
    "Data_NO2['Hour']=Data_NO2.index.hour\n",
    "Data_NO2['Ref_O3']=ref_O3\n",
    "NO2_Data=Data_NO2\n",
    "NO2_Data=NO2_Data[(NO2_Data[NO2_Data.columns] >= 0).all(axis=1)]\n",
    "NO2_Data=NO2_Data.dropna()\n",
    "NO2_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1598,
   "id": "recognized-watch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-02 11:55:00</th>\n",
       "      <td>621.625704</td>\n",
       "      <td>26.378438</td>\n",
       "      <td>58.063437</td>\n",
       "      <td>46.094860</td>\n",
       "      <td>3.605625</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>15.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:10:00</th>\n",
       "      <td>1788.609900</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>48.612609</td>\n",
       "      <td>55.810810</td>\n",
       "      <td>3.528696</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6.665136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:15:00</th>\n",
       "      <td>287.254970</td>\n",
       "      <td>25.765087</td>\n",
       "      <td>48.441408</td>\n",
       "      <td>57.907075</td>\n",
       "      <td>17.781453</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6.642805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:20:00</th>\n",
       "      <td>99.598353</td>\n",
       "      <td>26.120078</td>\n",
       "      <td>47.716553</td>\n",
       "      <td>58.880540</td>\n",
       "      <td>20.285180</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2.844210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:50:00</th>\n",
       "      <td>105.723457</td>\n",
       "      <td>32.399528</td>\n",
       "      <td>37.143389</td>\n",
       "      <td>48.533490</td>\n",
       "      <td>11.862076</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4.344894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:55:00</th>\n",
       "      <td>110.669169</td>\n",
       "      <td>32.289000</td>\n",
       "      <td>37.378125</td>\n",
       "      <td>45.984525</td>\n",
       "      <td>11.033542</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>4.166651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:00:00</th>\n",
       "      <td>108.194439</td>\n",
       "      <td>32.871031</td>\n",
       "      <td>36.805719</td>\n",
       "      <td>42.236993</td>\n",
       "      <td>10.815792</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>12.628474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:05:00</th>\n",
       "      <td>124.079250</td>\n",
       "      <td>33.118768</td>\n",
       "      <td>36.737316</td>\n",
       "      <td>46.640920</td>\n",
       "      <td>7.385167</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7.342928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:10:00</th>\n",
       "      <td>137.769717</td>\n",
       "      <td>33.883043</td>\n",
       "      <td>36.499822</td>\n",
       "      <td>38.007327</td>\n",
       "      <td>7.097694</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>18.422850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:15:00</th>\n",
       "      <td>125.494093</td>\n",
       "      <td>34.614665</td>\n",
       "      <td>36.048188</td>\n",
       "      <td>38.782210</td>\n",
       "      <td>8.868583</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>26.208070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:20:00</th>\n",
       "      <td>104.039089</td>\n",
       "      <td>34.744740</td>\n",
       "      <td>35.890073</td>\n",
       "      <td>45.677453</td>\n",
       "      <td>12.576750</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>9.359797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:25:00</th>\n",
       "      <td>160.778249</td>\n",
       "      <td>34.300540</td>\n",
       "      <td>36.031600</td>\n",
       "      <td>40.244075</td>\n",
       "      <td>7.376636</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>27.102994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:30:00</th>\n",
       "      <td>143.808110</td>\n",
       "      <td>35.063443</td>\n",
       "      <td>35.170690</td>\n",
       "      <td>37.248123</td>\n",
       "      <td>10.116958</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>22.946173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:35:00</th>\n",
       "      <td>152.773802</td>\n",
       "      <td>35.644755</td>\n",
       "      <td>34.368553</td>\n",
       "      <td>33.038515</td>\n",
       "      <td>9.458984</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>30.257515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:40:00</th>\n",
       "      <td>122.877435</td>\n",
       "      <td>36.030306</td>\n",
       "      <td>33.763306</td>\n",
       "      <td>46.101530</td>\n",
       "      <td>13.059146</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>10.350441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:45:00</th>\n",
       "      <td>153.954766</td>\n",
       "      <td>35.974952</td>\n",
       "      <td>33.479665</td>\n",
       "      <td>43.446910</td>\n",
       "      <td>10.461502</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>10.972485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:50:00</th>\n",
       "      <td>150.168431</td>\n",
       "      <td>36.280390</td>\n",
       "      <td>32.930087</td>\n",
       "      <td>46.036247</td>\n",
       "      <td>11.037185</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>11.394061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:55:00</th>\n",
       "      <td>145.934737</td>\n",
       "      <td>36.586708</td>\n",
       "      <td>32.431417</td>\n",
       "      <td>47.910815</td>\n",
       "      <td>12.648250</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>8.230621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 12:00:00</th>\n",
       "      <td>163.831278</td>\n",
       "      <td>36.205083</td>\n",
       "      <td>32.600444</td>\n",
       "      <td>47.067593</td>\n",
       "      <td>11.464513</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>12.473997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 12:05:00</th>\n",
       "      <td>186.771478</td>\n",
       "      <td>36.005711</td>\n",
       "      <td>32.610535</td>\n",
       "      <td>48.428010</td>\n",
       "      <td>9.082197</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>6.145587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Lab1       Temp         RH        Ref  Net Signal  \\\n",
       "Date                                                                            \n",
       "2019-10-02 11:55:00   621.625704  26.378438  58.063437  46.094860    3.605625   \n",
       "2019-10-02 12:10:00  1788.609900  25.500000  48.612609  55.810810    3.528696   \n",
       "2019-10-02 12:15:00   287.254970  25.765087  48.441408  57.907075   17.781453   \n",
       "2019-10-02 12:20:00    99.598353  26.120078  47.716553  58.880540   20.285180   \n",
       "2019-10-07 10:50:00   105.723457  32.399528  37.143389  48.533490   11.862076   \n",
       "2019-10-07 10:55:00   110.669169  32.289000  37.378125  45.984525   11.033542   \n",
       "2019-10-07 11:00:00   108.194439  32.871031  36.805719  42.236993   10.815792   \n",
       "2019-10-07 11:05:00   124.079250  33.118768  36.737316  46.640920    7.385167   \n",
       "2019-10-07 11:10:00   137.769717  33.883043  36.499822  38.007327    7.097694   \n",
       "2019-10-07 11:15:00   125.494093  34.614665  36.048188  38.782210    8.868583   \n",
       "2019-10-07 11:20:00   104.039089  34.744740  35.890073  45.677453   12.576750   \n",
       "2019-10-07 11:25:00   160.778249  34.300540  36.031600  40.244075    7.376636   \n",
       "2019-10-07 11:30:00   143.808110  35.063443  35.170690  37.248123   10.116958   \n",
       "2019-10-07 11:35:00   152.773802  35.644755  34.368553  33.038515    9.458984   \n",
       "2019-10-07 11:40:00   122.877435  36.030306  33.763306  46.101530   13.059146   \n",
       "2019-10-07 11:45:00   153.954766  35.974952  33.479665  43.446910   10.461502   \n",
       "2019-10-07 11:50:00   150.168431  36.280390  32.930087  46.036247   11.037185   \n",
       "2019-10-07 11:55:00   145.934737  36.586708  32.431417  47.910815   12.648250   \n",
       "2019-10-07 12:00:00   163.831278  36.205083  32.600444  47.067593   11.464513   \n",
       "2019-10-07 12:05:00   186.771478  36.005711  32.610535  48.428010    9.082197   \n",
       "\n",
       "                     Month  Day_of_week  Day  Hour    Ref_NO2  \n",
       "Date                                                           \n",
       "2019-10-02 11:55:00     10            2    2    11  15.230400  \n",
       "2019-10-02 12:10:00     10            2    2    12   6.665136  \n",
       "2019-10-02 12:15:00     10            2    2    12   6.642805  \n",
       "2019-10-02 12:20:00     10            2    2    12   2.844210  \n",
       "2019-10-07 10:50:00     10            0    7    10   4.344894  \n",
       "2019-10-07 10:55:00     10            0    7    10   4.166651  \n",
       "2019-10-07 11:00:00     10            0    7    11  12.628474  \n",
       "2019-10-07 11:05:00     10            0    7    11   7.342928  \n",
       "2019-10-07 11:10:00     10            0    7    11  18.422850  \n",
       "2019-10-07 11:15:00     10            0    7    11  26.208070  \n",
       "2019-10-07 11:20:00     10            0    7    11   9.359797  \n",
       "2019-10-07 11:25:00     10            0    7    11  27.102994  \n",
       "2019-10-07 11:30:00     10            0    7    11  22.946173  \n",
       "2019-10-07 11:35:00     10            0    7    11  30.257515  \n",
       "2019-10-07 11:40:00     10            0    7    11  10.350441  \n",
       "2019-10-07 11:45:00     10            0    7    11  10.972485  \n",
       "2019-10-07 11:50:00     10            0    7    11  11.394061  \n",
       "2019-10-07 11:55:00     10            0    7    11   8.230621  \n",
       "2019-10-07 12:00:00     10            0    7    12  12.473997  \n",
       "2019-10-07 12:05:00     10            0    7    12   6.145587  "
      ]
     },
     "execution_count": 1598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "Ref_O3=10*np.array(Ref_O3)\n",
    "data = pd.read_csv('O3.txt', header = None,low_memory=False)\n",
    "data.columns=['AE','WE','Temp','RH','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "Data_O3['Ref']=Ref_O3\n",
    "WE=Data_O3['WE'].to_list()\n",
    "AE=Data_O3['AE'].to_list()\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data=O3_Data.dropna()\n",
    "data = pd.read_csv('Conc_O3.txt', header = None,low_memory=False)\n",
    "data.columns=['Lab1','Temp','RH','Time','Ref']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "data=data.resample('5min').mean()\n",
    "Data_O3=data\n",
    "signal=np.array(WE)-np.array(AE)\n",
    "Data_O3['Net Signal']=signal\n",
    "Data_O3['Month']=Data_O3.index.month\n",
    "Data_O3['Day_of_week']=Data_O3.index.dayofweek\n",
    "Data_O3['Day']=Data_O3.index.day\n",
    "Data_O3['Hour']=Data_O3.index.hour\n",
    "ref_NO2=Data_NO2['Ref'].to_list()\n",
    "Data_O3['Ref_NO2']=ref_NO2\n",
    "O3_Data=Data_O3\n",
    "O3_Data=O3_Data[(O3_Data[O3_Data.columns] >= 0).all(axis=1)]\n",
    "O3_Data.dropna()\n",
    "O3_Data.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1599,
   "id": "arbitrary-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35976, 10)"
      ]
     },
     "execution_count": 1599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "O3_Data=O3_Data[(np.abs(stats.zscore(O3_Data)) < 3).all(axis=1)]\n",
    "O3_Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-doctor",
   "metadata": {},
   "source": [
    "#  New Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-witch",
   "metadata": {},
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = pd.read_csv('CO_2.txt', header = None,low_memory=False)\n",
    "data.columns=['C1','C2','C3','C4','Temp','RH','Ref','Time']\n",
    "Time=data['Time'].to_list()\n",
    "time=[]\n",
    "for i in range(len(Time)):\n",
    "    time.append(float(abs(Time[i])))\n",
    "Time=np.array(time)\n",
    "Date=pd.to_datetime(Time-719529,unit='d').round('s')\n",
    "data['Date'] = Date.tolist()\n",
    "data=data.set_index('Date')\n",
    "data.drop('Time',axis = 1, inplace = True)\n",
    "\n",
    "data=data[['C1','Temp','RH','Ref']]\n",
    "Data_CO=data\n",
    "Data_CO['Month']=Data_CO.index.month\n",
    "Data_CO['Day_of_week']=Data_CO.index.dayofweek\n",
    "#Data_CO['Day']=Data_CO.index.day\n",
    "Data_CO['Hour']=Data_CO.index.hour\n",
    "CO_Data=Data_CO\n",
    "CO_Data=CO_Data[(CO_Data[CO_Data.columns] >= 0).all(axis=1)]\n",
    "CO_Data=CO_Data.dropna()\n",
    "CO_Data=CO_Data[:3446]\n",
    "CO_Data=CO_Data.resample('h').mean()\n",
    "CO_Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-trash",
   "metadata": {},
   "source": [
    "Temp1=CO_Data['Temp'].to_list()\n",
    "RH1=CO_Data['RH'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-parameter",
   "metadata": {},
   "source": [
    "CO_Data1.tail()\n",
    "CO_Data1=CO_Data1.resample('h').mean()\n",
    "CO_Data1['Temp1']=Temp1\n",
    "CO_Data1['RH1']=RH1\n",
    "CO_Data1=CO_Data1.dropna()\n",
    "Temp=CO_Data1['Temp'].to_list()\n",
    "RH=CO_Data1['RH'].to_list()\n",
    "Temp2=CO_Data1['Temp1'].to_list()\n",
    "RH2=CO_Data1['RH1'].to_list()\n",
    "\n",
    "ind=[i for i in range(len(Temp))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-grave",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ind, RH, color='green')\n",
    "plt.plot(ind, RH2, color='red')\n",
    "plt.ylabel('RH')\n",
    "plt.xlabel('Hours')\n",
    "plt.legend(['Sensor','Station'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-april",
   "metadata": {},
   "source": [
    "NO2_Data1=NO2_Data.resample('h').mean()\n",
    "NO2_Data1=NO2_Data1[28:]\n",
    "NO2_Data1['Temp1']=Temp1\n",
    "NO2_Data1['RH1']=RH1\n",
    "NO2_Data1=NO2_Data1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-least",
   "metadata": {},
   "source": [
    "O3_Data1=O3_Data.resample('h').mean()\n",
    "O3_Data1=O3_Data1[28:]\n",
    "O3_Data1['Temp1']=Temp1\n",
    "O3_Data1['RH1']=RH1\n",
    "O3_Data1=O3_Data1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-paper",
   "metadata": {},
   "source": [
    "CO_Data=CO_Data1\n",
    "NO2_Data=NO2_Data1\n",
    "O3_Data=O3_Data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1569,
   "id": "vital-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    mbe=np.mean(true-pred)\n",
    "    return mbe\n",
    "def CRMSE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    crmse=np.sqrt(np.mean(((true-np.mean(true))-(pred-np.mean(pred)))**2))\n",
    "    if np.std(pred)>np.std(true):\n",
    "        crmse=crmse\n",
    "    else:\n",
    "        crmse=-crmse\n",
    "    return crmse\n",
    "\n",
    "def sMAE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    smae=np.mean(abs(true-pred)/((abs(true)+abs(pred))/2))\n",
    "    return smae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1570,
   "id": "focal-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sMAE(true,pred):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    mae=sum(abs(pred-true))/len(pred)\n",
    "    nmae=mae/np.mean(true)\n",
    "    return 1-nmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1571,
   "id": "limiting-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "p = np.array([ 0.9])\n",
    "df=np.array(range(1,10000))\n",
    "chi = [0]+list(chi2.isf(p, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1572,
   "id": "tight-reputation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.66023432606575"
      ]
     },
     "execution_count": 1572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "#find T critical value\n",
    "scipy.stats.t.ppf(q=.95,df=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1573,
   "id": "prime-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred,true):\n",
    "    pred=list(pred)\n",
    "    true=list(true)\n",
    "    for i in range(len(true)):\n",
    "        if true[i]==0:\n",
    "            true.pop(i)\n",
    "            pred.pop(i)\n",
    "    pred=np.array(pred)\n",
    "    true=np.array(true)\n",
    "    d=((pred-true)/np.mean(true))*100\n",
    "    n=len(pred)\n",
    "    A=np.sqrt((n-1)/chi[n-1])\n",
    "    cv=np.sqrt(((n*sum(abs(d)**2)-(sum(abs(d)))**2)/(n*(n-1))))*A\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1574,
   "id": "polish-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(pred,true):\n",
    "    pred=list(pred)\n",
    "    true=list(true)\n",
    "    for i in range(len(true)):\n",
    "        if true[i]==0:\n",
    "            true.pop(i)\n",
    "            pred.pop(i)\n",
    "    pred=np.array(pred)\n",
    "    true=np.array(true)\n",
    "    d=((pred-true)/np.mean(true))*100\n",
    "    n=len(pred)\n",
    "    AB=sum(abs(d))/n\n",
    "    AS=np.sqrt(((n*sum(abs(d)**2)-(sum(abs(d)))**2)/(n*(n-1))))\n",
    "    t=scipy.stats.t.ppf(q=.95,df=n-1)\n",
    "    bias=AB+(t*AS/np.sqrt(n))\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1575,
   "id": "fixed-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOA(pred,true):\n",
    "    true=np.array(true)\n",
    "    pred=np.array(pred)\n",
    "    frac=sum(abs(true-pred))/sum((abs(pred-np.mean(true))+abs(true-np.mean(true))))\n",
    "    d=1-frac\n",
    "    return d  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1576,
   "id": "challenging-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REF(pred,y_test,alpha):\n",
    "    import random\n",
    "    cal=np.array(pred)\n",
    "    ref=np.array(y_test.to_list())\n",
    "    ref_mean=np.mean(ref)\n",
    "    cal_mean=np.mean(cal)\n",
    "    prec=np.array([20 for i in range(len(ref))])\n",
    "    u=np.maximum(prec,0.001*ref)\n",
    "    #cal=np.log(cal)\n",
    "    #ref=np.log(ref)\n",
    "    sx_s=(1/len(ref))*sum((ref-ref_mean)**2)\n",
    "    sy_s=(1/len(cal))*sum((cal-cal_mean)**2)\n",
    "    sxy=(1/len(cal))*sum((cal-cal_mean)*(ref-ref_mean))\n",
    "    beta_1=((sy_s-alpha*sx_s)+np.sqrt((sy_s-sx_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    beta_0=cal_mean-beta_1*ref_mean\n",
    "    RSS=sum((cal-beta_0-beta_1*ref)**2-(beta_1**2+alpha)*u**2)\n",
    "    du_s=RSS/(len(cal)-2)\n",
    "    Beta_1=((sy_s-alpha*sx_s-du_s)+np.sqrt((sy_s-alpha*sx_s-du_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    Beta_0=cal_mean-Beta_1*ref_mean\n",
    "    P1=(RSS/(len(cal)-2))\n",
    "    P2=(Beta_1**2+alpha)*u**2+(-2*Beta_1**2+2*Beta_1-1)*u**2\n",
    "    P3=(Beta_0+(Beta_1-1)*ref)**2\n",
    "    P=[]\n",
    "    for i in range(len(P3)):\n",
    "        P.append(P1+P2[i]+P3[i])\n",
    "    for i in range(len(P)):\n",
    "        if P[i]<0:\n",
    "            P[i]=random.randint(1,100)\n",
    "    u_cal=(2*np.sqrt(np.array(P))/cal)*100\n",
    "    #u_cal=((2*np.sqrt((RSS/(len(cal)-2))+(1-(beta_1-1)**2)*(0.08*ref)**2+(Beta_0+(Beta_1-1)*ref)**2))/cal)*100\n",
    "    return u_cal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1577,
   "id": "portuguese-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REF2(pred,y_test,alpha,LV):\n",
    "    import random\n",
    "    cal=np.array(pred)\n",
    "    ref=np.array(y_test.to_list())\n",
    "    ref_mean=np.mean(ref)\n",
    "    cal_mean=np.mean(cal)\n",
    "    prec=np.array([20 for i in range(len(ref))])\n",
    "    u=0.001*ref\n",
    "    #cal=np.log(cal)\n",
    "    #ref=np.log(ref)\n",
    "    sx_s=(1/len(ref))*sum((ref-ref_mean)**2)\n",
    "    sy_s=(1/len(cal))*sum((cal-cal_mean)**2)\n",
    "    sxy=(1/len(cal))*sum((cal-cal_mean)*(ref-ref_mean))\n",
    "    #beta_1=((sy_s-sx_s)+np.sqrt((sy_s-sx_s)**2+4*sxy**2))/(2*sxy)\n",
    "    beta_1=((sy_s-alpha*sx_s)+np.sqrt((sy_s-sx_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    beta_0=cal_mean-beta_1*ref_mean\n",
    "    RSS=sum((cal-beta_0-beta_1*ref)**2-(beta_1**2+alpha)*(0.001*LV)**2)\n",
    "    du_s=RSS/(len(cal)-2)\n",
    "    #Beta_1=((sy_s-sx_s-du_s)+np.sqrt((sy_s-sx_s-du_s)**2+4*sxy**2))/(2*sxy)\n",
    "    Beta_1=((sy_s-alpha*sx_s-du_s)+np.sqrt((sy_s-alpha*sx_s-du_s)**2+4*alpha*sxy**2))/(2*sxy)\n",
    "    Beta_0=cal_mean-Beta_1*ref_mean\n",
    "    P1=(RSS/(len(cal)-2))\n",
    "    P2=(Beta_1**2+alpha)*(0.001*LV)**2+(-2*Beta_1**2+2*Beta_1-1)*(0.001*LV)**2\n",
    "    P3=(Beta_0+(Beta_1-1)*LV)**2\n",
    "    P=P1+P2+P3\n",
    "    if P<0:\n",
    "        P=random.randint(1,100)\n",
    "    u_cal=(2*np.sqrt(P)/(Beta_0+Beta_1*LV))*100\n",
    "    #u_cal=((2*np.sqrt((RSS/(len(cal)-2))+(1-(beta_1-1)**2)*0.1+(Beta_0+(Beta_1-1)*ref)**2))/cal)*100\n",
    "    return u_cal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1578,
   "id": "comic-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1579,
   "id": "completed-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "id": "attempted-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1581,
   "id": "attached-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1582,
   "id": "rational-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "id": "featured-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "id": "assisted-abuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-02 12:20:00</th>\n",
       "      <td>0.944380</td>\n",
       "      <td>1.535880</td>\n",
       "      <td>-1.311972</td>\n",
       "      <td>2.679186</td>\n",
       "      <td>2.822459</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-0.512536</td>\n",
       "      <td>-1.752588</td>\n",
       "      <td>0.085617</td>\n",
       "      <td>-1.300117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:50:00</th>\n",
       "      <td>1.100582</td>\n",
       "      <td>2.474765</td>\n",
       "      <td>-1.924873</td>\n",
       "      <td>1.992030</td>\n",
       "      <td>0.779280</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-1.503068</td>\n",
       "      <td>-1.145237</td>\n",
       "      <td>-0.187895</td>\n",
       "      <td>-1.172223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 10:55:00</th>\n",
       "      <td>1.226708</td>\n",
       "      <td>2.458239</td>\n",
       "      <td>-1.911266</td>\n",
       "      <td>1.822751</td>\n",
       "      <td>0.578304</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-1.503068</td>\n",
       "      <td>-1.145237</td>\n",
       "      <td>-0.187895</td>\n",
       "      <td>-1.187413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:00:00</th>\n",
       "      <td>1.163597</td>\n",
       "      <td>2.545263</td>\n",
       "      <td>-1.944447</td>\n",
       "      <td>1.573875</td>\n",
       "      <td>0.525485</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-1.503068</td>\n",
       "      <td>-1.145237</td>\n",
       "      <td>-0.051139</td>\n",
       "      <td>-0.466262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-07 11:05:00</th>\n",
       "      <td>1.568692</td>\n",
       "      <td>2.582304</td>\n",
       "      <td>-1.948413</td>\n",
       "      <td>1.866343</td>\n",
       "      <td>-0.306677</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-1.503068</td>\n",
       "      <td>-1.145237</td>\n",
       "      <td>-0.051139</td>\n",
       "      <td>-0.916718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 23:35:00</th>\n",
       "      <td>0.242063</td>\n",
       "      <td>0.217307</td>\n",
       "      <td>0.511996</td>\n",
       "      <td>-0.878944</td>\n",
       "      <td>0.096611</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>1.770049</td>\n",
       "      <td>1.589933</td>\n",
       "      <td>0.780756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 23:40:00</th>\n",
       "      <td>0.178176</td>\n",
       "      <td>0.204326</td>\n",
       "      <td>0.520947</td>\n",
       "      <td>-1.109448</td>\n",
       "      <td>0.146522</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>1.770049</td>\n",
       "      <td>1.589933</td>\n",
       "      <td>0.989039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 23:45:00</th>\n",
       "      <td>0.197486</td>\n",
       "      <td>0.191013</td>\n",
       "      <td>0.526722</td>\n",
       "      <td>-1.075492</td>\n",
       "      <td>0.099167</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>1.770049</td>\n",
       "      <td>1.589933</td>\n",
       "      <td>0.750136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 23:50:00</th>\n",
       "      <td>0.198568</td>\n",
       "      <td>0.177604</td>\n",
       "      <td>0.535117</td>\n",
       "      <td>-1.038746</td>\n",
       "      <td>0.188429</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>1.770049</td>\n",
       "      <td>1.589933</td>\n",
       "      <td>0.563796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31 23:55:00</th>\n",
       "      <td>0.175013</td>\n",
       "      <td>0.164739</td>\n",
       "      <td>0.544589</td>\n",
       "      <td>-1.072130</td>\n",
       "      <td>0.276868</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>-0.017271</td>\n",
       "      <td>1.770049</td>\n",
       "      <td>1.589933</td>\n",
       "      <td>0.656916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4449 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lab1      Temp        RH       Ref  Net Signal  \\\n",
       "Date                                                                      \n",
       "2019-10-02 12:20:00  0.944380  1.535880 -1.311972  2.679186    2.822459   \n",
       "2019-10-07 10:50:00  1.100582  2.474765 -1.924873  1.992030    0.779280   \n",
       "2019-10-07 10:55:00  1.226708  2.458239 -1.911266  1.822751    0.578304   \n",
       "2019-10-07 11:00:00  1.163597  2.545263 -1.944447  1.573875    0.525485   \n",
       "2019-10-07 11:05:00  1.568692  2.582304 -1.948413  1.866343   -0.306677   \n",
       "...                       ...       ...       ...       ...         ...   \n",
       "2019-10-31 23:35:00  0.242063  0.217307  0.511996 -0.878944    0.096611   \n",
       "2019-10-31 23:40:00  0.178176  0.204326  0.520947 -1.109448    0.146522   \n",
       "2019-10-31 23:45:00  0.197486  0.191013  0.526722 -1.075492    0.099167   \n",
       "2019-10-31 23:50:00  0.198568  0.177604  0.535117 -1.038746    0.188429   \n",
       "2019-10-31 23:55:00  0.175013  0.164739  0.544589 -1.072130    0.276868   \n",
       "\n",
       "                        Month  Day_of_week       Day      Hour   Ref_NO2  \n",
       "Date                                                                      \n",
       "2019-10-02 12:20:00  0.766717    -0.512536 -1.752588  0.085617 -1.300117  \n",
       "2019-10-07 10:50:00  0.766717    -1.503068 -1.145237 -0.187895 -1.172223  \n",
       "2019-10-07 10:55:00  0.766717    -1.503068 -1.145237 -0.187895 -1.187413  \n",
       "2019-10-07 11:00:00  0.766717    -1.503068 -1.145237 -0.051139 -0.466262  \n",
       "2019-10-07 11:05:00  0.766717    -1.503068 -1.145237 -0.051139 -0.916718  \n",
       "...                       ...          ...       ...       ...       ...  \n",
       "2019-10-31 23:35:00  0.766717    -0.017271  1.770049  1.589933  0.780756  \n",
       "2019-10-31 23:40:00  0.766717    -0.017271  1.770049  1.589933  0.989039  \n",
       "2019-10-31 23:45:00  0.766717    -0.017271  1.770049  1.589933  0.750136  \n",
       "2019-10-31 23:50:00  0.766717    -0.017271  1.770049  1.589933  0.563796  \n",
       "2019-10-31 23:55:00  0.766717    -0.017271  1.770049  1.589933  0.656916  \n",
       "\n",
       "[4449 rows x 10 columns]"
      ]
     },
     "execution_count": 1584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=[x for _, x in O3_Data.groupby('Month')]\n",
    "data_oct=df1[4]\n",
    "#data_oct=data_oct.sample(frac=1)\n",
    "data_nov=df1[5]\n",
    "#data_nov=data_nov.sample(frac=1)\n",
    "data_dec=df1[6]\n",
    "#data_dec=data_dec.sample(frac=1)\n",
    "data_jan=df1[0]\n",
    "#data_jan=data_jan.sample(frac=1)\n",
    "data_feb=df1[1]\n",
    "#data_feb=data_feb.sample(frac=1)\n",
    "data_mar=df1[2]\n",
    "#data_mar=data_mar.sample(frac=1)\n",
    "data_apr=df1[3]\n",
    "#data_apr=data_apr.sample(frac=1)\n",
    "data=[data_oct,data_nov,data_dec,data_jan,data_feb,data_mar]\n",
    "data_oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "id": "viral-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Oct=data_oct[::3]\n",
    "data_Oct=data_Oct.dropna()\n",
    "data_Nov=data_nov[::3]\n",
    "data_Nov=data_Nov.dropna()\n",
    "data_Dec=data_dec[::3]\n",
    "data_Dec=data_Dec.dropna()\n",
    "data_Jan=data_jan[::3]\n",
    "data_Jan=data_Jan.dropna()\n",
    "data_Feb=data_feb[::3]\n",
    "data_Feb=data_Feb.dropna()\n",
    "data_Mar=data_mar[::3]\n",
    "data_Mar=data_Mar.dropna()\n",
    "data_Apr=data_apr[::3]\n",
    "data_Apr=data_Apr.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-video",
   "metadata": {},
   "source": [
    "Temp=data_Jan['RH'].to_list()\n",
    "len(Temp)\n",
    "\n",
    "ind=[i for i in range(len(Temp))]\n",
    "plt.plot(ind,Temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "id": "experienced-acting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:00:00</th>\n",
       "      <td>1.500994</td>\n",
       "      <td>-0.107287</td>\n",
       "      <td>-0.952319</td>\n",
       "      <td>0.615877</td>\n",
       "      <td>-1.157991</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.076333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:05:00</th>\n",
       "      <td>0.902736</td>\n",
       "      <td>-0.113566</td>\n",
       "      <td>-0.911753</td>\n",
       "      <td>0.658288</td>\n",
       "      <td>-1.006825</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.261015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:10:00</th>\n",
       "      <td>0.693302</td>\n",
       "      <td>-0.385197</td>\n",
       "      <td>-0.538060</td>\n",
       "      <td>0.530235</td>\n",
       "      <td>-1.714211</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.632641</td>\n",
       "      <td>-0.093601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:15:00</th>\n",
       "      <td>0.879229</td>\n",
       "      <td>-0.397751</td>\n",
       "      <td>-0.532769</td>\n",
       "      <td>0.396740</td>\n",
       "      <td>-1.753739</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.632641</td>\n",
       "      <td>0.419292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:20:00</th>\n",
       "      <td>0.846685</td>\n",
       "      <td>-0.410153</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>0.450265</td>\n",
       "      <td>-1.685907</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.632641</td>\n",
       "      <td>0.273338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lab1      Temp        RH       Ref  Net Signal  \\\n",
       "Date                                                                      \n",
       "2020-02-14 15:00:00  1.500994 -0.107287 -0.952319  0.615877   -1.157991   \n",
       "2020-02-14 15:05:00  0.902736 -0.113566 -0.911753  0.658288   -1.006825   \n",
       "2020-02-14 16:10:00  0.693302 -0.385197 -0.538060  0.530235   -1.714211   \n",
       "2020-02-14 16:15:00  0.879229 -0.397751 -0.532769  0.396740   -1.753739   \n",
       "2020-02-14 16:20:00  0.846685 -0.410153 -0.525930  0.450265   -1.685907   \n",
       "\n",
       "                       Month  Day_of_week       Day      Hour   Ref_NO2  \n",
       "Date                                                                     \n",
       "2020-02-14 15:00:00 -1.01075     0.477995 -0.294945  0.495885  0.076333  \n",
       "2020-02-14 15:05:00 -1.01075     0.477995 -0.294945  0.495885  0.261015  \n",
       "2020-02-14 16:10:00 -1.01075     0.477995 -0.294945  0.632641 -0.093601  \n",
       "2020-02-14 16:15:00 -1.01075     0.477995 -0.294945  0.632641  0.419292  \n",
       "2020-02-14 16:20:00 -1.01075     0.477995 -0.294945  0.632641  0.273338  "
      ]
     },
     "execution_count": 1586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=[x for _, x in O3_Data.groupby('Month')]\n",
    "data_oct2=df2[4]\n",
    "#data_oct2=data_oct2.sample(frac=1)\n",
    "data_nov2=df2[5]\n",
    "#data_nov2=data_nov2.sample(frac=1)\n",
    "data_dec2=df2[6]\n",
    "#data_dec2=data_dec2.sample(frac=1)\n",
    "data_jan2=df2[0]\n",
    "#data_jan2=data_jan2.sample(frac=1)\n",
    "data_feb2=df2[1]\n",
    "#data_feb2=data_feb2.sample(frac=1)\n",
    "data_mar2=df2[2]\n",
    "#data_mar2=data_mar2.sample(frac=1)\n",
    "data_apr2=df2[3]\n",
    "#data_apr2=data_apr2.sample(frac=1)\n",
    "data=[data_oct2,data_nov2,data_dec2,data_jan2,data_feb2,data_mar2]\n",
    "data_feb2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "id": "natural-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Oct2=data_oct2[::3]\n",
    "data_Oct2=data_Oct2.dropna()\n",
    "data_Nov2=data_nov2[::3]\n",
    "data_Nov2=data_Nov2.dropna()\n",
    "data_Dec2=data_dec2[::3]\n",
    "data_Dec2=data_Dec2.dropna()\n",
    "data_Jan2=data_jan2[::3]\n",
    "data_Jan2=data_Jan2.dropna()\n",
    "data_Feb2=data_feb2[::3]\n",
    "data_Feb2=data_Feb2.dropna()\n",
    "data_Mar2=data_mar2[::3]\n",
    "data_Mar2=data_Mar2.dropna()\n",
    "data_Apr2=data_apr2[::3]\n",
    "data_Apr2=data_Apr2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "id": "controlled-removal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab1</th>\n",
       "      <th>Temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ref</th>\n",
       "      <th>Net Signal</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day_of_week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Ref_NO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:00:00</th>\n",
       "      <td>1.500994</td>\n",
       "      <td>-0.107287</td>\n",
       "      <td>-0.952319</td>\n",
       "      <td>0.615877</td>\n",
       "      <td>-1.157991</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.076333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 15:05:00</th>\n",
       "      <td>0.902736</td>\n",
       "      <td>-0.113566</td>\n",
       "      <td>-0.911753</td>\n",
       "      <td>0.658288</td>\n",
       "      <td>-1.006825</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.261015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:10:00</th>\n",
       "      <td>0.693302</td>\n",
       "      <td>-0.385197</td>\n",
       "      <td>-0.538060</td>\n",
       "      <td>0.530235</td>\n",
       "      <td>-1.714211</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.632641</td>\n",
       "      <td>-0.093601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:15:00</th>\n",
       "      <td>0.879229</td>\n",
       "      <td>-0.397751</td>\n",
       "      <td>-0.532769</td>\n",
       "      <td>0.396740</td>\n",
       "      <td>-1.753739</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.632641</td>\n",
       "      <td>0.419292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14 16:20:00</th>\n",
       "      <td>0.846685</td>\n",
       "      <td>-0.410153</td>\n",
       "      <td>-0.525930</td>\n",
       "      <td>0.450265</td>\n",
       "      <td>-1.685907</td>\n",
       "      <td>-1.01075</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.294945</td>\n",
       "      <td>0.632641</td>\n",
       "      <td>0.273338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lab1      Temp        RH       Ref  Net Signal  \\\n",
       "Date                                                                      \n",
       "2020-02-14 15:00:00  1.500994 -0.107287 -0.952319  0.615877   -1.157991   \n",
       "2020-02-14 15:05:00  0.902736 -0.113566 -0.911753  0.658288   -1.006825   \n",
       "2020-02-14 16:10:00  0.693302 -0.385197 -0.538060  0.530235   -1.714211   \n",
       "2020-02-14 16:15:00  0.879229 -0.397751 -0.532769  0.396740   -1.753739   \n",
       "2020-02-14 16:20:00  0.846685 -0.410153 -0.525930  0.450265   -1.685907   \n",
       "\n",
       "                       Month  Day_of_week       Day      Hour   Ref_NO2  \n",
       "Date                                                                     \n",
       "2020-02-14 15:00:00 -1.01075     0.477995 -0.294945  0.495885  0.076333  \n",
       "2020-02-14 15:05:00 -1.01075     0.477995 -0.294945  0.495885  0.261015  \n",
       "2020-02-14 16:10:00 -1.01075     0.477995 -0.294945  0.632641 -0.093601  \n",
       "2020-02-14 16:15:00 -1.01075     0.477995 -0.294945  0.632641  0.419292  \n",
       "2020-02-14 16:20:00 -1.01075     0.477995 -0.294945  0.632641  0.273338  "
      ]
     },
     "execution_count": 1588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=[x for _, x in O3_Data.groupby('Month')]\n",
    "data_oct3=df3[4]\n",
    "#data_oct3=data_oct3.sample(frac=1)\n",
    "data_nov3=df3[5]\n",
    "#data_nov3=data_nov3.sample(frac=1)\n",
    "data_dec3=df3[6]\n",
    "#data_dec3=data_dec3.sample(frac=1)\n",
    "data_jan3=df3[0]\n",
    "#data_jan3=data_jan3.sample(frac=1)\n",
    "data_feb3=df3[1]\n",
    "#data_feb3=data_feb3.sample(frac=1)\n",
    "data_mar3=df3[2]\n",
    "#data_mar3=data_mar3.sample(frac=1)\n",
    "data_apr3=df3[3]\n",
    "#data_apr3=data_apr3.sample(frac=1)\n",
    "data=[data_oct3,data_nov3,data_dec3,data_jan3,data_feb3,data_mar3]\n",
    "data_feb3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "id": "black-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Oct3=data_oct3[::3]\n",
    "data_Oct3=data_Oct3.dropna()\n",
    "data_Nov3=data_nov3[::3]\n",
    "data_Nov3=data_Nov3.dropna()\n",
    "data_Dec3=data_dec3[::3]\n",
    "data_Dec3=data_Dec3.dropna()\n",
    "data_Jan3=data_jan3[::3]\n",
    "data_Jan3=data_Jan3.dropna()\n",
    "data_Feb3=data_feb3[::3]\n",
    "data_Feb3=data_Feb3.dropna()\n",
    "data_Mar3=data_mar3[::3]\n",
    "data_Mar3=data_Mar3.dropna()\n",
    "data_Apr3=data_apr3[::3]\n",
    "data_Apr3=data_Apr3.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "id": "round-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "id": "peaceful-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import  XGBRegressor\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "# create an xgboost regression model\n",
    "#n_estimators=10000, max_depth=5, eta=0.01, subsample=0.9,colsample_bytree=0.4,alpha=10\n",
    "model = XGBRegressor(n_estimators=10000, max_depth=5, eta=0.01, subsample=0.9, \n",
    "                     colsample_bytree=0.4,alpha=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-michael",
   "metadata": {},
   "source": [
    "# Mothly schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-interstate",
   "metadata": {},
   "source": [
    "# Oct 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-andrew",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1592,
   "id": "empirical-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "#from sklearn.metrics import mean_absolute_error as mae\n",
    "#from sklearn.metrics import mean_absolute_percentage_error\n",
    "import sklearn.metrics as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "  \n",
    " # create regressor object\n",
    "regressor = RandomForestRegressor(n_estimators = 500,min_samples_split= 4,min_samples_leaf= 1,max_features= 'sqrt', \n",
    "                                  random_state = 1,max_depth=20,bootstrap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-politics",
   "metadata": {},
   "source": [
    "# October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1593,
   "id": "handled-privilege",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1473\n"
     ]
    }
   ],
   "source": [
    "X=data_Oct[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Oct['Ref']\n",
    "#frame1=[data_oct3,data_nov3]\n",
    "#fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "#Fall=fall.resample('h').mean()\n",
    "#Fall=Fall.dropna()\n",
    "#X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,random_state=20)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200*4]\n",
    "X_train3=X_Data[:350*4]\n",
    "X_train4=X_Data[:550*4]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200*4]\n",
    "y_train3=y_Data[:350*4]\n",
    "y_train4=y_Data[:550*4]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_o=[]\n",
    "B3_o=[]\n",
    "B4_o=[]\n",
    "\n",
    "P2_o=[]\n",
    "P3_o=[]\n",
    "P4_o=[]\n",
    "model2=regressor.fit(X_train2[:int(0.4*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.4*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(0.4*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "B2_o.append(b)\n",
    "P2_o.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1594,
   "id": "paperback-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "er2_o=[]\n",
    "er3_o=[]\n",
    "er4_o=[]\n",
    "RMSE2_o=[]\n",
    "RMSE3_o=[]\n",
    "RMSE4_o=[]\n",
    "R2_o=[]\n",
    "R3_o=[]\n",
    "R4_o=[]\n",
    "for i in range(1,41):\n",
    "    model2=regressor.fit(X_train2[:int(0.02*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.02*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.02*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)),1)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.02*i*X_train2.shape[0]):], pred2), 2)\n",
    "    RMSE2_o.append(RMSE2)\n",
    "    R2_o.append(r2)\n",
    "    er2_o.append(rmse2)\n",
    "for i in range(1,41):\n",
    "    model3=regressor.fit(X_train3[:int(0.02*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.02*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.02*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)),1)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.02*i*X_train3.shape[0]):], pred3), 2)\n",
    "    RMSE3_o.append(RMSE3)\n",
    "    R3_o.append(r3)\n",
    "    er3_o.append(rmse3)\n",
    "for i in range(1,41):\n",
    "    model4=regressor.fit(X_train4[:int(0.02*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.02*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.02*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)),1)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.02*i*X_train4.shape[0]):], pred4), 2)\n",
    "    RMSE4_o.append(RMSE4)\n",
    "    R4_o.append(r4)\n",
    "    er4_o.append(rmse4)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1595,
   "id": "durable-fever",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.9229882482073055,\n",
       " -3.9486588321382037,\n",
       " -3.9486588321382037,\n",
       " -3.9486588321382037,\n",
       " -2.9614941241036528,\n",
       " -2.9614941241036528,\n",
       " -2.9614941241036528,\n",
       " -2.9614941241036528,\n",
       " -2.9614941241036528,\n",
       " -2.9614941241036528,\n",
       " -2.9614941241036528,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018,\n",
       " -1.9743294160691018]"
      ]
     },
     "execution_count": 1595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE4_o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-qatar",
   "metadata": {},
   "source": [
    "# Nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1596,
   "id": "rubber-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799\n"
     ]
    }
   ],
   "source": [
    "X=data_Nov[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Nov['Ref']\n",
    "\n",
    "#X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "#frame1=[data_dec3,data_jan3]#,data_feb\n",
    "#winter=pd.concat(frame1)\n",
    "#Winter=winter.resample('h').mean()\n",
    "#Winter=Winter.dropna()\n",
    "#X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Winter['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,random_state=30 )\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200*4]\n",
    "X_train3=X_Data[:350*4]\n",
    "X_train4=X_Data[:550*4]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200*4]\n",
    "y_train3=y_Data[:350*4]\n",
    "y_train4=y_Data[:550*4]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1597,
   "id": "wrong-child",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1597-7b3a1d80c9da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mer2_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     model3=regressor.fit(X_train3[:int(0.02*i*X_train3.shape[0])], \n\u001b[0m\u001b[1;32m     22\u001b[0m                          y_train3[:int(0.02*i*X_train3.shape[0])])\n\u001b[1;32m     23\u001b[0m     \u001b[0mpred3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX_train3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_INT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             trees = [self._make_estimator(append=False,\n\u001b[0m\u001b[1;32m    377\u001b[0m                                           random_state=random_state)\n\u001b[1;32m    378\u001b[0m                      for i in range(n_more_estimators)]\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_INT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             trees = [self._make_estimator(append=False,\n\u001b[0m\u001b[1;32m    377\u001b[0m                                           random_state=random_state)\n\u001b[1;32m    378\u001b[0m                      for i in range(n_more_estimators)]\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0m_set_random_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_set_random_states\u001b[0;34m(estimator, random_state)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random_state'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__random_state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mto_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "er2_n=[]\n",
    "er3_n=[]\n",
    "er4_n=[]\n",
    "RMSE2_n=[]\n",
    "RMSE3_n=[]\n",
    "RMSE4_n=[]\n",
    "R2_n=[]\n",
    "R3_n=[]\n",
    "R4_n=[]\n",
    "for i in range(1,41):\n",
    "    model2=regressor.fit(X_train2[:int(0.02*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.02*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.02*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)),1)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.02*i*X_train2.shape[0]):], pred2), 2)\n",
    "    RMSE2_n.append(RMSE2)\n",
    "    R2_n.append(r2)\n",
    "    er2_n.append(rmse2)\n",
    "for i in range(1,41):\n",
    "    model3=regressor.fit(X_train3[:int(0.02*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.02*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.02*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)),1)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.02*i*X_train3.shape[0]):], pred3), 2)\n",
    "    RMSE3_n.append(RMSE3)\n",
    "    R3_n.append(r3)\n",
    "    er3_n.append(rmse3)\n",
    "for i in range(1,41):\n",
    "    model4=regressor.fit(X_train4[:int(0.02*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.02*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.02*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)),1)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.02*i*X_train4.shape[0]):], pred4), 2)\n",
    "    RMSE4_n.append(RMSE4)\n",
    "    R4_n.append(r4)\n",
    "    er4_n.append(rmse4)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-company",
   "metadata": {},
   "source": [
    "# Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Dec['Ref']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=50)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200*4]\n",
    "X_train3=X_Data[:350*4]\n",
    "X_train4=X_Data[:550*4]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200*4]\n",
    "y_train3=y_Data[:350*4]\n",
    "y_train4=y_Data[:550*4]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "er2_d=[]\n",
    "er3_d=[]\n",
    "er4_d=[]\n",
    "RMSE2_d=[]\n",
    "RMSE3_d=[]\n",
    "RMSE4_d=[]\n",
    "R2_d=[]\n",
    "R3_d=[]\n",
    "R4_d=[]\n",
    "for i in range(1,41):\n",
    "    model2=regressor.fit(X_train2[:int(0.02*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.02*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.02*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)),1)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.02*i*X_train2.shape[0]):], pred2), 2)\n",
    "    RMSE2_d.append(RMSE2)\n",
    "    R2_d.append(r2)\n",
    "    er2_d.append(rmse2)\n",
    "for i in range(1,41):\n",
    "    model3=regressor.fit(X_train3[:int(0.02*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.02*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.02*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)),1)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.02*i*X_train3.shape[0]):], pred3), 2)\n",
    "    RMSE3_d.append(RMSE3)\n",
    "    R3_d.append(r3)\n",
    "    er3_d.append(rmse3)\n",
    "for i in range(1,41):\n",
    "    model4=regressor.fit(X_train4[:int(0.02*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.02*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.02*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)),1)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.02*i*X_train4.shape[0]):], pred4), 2)\n",
    "    RMSE4_d.append(RMSE4)\n",
    "    R4_d.append(r4)\n",
    "    er4_d.append(rmse4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-bradley",
   "metadata": {},
   "source": [
    "# Jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Jan[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Jan['Ref']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=60)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:200*4]\n",
    "X_train3=X_Data[:350*4]\n",
    "X_train4=X_Data[:550*4]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:200*4]\n",
    "y_train3=y_Data[:350*4]\n",
    "y_train4=y_Data[:550*4]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "er2_j=[]\n",
    "er3_j=[]\n",
    "er4_j=[]\n",
    "RMSE2_j=[]\n",
    "RMSE3_j=[]\n",
    "RMSE4_j=[]\n",
    "R2_j=[]\n",
    "R3_j=[]\n",
    "R4_j=[]\n",
    "for i in range(1,41):\n",
    "    model2=regressor.fit(X_train2[:int(0.02*i*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.02*i*X_train2.shape[0])])\n",
    "    pred2=model2.predict(X_train2[int(0.02*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)[0, 1],2)\n",
    "    RMSE2=(round(np.sqrt(sm.mean_squared_error(y_train2[int(0.02*i*X_train2.shape[0]):], pred2)),1)/np.mean(y))\n",
    "    r2=round(sm.r2_score(y_train2[int(0.02*i*X_train2.shape[0]):], pred2), 2)\n",
    "    RMSE2_j.append(RMSE2)\n",
    "    R2_j.append(r2)\n",
    "    er2_j.append(rmse2)\n",
    "for i in range(1,41):\n",
    "    model3=regressor.fit(X_train3[:int(0.02*i*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.02*i*X_train3.shape[0])])\n",
    "    pred3=model3.predict(X_train3[int(0.02*i*X_train3.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)[0, 1],2)\n",
    "    RMSE3=(round(np.sqrt(sm.mean_squared_error(y_train3[int(0.02*i*X_train3.shape[0]):], pred3)),1)/np.mean(y))\n",
    "    r3=round(sm.r2_score(y_train3[int(0.02*i*X_train3.shape[0]):], pred3), 2)\n",
    "    RMSE3_j.append(RMSE3)\n",
    "    R3_j.append(r3)\n",
    "    er3_j.append(rmse3)\n",
    "for i in range(1,41):\n",
    "    model4=regressor.fit(X_train4[:int(0.02*i*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.02*i*X_train4.shape[0])])\n",
    "    pred4=model3.predict(X_train4[int(0.02*i*X_train4.shape[0]):])\n",
    "    rmse4=round(np.corrcoef(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)[0, 1],2)\n",
    "    RMSE4=(round(np.sqrt(sm.mean_squared_error(y_train4[int(0.02*i*X_train4.shape[0]):], pred4)),1)/np.mean(y))\n",
    "    r4=round(sm.r2_score(y_train4[int(0.02*i*X_train4.shape[0]):], pred4), 2)\n",
    "    RMSE4_j.append(RMSE4)\n",
    "    R4_j.append(r4)\n",
    "    er4_j.append(rmse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[er4_o[i],er4_n[i],er4_d[i],er4_j[i],er2_o[i],er2_n[i],er2_d[i],er2_j[i],\n",
    "         er3_o[i],er3_n[i],er3_d[i],er3_j[i]] for i in range(40)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[R4_o[i],R4_n[i],R4_d[i],R4_j[i],R2_o[i],R2_n[i],R2_d[i],R2_j[i],\n",
    "         R3_o[i],R3_n[i],R3_d[i],R3_j[i]] for i in range(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 =[[RMSE4_o[i],RMSE4_n[i],RMSE4_d[i],RMSE4_j[i]] for i in range(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr3=[np.mean(data3[0]) ]\n",
    "data2[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "Corr_mean3=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    corr2=np.mean(data2[i])\n",
    "    corr3=np.mean(data3[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "    Corr_mean3.append(corr3)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff3=abs(((Corr_mean3[39]-np.array(Corr_mean3)))*100)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "Diff3=list(Diff3[:5]+3*(moving_avg(Diff3,6)[0]-Diff3[4]))+list(moving_avg(Diff3,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(6.5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.hlines([2], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([4], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([6], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([8], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([10], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "m1,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"d\",markersize=9, alpha=1)\n",
    "m2,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "#plt.plot(Diff2[:-1],A[:-1], color='#CD5B45',marker=\"d\",markersize=13, alpha=0.9)\n",
    "m3,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"d\",markersize=9, alpha=1)\n",
    "m4,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0,15)\n",
    "ax.set_xlim(right=40)\n",
    "ax.set_ylim(bottom=-0.2)\n",
    "plt.yticks(np.arange(0,15, step=2))\n",
    "plt.xticks(np.arange(0,40, step=5))\n",
    "ax.set_xticks([0,5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['0','10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.ylabel('Change in performance (%)',fontsize=20)\n",
    "plt.yticks([2,4,6,8,10,12,14])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "#plt.xlabel('Tolerance, Tc (%)',fontsize=20)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "#plt.text(0.2,2, 'Tc=2',fontsize=14)\n",
    "#plt.text(0.2,4, 'Tc=4',fontsize=14)\n",
    "#plt.text(0.2,6, 'Tc=6',fontsize=14)\n",
    "#plt.text(0.2,8, 'Tc=8',fontsize=14)\n",
    "#plt.text(0.2,10, 'Tc=10',fontsize=14)\n",
    "textstr = 'O3-'.translate(SUB) +'Monthly'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.745, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.savefig(\"CS_O3_M1.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array(Diff3)\n",
    "A=np.round(A,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "Corr_mean3=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    corr2=np.mean(data2[i])\n",
    "    corr3=np.mean(data3[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "    Corr_mean3.append(corr3)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff3=abs(((max(Corr_mean3)-np.array(Corr_mean3))/min(Corr_mean3))*100)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=50,color='darkgoldenrod', alpha=1)\n",
    "plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=50,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.73,0.8), fontsize=16)\n",
    "plt.fill_between(np.array(Y_Test), y1, y2, color='teal', alpha=0.4)\n",
    "plt.fill_between(np.array(Y_Test), y1, y4, color='teal', alpha=0.35)\n",
    "plt.fill_between(np.array(Y_Test), y1, y6, color='teal', alpha=0.3)\n",
    "plt.fill_between(np.array(Y_Test), y1, y8, color='teal', alpha=0.25)\n",
    "plt.fill_between(np.array(Y_Test), y1, y10, color='teal', alpha=0.2)\n",
    "plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=120,color='darkgoldenrod', alpha=1)\n",
    "plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=120,color='#CD5B45', alpha=0.9)\n",
    "plt.plot(A[:-1],Diff[:-1], color='darkgoldenrod', alpha=1)\n",
    "plt.plot(A[:-1],Diff2[:-1], color='#CD5B45', alpha=0.9)\n",
    "plt.scatter(A[:-1],Diff[:-1], s=3,color='black')\n",
    "plt.scatter(A[:-1],Diff2[:-1], s=3,color='black')\n",
    "#plt.scatter(A,Diff3, marker=\"d\",s=500,color='orange', alpha=1)\n",
    "#plt.scatter(A,Diff3, s=5,color='black')\n",
    "#plt.plot(A,Diff2,color='darkred', alpha=0.9)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,15)\n",
    "plt.ylim(0, 20)\n",
    "ax.set_xlim(right=15.5)\n",
    "ax.set_ylim(bottom=-0.5)\n",
    "plt.xticks(np.arange(0,16, step=2))\n",
    "plt.yticks(np.arange(0,20, step=5))\n",
    "ax.set_xticks([5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.yticks([4,8,12,16,20])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Difference (%)',fontsize=20)\n",
    "plt.text(0.2,1.5, 'Tc=2',fontsize=14)\n",
    "plt.text(0.2,3.5, 'Tc=4',fontsize=14)\n",
    "plt.text(0.2,5.5, 'Tc=6',fontsize=14)\n",
    "plt.text(0.2,7.5, 'Tc=8',fontsize=14)\n",
    "plt.text(0.2,9.5, 'Tc=10',fontsize=14)\n",
    "textstr = 'O3-'.translate(SUB) +'Monthly'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.76, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.savefig(\"CS_O3_M.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tc=[2,4,6,8,10]\n",
    "Frac=[68,52,38,32,28]\n",
    "plt.plot(Tc,Frac)\n",
    "plt.scatter(Tc,Frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "def estimate_coef(x, y):\n",
    "    # number of observations/points\n",
    "    n = np.size(x)\n",
    "  \n",
    "    # mean of x and y vector\n",
    "    m_x = np.mean(x)\n",
    "    m_y = np.mean(y)\n",
    "  \n",
    "    # calculating cross-deviation and deviation about x\n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x\n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x\n",
    "  \n",
    "    # calculating regression coefficients\n",
    "    b_1 = SS_xy / SS_xx\n",
    "    b_0 = m_y - b_1*m_x\n",
    "  \n",
    "    return (b_0, b_1)\n",
    "\n",
    "def plot_regression_line(x, y, b):\n",
    "    # plotting the actual points as scatter plot\n",
    "    plt.scatter(x, y, color = \"m\",\n",
    "               marker = \"o\", s = 30)\n",
    "  \n",
    "    # predicted response vector\n",
    "    y_pred = b[0] + b[1]*x\n",
    "  \n",
    "    # plotting the regression line\n",
    "    plt.plot(x, y_pred, color = \"g\")\n",
    "  \n",
    "    # putting labels\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "  \n",
    "    # function to show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=list(estimate_coef(np.array(Frac), np.array(Tc)))\n",
    "plt.plot(Frac,Tc)\n",
    "plt.scatter(Frac,Tc)\n",
    "plot_regression_line(np.array(Frac), np.array(Tc), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam.datasets import wage\n",
    "from pygam import LinearGAM, s, f\n",
    "X, y = wage()\n",
    "X=np.array(Frac)\n",
    "y=np.array(Tc)\n",
    "gam = LinearGAM(s(0)).fit(X, y)\n",
    "gam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gammy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import numpy as np\n",
    "\n",
    ">>> import gammy\n",
    ">>> from gammy.models.bayespy import GAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> np.random.seed(42)\n",
    "\n",
    ">>> n = 30\n",
    ">>> input_data = 10 * np.random.rand(n)\n",
    ">>> y = 5 * input_data + 2.0 * input_data ** 2 + 7 + 10 * np.random.randn(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=np.array(Frac)\n",
    "y=np.array(Tc)\n",
    "from gammy.arraymapper import x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> a = gammy.formulae.Scalar(prior=(0, 1e-6))\n",
    ">>> b = gammy.formulae.Scalar(prior=(0, 1e-6))\n",
    "c = gammy.formulae.Scalar(prior=(0, 1e-6))\n",
    ">>> bias = gammy.formulae.Scalar(prior=(0, 1e-6))\n",
    ">>> formula = a * x + b * x ** 2 + c * x **4+bias\n",
    ">>> model = GAM(formula).fit(input_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mean_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(model.inv_mean_tau, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=list(estimate_coef(np.array(Frac), np.array(Tc)))\n",
    "plt.plot(Frac,Tc)\n",
    "plt.scatter(Frac,Tc)\n",
    "plt.plot(Frac,pred, color='red')\n",
    "plot_regression_line(np.array(Frac), np.array(Tc), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>2 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=2 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=2%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M2_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>4 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=4 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=4%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M4_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>6 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=6 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=6%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M6_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>8 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=8 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=8%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M8_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>10 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=10 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=10%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_M10_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-intranet",
   "metadata": {},
   "source": [
    "Data=[]\n",
    "for i in range(len(data)):\n",
    "    data_mean, data_std = np.mean(data[i]), np.std(data[i])\n",
    "    cut_off = data_std * 1.9\n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "    outliers_removed = [x for x in data[i] if x > lower and x < upper]\n",
    "    Data.append(outliers_removed )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# Creating dataset\n",
    "np.random.seed(10)\n",
    "#data_1 =[Av1[10],Av3[10],Av5[10]]\n",
    "#data_2 =[Av1[20],Av3[20],Av5[20]]\n",
    "#data_3 =[Av1[30],Av3[30],Av5[30]]\n",
    "#data_4 =[Av1[40],Av3[40],Av5[40]]\n",
    "#data = [[er4_o[i],er4_n[i],er4_d[i],er4_j[i],er2_o[i],er2_n[i],er2_d[i],er2_j[i],\n",
    "         #er3_o[i],er3_n[i],er3_d[i],er3_j[i]] for i in range(18)]\n",
    "#data2= [[Av2[i],Av4[i],Av6[i]] for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "colors= ['#00688B' for i in range(40)]\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "for median in bp['medians']:\n",
    "    median.set(color ='green',\n",
    "               linewidth = 1)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([2,4,6,8,10,12,14,16])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'])\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.savefig(\"CS_M_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-reaction",
   "metadata": {},
   "source": [
    "# CO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Oct3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Oct3['Ref']\n",
    "frame1=[data_oct,data_nov]\n",
    "fall=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall[::3]\n",
    "Fall=Fall.dropna()\n",
    "X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=25)\n",
    "#Fall=Fall.sample(frac=1)\n",
    "P1=[]\n",
    "B1=[]\n",
    "corr1=[]\n",
    "for i in range(1,19):\n",
    "    regressor.fit(X_train[:int(0.05*i*X_train2.shape[0])], y_train[:int(0.05*i*X_train2.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.05*i*X_train.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.05*i*X_train.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.05*i*X_train.shape[0]):])\n",
    "    rmse1=round(np.corrcoef(y_train[int(0.05*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    P1.append(p1)\n",
    "    B1.append(b1)\n",
    "    corr1.append(rmse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Oct3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Oct3['Ref']\n",
    "frame1=[data_oct,data_nov]\n",
    "fall=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall[::3]\n",
    "Fall=Fall.dropna()\n",
    "X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,random_state=35)\n",
    "P11=[]\n",
    "B11=[]\n",
    "corr11=[]\n",
    "R11=[]\n",
    "RMSE11=[]\n",
    "for i in range(1,41):\n",
    "    regressor.fit(X_train[:int(0.02*i*X_train.shape[0])], y_train[:int(0.02*i*X_train.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.02*i*X_train.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    rmse11=round(np.corrcoef(y_train[int(0.02*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    r=round(sm.r2_score(y_train[int(0.02*i*X_train.shape[0]):], pred), 2)\n",
    "    RMSE=(round(np.sqrt(sm.mean_squared_error(y_train[int(0.02*i*X_train.shape[0]):], pred)),1)/np.mean(y))\n",
    "    RMSE11.append(RMSE)\n",
    "    R11.append(r)\n",
    "    P11.append(p1)\n",
    "    B11.append(b1)\n",
    "    corr11.append(rmse11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Nov3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Nov3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,random_state=45)\n",
    "frame1=[data_dec,data_jan]#,data_feb\n",
    "winter=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter=winter[::3]\n",
    "Winter=Winter.dropna()\n",
    "#Winter=Winter.sample(frac=1)\n",
    "X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Winter['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=55)\n",
    "P2=[]\n",
    "B2=[]\n",
    "corr2=[]\n",
    "for i in range(1,19):\n",
    "    regressor.fit(X_train[:int(0.05*i*X_train2.shape[0])], y_train[:int(0.05*i*X_train2.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    rmse2=round(np.corrcoef(y_train[int(0.05*i*X_train2.shape[0]):], pred)[0, 1],2)\n",
    "    P2.append(p1)\n",
    "    B2.append(b1)\n",
    "    corr2.append(rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Nov3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Nov3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=65)\n",
    "frame1=[data_dec,data_jan]#,data_feb\n",
    "winter=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter=winter[::3]\n",
    "Winter=Winter.dropna()\n",
    "#Winter=Winter.sample(frac=1)\n",
    "X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Winter['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=75)\n",
    "P21=[]\n",
    "B21=[]\n",
    "corr21=[]\n",
    "R21=[]\n",
    "RMSE21=[]\n",
    "for i in range(1,41):\n",
    "    regressor.fit(X_train[:int(0.02*i*X_train.shape[0])], y_train[:int(0.02*i*X_train.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.02*i*X_train.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    rmse21=round(np.corrcoef(y_train[int(0.02*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    r=round(sm.r2_score(y_train[int(0.02*i*X_train.shape[0]):], pred), 2)\n",
    "    RMSE=(round(np.sqrt(sm.mean_squared_error(y_train[int(0.02*i*X_train.shape[0]):], pred)),1)/np.mean(y))\n",
    "    RMSE21.append(RMSE)\n",
    "    R21.append(r)\n",
    "    P21.append(p1)\n",
    "    B21.append(b1)\n",
    "    corr21.append(rmse21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Dec3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Dec3['Ref']\n",
    "frame1=[data_mar,data_apr]\n",
    "spring=pd.concat(frame1)\n",
    "Spring=spring[::3]\n",
    "Spring=Spring.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Spring['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=85)\n",
    "P3=[]\n",
    "B3=[]\n",
    "corr3=[]\n",
    "for i in range(1,19):\n",
    "    regressor.fit(X_train[:int(0.05*i*X_train2.shape[0])], y_train[:int(0.05*i*X_train2.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.05*i*X_train2.shape[0]):])\n",
    "    rmse3=round(np.corrcoef(y_train[int(0.05*i*X_train2.shape[0]):], pred)[0, 1],2)\n",
    "    P3.append(p1)\n",
    "    B3.append(b1)\n",
    "    corr3.append(rmse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data_Dec3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "#y=data_Dec3['Ref']\n",
    "frame1=[data_mar,data_apr]\n",
    "spring=pd.concat(frame1)\n",
    "Spring=spring[::3]\n",
    "Spring=Spring.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Spring['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, random_state=95)\n",
    "P31=[]\n",
    "B31=[]\n",
    "corr31=[]\n",
    "R31=[]\n",
    "RMSE31=[]\n",
    "for i in range(1,41):\n",
    "    regressor.fit(X_train[:int(0.02*i*X_train.shape[0])], y_train[:int(0.02*i*X_train.shape[0])])\n",
    "    pred=regressor.predict(X_train[int(0.02*i*X_train.shape[0]):])\n",
    "    p1=precision(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    b1=bias(pred,y_train[int(0.02*i*X_train.shape[0]):])\n",
    "    rmse31=round(np.corrcoef(y_train[int(0.02*i*X_train.shape[0]):], pred)[0, 1],2)\n",
    "    r=round(sm.r2_score(y_train[int(0.02*i*X_train.shape[0]):], pred), 2)\n",
    "    RMSE=(round(np.sqrt(sm.mean_squared_error(y_train[int(0.02*i*X_train.shape[0]):], pred)),1)/np.mean(y))\n",
    "    RMSE31.append(RMSE)\n",
    "    R31.append(r)\n",
    "    P31.append(p1)\n",
    "    B31.append(b1)\n",
    "    corr31.append(rmse31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = [[corr11[i],corr21[i],corr31[i]] for i in range(40)]\n",
    "Data2=[[R11[i],R21[i],R31[i]] for i in range(40)]\n",
    "Data3=[[RMSE11[i],RMSE21[i],RMSE31[i]] for i in range(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "Corr_mean3=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    corr2=np.mean(Data2[i])\n",
    "    corr3=np.mean(Data3[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "    Corr_mean3.append(corr3)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff3=abs(((Corr_mean3[39]-np.array(Corr_mean3)))*100)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "Diff3=list(Diff3[:5]+3*(moving_avg(Diff3,6)[0]-Diff3[4]))+list(moving_avg(Diff3,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(6.5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.hlines([2], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([4], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([6], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([8], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "plt.hlines([10], 0, 45, linestyles='dashed', color='black', linewidth=0.7)\n",
    "m1,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"d\",markersize=9, alpha=1)\n",
    "m2,=ax.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "#plt.plot(Diff2[:-1],A[:-1], color='#CD5B45',marker=\"d\",markersize=13, alpha=0.9)\n",
    "m3,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"d\",markersize=9, alpha=1)\n",
    "m4,=ax.plot(A[:-1],Diff3[:-1], color='teal',marker=\"o\",markersize=4,markerfacecolor='black', alpha=1)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0,15)\n",
    "ax.set_xlim(right=40)\n",
    "ax.set_ylim(bottom=-0.2)\n",
    "plt.yticks(np.arange(0,15, step=2))\n",
    "plt.xticks(np.arange(0,40, step=5))\n",
    "ax.set_xticks([0,5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['0','10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.ylabel('Change in performance (%)',fontsize=20)\n",
    "plt.yticks([2,4,6,8,10,12,14])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "#plt.xlabel('Tolerance, Tc (%)',fontsize=20)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "#plt.text(0.2,2, 'Tc=2',fontsize=14)\n",
    "#plt.text(0.2,4, 'Tc=4',fontsize=14)\n",
    "#plt.text(0.2,6, 'Tc=6',fontsize=14)\n",
    "#plt.text(0.2,8, 'Tc=8',fontsize=14)\n",
    "#plt.text(0.2,10, 'Tc=10',fontsize=14)\n",
    "textstr = 'O3-'.translate(SUB)+'Seasonal'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.727, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.savefig(\"CS_O3_S1.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array(Diff3)\n",
    "A=np.round(A,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    corr2=np.mean(Data2[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(6.5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "#plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=50,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=50,color='#CD5B45', alpha=0.9)\n",
    "\n",
    "#plt.fill_between(np.array(Y_Test), y1, y2, color='teal', alpha=0.4)\n",
    "#plt.fill_between(np.array(Y_Test), y1, y4, color='teal', alpha=0.35)\n",
    "#plt.fill_between(np.array(Y_Test), y1, y6, color='teal', alpha=0.3)\n",
    "#plt.fill_between(np.array(Y_Test), y1, y8, color='teal', alpha=0.25)\n",
    "#plt.fill_between(np.array(Y_Test), y1, y10, color='teal', alpha=0.2)\n",
    "plt.hlines([2], 0, 45, linestyles='dashed', color='black', linewidth=0.9)\n",
    "plt.hlines([4], 0, 45, linestyles='dashed', color='black', linewidth=0.9)\n",
    "plt.hlines([6], 0, 45, linestyles='dashed', color='black', linewidth=0.9)\n",
    "plt.hlines([8], 0, 45, linestyles='dashed', color='black', linewidth=0.9)\n",
    "plt.hlines([10], 0, 45, linestyles='dashed', color='black', linewidth=0.9)\n",
    "#plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=120,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=120,color='#CD5B45', alpha=0.9)\n",
    "plt.scatter(A[:-1],Diff[:-1], s=20,color='black')\n",
    "plt.scatter(A[:-1],Diff2[:-1], s=20,color='black')\n",
    "plt.plot(A[:-1],Diff[:-1], color='darkgoldenrod',marker=\"d\",markersize=12, alpha=1)\n",
    "plt.plot(A[:-1],Diff2[:-1], color='#CD5B45',marker=\"d\",markersize=12, alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.71,0.8), fontsize=16)\n",
    "#plt.scatter(A,Diff3, marker=\"d\",s=500,color='orange', alpha=1)\n",
    "#plt.scatter(A,Diff3, s=5,color='black')\n",
    "#plt.plot(A,Diff2,color='darkred', alpha=0.9)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,15)\n",
    "plt.ylim(0, 20)\n",
    "ax.set_xlim(right=15.5)\n",
    "ax.set_ylim(bottom=-0.5)\n",
    "plt.xticks(np.arange(0,16, step=2))\n",
    "plt.yticks(np.arange(0,20, step=5))\n",
    "ax.set_xticks([5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.yticks([4,8,12,16,20])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Difference (%)',fontsize=20)\n",
    "plt.text(0.2,2, 'Tc=2',fontsize=14)\n",
    "plt.text(0.2,4, 'Tc=4',fontsize=14)\n",
    "plt.text(0.2,6, 'Tc=6',fontsize=14)\n",
    "plt.text(0.2,8, 'Tc=8',fontsize=14)\n",
    "plt.text(0.2,10, 'Tc=10',fontsize=14)\n",
    "textstr = textstr = 'O3-'.translate(SUB)+'Seasonal'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.725, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.savefig(\"CS_O3_S2.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "Corr_mean=[]\n",
    "Corr_mean2=[]\n",
    "for i in range(len(Data)):\n",
    "    corr=np.mean(Data[i])\n",
    "    corr2=np.mean(Data2[i])\n",
    "    Corr_mean.append(corr)\n",
    "    Corr_mean2.append(corr2)\n",
    "Diff=abs(((Corr_mean[39]-np.array(Corr_mean))/Corr_mean[39])*100)\n",
    "Diff2=abs(((Corr_mean2[39]-np.array(Corr_mean2))/Corr_mean2[39])*100)\n",
    "Diff3=abs(((max(Corr_mean3)-np.array(Corr_mean3))/min(Corr_mean3))*100)\n",
    "Diff=list(Diff[:5]+3*(moving_avg(Diff,6)[0]-Diff[4]))+list(moving_avg(Diff,6))\n",
    "Diff2=list(Diff2[:5]+3*(moving_avg(Diff2,6)[0]-Diff2[4]))+list(moving_avg(Diff2,6))\n",
    "A=[i for i in range(1,41)]\n",
    "#Diff=[Diff[i] for i in range(16) if i%2==0]\n",
    "#Diff2=[Diff2[i] for i in range(16) if i%2==0]\n",
    "#Diff3=[Diff3[i] for i in range(16) if i%2==0]\n",
    "Y_Test=[i for i in np.arange(0,41,1)]\n",
    "fig= plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.scatter(A[1],Corr_mean[1], marker=\"d\",s=200,color='darkgoldenrod', alpha=1)\n",
    "#plt.scatter(A[1],Corr_mean2[1], marker=\"d\",s=200,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.7,0.7), fontsize=16)\n",
    "\n",
    "y1=[-0.5 for i in range(len(Y_Test))]\n",
    "y2=[2 for i in range(len(Y_Test))]\n",
    "y4=[4 for i in range(len(Y_Test))]\n",
    "y6=[6 for i in range(len(Y_Test))]\n",
    "y8=[8 for i in range(len(Y_Test))]\n",
    "y10=[10 for i in range(len(Y_Test))]\n",
    "plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=50,color='darkgoldenrod', alpha=1)\n",
    "plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=50,color='#CD5B45', alpha=0.9)\n",
    "#plt.legend(['r','R^2'],loc = 2, bbox_to_anchor = (0.68,0.8), fontsize=16)\n",
    "plt.fill_between(np.array(Y_Test), y1, y2, color='teal', alpha=0.4)\n",
    "plt.fill_between(np.array(Y_Test), y1, y4, color='teal', alpha=0.35)\n",
    "plt.fill_between(np.array(Y_Test), y1, y6, color='teal', alpha=0.3)\n",
    "plt.fill_between(np.array(Y_Test), y1, y8, color='teal', alpha=0.25)\n",
    "plt.fill_between(np.array(Y_Test), y1, y10, color='teal', alpha=0.2)\n",
    "plt.scatter(A[:-1],Diff[:-1], marker=\"d\",s=120,color='darkgoldenrod', alpha=1)\n",
    "plt.scatter(A[:-1],Diff2[:-1], marker=\"d\",s=120,color='#CD5B45', alpha=0.9)\n",
    "plt.scatter(A[:-1],Diff[:-1], s=3,color='black')\n",
    "plt.scatter(A[:-1],Diff2[:-1], s=3,color='black')\n",
    "plt.plot(A[:-1],Diff[:-1], color='darkgoldenrod', alpha=1)\n",
    "plt.plot(A[:-1],Diff2[:-1], color='#CD5B45', alpha=0.9)\n",
    "#plt.scatter(A,Diff3, marker=\"d\",s=500,color='orange', alpha=1)\n",
    "#plt.scatter(A,Diff3, s=5,color='black')\n",
    "#plt.plot(A,Diff2,color='darkred', alpha=0.9)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=0)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show\n",
    "plt.xlim(0,15)\n",
    "plt.ylim(0, 20)\n",
    "ax.set_xlim(right=15.5)\n",
    "ax.set_ylim(bottom=-0.5)\n",
    "plt.xticks(np.arange(0,16, step=2))\n",
    "plt.yticks(np.arange(0,20, step=5))\n",
    "ax.set_xticks([5,10,15,20,25,30,35,40])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'],fontsize=16)\n",
    "plt.yticks([4,8,12,16,20])\n",
    "plt.setp(ax.spines.values(), linewidth=2)\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Difference (%)',fontsize=20)\n",
    "plt.text(0.2,1.5, 'Tc=2',fontsize=14)\n",
    "plt.text(0.2,3.5, 'Tc=4',fontsize=14)\n",
    "plt.text(0.2,5.5, 'Tc=6',fontsize=14)\n",
    "plt.text(0.2,7.5, 'Tc=8',fontsize=14)\n",
    "plt.text(0.2,9.5, 'Tc=10',fontsize=14)\n",
    "textstr = textstr = 'O3-'.translate(SUB)+'Seasonal'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.742, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.savefig(\"CS_O3_S.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>2 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=2 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.7,0.96, step=0.05))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=2%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S2_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>4 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=4 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=4%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S4_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>6 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=6 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=6%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S6_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>8 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=8 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=8%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S8_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
    "SUP = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
    "Corr_mean=[]\n",
    "for i in range(len(data)):\n",
    "    corr=np.mean(data[i])\n",
    "    Corr_mean.append(corr)\n",
    "\n",
    "ind1=[]\n",
    "ind2=[]\n",
    "Corr_mean1=[]\n",
    "Corr_mean2=[]\n",
    "Diff=((max(Corr_mean)-np.array(Corr_mean))/max(Corr_mean))*100\n",
    "for i in range(len(Diff)):\n",
    "    if Diff[i]>10 and i%2!=0:\n",
    "        ind1.append(i)\n",
    "        Corr_mean1.append(Corr_mean[i])\n",
    "    elif Diff[i]<=10 and i%2!=0 :\n",
    "        ind2.append(i)\n",
    "        Corr_mean2.append(Corr_mean[i])\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(ind1,Corr_mean1, marker=\"d\",s=400,color='darkred', alpha=0.9)\n",
    "plt.scatter(ind2,Corr_mean2, marker=\"d\",s=400,color='teal', alpha=0.9)\n",
    "plt.scatter(ind1,Corr_mean1, s=20,color='black')\n",
    "plt.scatter(ind2,Corr_mean2, s=20,color='black')\n",
    "plt.yticks([0.6,0.7,0.8,0.9], fontsize=18)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,3,5,7,9,11,13,15])\n",
    "ax.set_xticklabels(['5','15','25','35','45','55','65','75'],fontsize=16)\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "textstr = 'O3'.translate(SUB) + ' (Tc=10%)'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1)\n",
    "plt.text(0.017, 0.975, textstr, transform=ax.transAxes, fontsize=15,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.setp(ax.spines.values(), linewidth=1.6)\n",
    "plt.savefig(\"CS_S10_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "data=Data\n",
    "\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "colors= ['#00688B' for i in range(40)]\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "for median in bp['medians']:\n",
    "    median.set(color ='green',\n",
    "               linewidth = 1)\n",
    "\n",
    "plt.xlabel('Fraction of training data (%)',fontsize=20)\n",
    "plt.ylabel('Pearson correlation (r)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "plt.xticks(np.arange(0,18 , step=2))\n",
    "plt.yticks(np.arange(0.6,1.01, step=0.1))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([2,4,6,8,10,12,14,16])\n",
    "ax.set_xticklabels(['10','20','30','40','50','60','70','80'])\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.savefig(\"CS_S_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-sympathy",
   "metadata": {},
   "source": [
    "# Bias and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-pharmacology",
   "metadata": {},
   "source": [
    "# October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Oct[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Oct['Ref']\n",
    "#frame1=[data_oct,data_nov]\n",
    "#fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "#Fall=fall.resample('h').mean()\n",
    "#Fall=Fall.dropna()\n",
    "#X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_o=[]\n",
    "B3_o=[]\n",
    "B4_o=[]\n",
    "\n",
    "P2_o=[]\n",
    "P3_o=[]\n",
    "P4_o=[]\n",
    "model2=regressor.fit(X_train2[:int(0.4*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.4*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(0.4*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "B2_o.append(b)\n",
    "P2_o.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(0.4*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.4*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(0.4*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "B3_o.append(b)\n",
    "P3_o.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(0.4*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.4*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(0.4*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "B4_o.append(b)\n",
    "P4_o.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-assembly",
   "metadata": {},
   "source": [
    "# November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Nov['Ref']\n",
    "#frame1=[data_dec,data_jan,data_feb]\n",
    "#fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "#Fall=fall.resample('h').mean()\n",
    "#Fall=Fall.dropna()\n",
    "#X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_n=[]\n",
    "B3_n=[]\n",
    "B4_n=[]\n",
    "\n",
    "P2_n=[]\n",
    "P3_n=[]\n",
    "P4_n=[]\n",
    "model2=regressor.fit(X_train2[:int(0.4*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.4*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(0.4*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "B2_n.append(b)\n",
    "P2_n.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(0.4*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.4*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(0.4*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "B3_n.append(b)\n",
    "P3_n.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(0.4*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.4*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(0.4*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "B4_n.append(b)\n",
    "P4_n.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-prototype",
   "metadata": {},
   "source": [
    "# December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Dec['Ref']\n",
    "#frame1=[data_mar,data_apr]\n",
    "#fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "#Fall=fall.resample('h').mean()\n",
    "#Fall=Fall.dropna()\n",
    "#X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "#y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_d=[]\n",
    "B3_d=[]\n",
    "B4_d=[]\n",
    "\n",
    "P2_d=[]\n",
    "P3_d=[]\n",
    "P4_d=[]\n",
    "model2=regressor.fit(X_train2[:int(0.4*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.4*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(0.4*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "B2_d.append(b)\n",
    "P2_d.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(0.4*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.4*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(0.4*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "B3_d.append(b)\n",
    "P3_d.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(0.4*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.4*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(0.4*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "B4_d.append(b)\n",
    "P4_d.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Jan[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Jan['Ref']\n",
    "#frame1=[data_mar,data_apr]\n",
    "#fall=pd.concat(frame1)\n",
    "#Day=[5*i for i in range(1,11) ]\n",
    "#Fall=fall.resample('h').mean()\n",
    "#Fall=Fall.dropna()\n",
    "#X=Fall[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "#y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "print(X_train.shape[0])\n",
    "X_Data=X_train\n",
    "y_Data=y_train\n",
    "X_train2=X_Data[:500]\n",
    "X_train3=X_Data[:700]\n",
    "X_train4=X_Data[:900]\n",
    "#X_train4=X_Data[:2000]\n",
    "#X_train5=X_Data[:2500]\n",
    "#X_train6=X_Data[:3000]\n",
    "\n",
    "y_train2=y_Data[:500]\n",
    "y_train3=y_Data[:700]\n",
    "y_train4=y_Data[:900]\n",
    "#y_train4=y_Data[:2000]\n",
    "#y_train5=y_Data[:2500]\n",
    "#y_train6=y_Data[:3000]\n",
    "\n",
    "B2_j=[]\n",
    "B3_j=[]\n",
    "B4_j=[]\n",
    "\n",
    "P2_j=[]\n",
    "P3_j=[]\n",
    "P4_j=[]\n",
    "model2=regressor.fit(X_train2[:int(0.4*X_train2.shape[0])], \n",
    "                         y_train2[:int(0.4*X_train2.shape[0])])\n",
    "pred2=model2.predict(X_train2[int(0.4*X_train2.shape[0]):])\n",
    "b=bias(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "p=precision(y_train2[int(0.4*X_train2.shape[0]):], pred2)\n",
    "B2_j.append(b)\n",
    "P2_j.append(p)\n",
    "\n",
    "\n",
    "model3=regressor.fit(X_train3[:int(0.4*X_train3.shape[0])], \n",
    "                         y_train3[:int(0.4*X_train3.shape[0])])\n",
    "pred3=model3.predict(X_train3[int(0.4*X_train3.shape[0]):])\n",
    "b=bias(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "p=precision(y_train3[int(0.4*X_train3.shape[0]):], pred3)\n",
    "B3_j.append(b)\n",
    "P3_j.append(p)\n",
    "\n",
    "model4=regressor.fit(X_train4[:int(0.4*X_train4.shape[0])], \n",
    "                         y_train4[:int(0.4*X_train4.shape[0])])\n",
    "pred4=model4.predict(X_train4[int(0.4*X_train4.shape[0]):])\n",
    "b=bias(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "p=precision(y_train4[int(0.4*X_train4.shape[0]):], pred4)\n",
    "B4_j.append(b)\n",
    "P4_j.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-performer",
   "metadata": {},
   "source": [
    "X=data_Jan[['Net Signal','Temp','RH','Month','Day_of_week','Hour']]\n",
    "y=data_Jan['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=True)\n",
    "regressor.fit(X_train[:int(0.4*X_train2.shape[0])], y_train[:int(0.4*X_train2.shape[0])])\n",
    "pred=regressor.predict(X_train[int(0.05*X_train2.shape[0]):])\n",
    "P4=precision(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "B4=bias(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "(P4,B4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-kidney",
   "metadata": {},
   "source": [
    "X=data_Jan3[['Net Signal','Temp','RH','Month','Day_of_week','Hour','Ref_NO2']]\n",
    "y=data_Jan3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "regressor.fit(X_train[:int(0.4*X_train2.shape[0])], y_train[:int(0.4*X_train2.shape[0])])\n",
    "pred=regressor.predict(X_train[int(0.05*X_train2.shape[0]):])\n",
    "P41=precision(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "B41=bias(pred,y_train[int(0.05*X_train2.shape[0]):])\n",
    "(P41,B41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=[B2_o[0],B3_o[0],B4_o[0],B2_n[0],B3_n[0],B4_n[0],B2_d[0],B3_d[0],B4_d[0],B2_j[0],B3_j[0],B4_j[0]]\n",
    "P=[P2_o[0],P3_o[0],P4_o[0],P2_n[0],P3_n[0],P4_n[0],P2_d[0],P3_d[0],P4_d[0],P2_j[0],P3_j[0],P4_j[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=[B,P]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "# Creating dataset\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "#data_1 =[Av1[10],Av3[10],Av5[10]]\n",
    "#data_2 =[Av1[20],Av3[20],Av5[20]]\n",
    "#data_3 =[Av1[30],Av3[30],Av5[30]]\n",
    "#data_4 =[Av1[40],Av3[40],Av5[40]]\n",
    "#data = [[er4_o[i],er4_n[i],er4_d[i],er4_j[i],er2_o[i],er2_n[i],er2_d[i],er2_j[i],\n",
    "         #er3_o[i],er3_n[i],er3_d[i],er3_j[i]] for i in range(18)]\n",
    "#data2= [[Av2[i],Av4[i],Av6[i]] for i in range(20)]\n",
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "data=Data\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "plt.axhline(y= 10, color = 'green',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 25, color = 'purple',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 30, color = 'orange',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 50, color = 'dodgerblue',label='RC', linestyle = ':',linewidth=2 )\n",
    "#plt.legend(['RC','SGS','IS','HA & CSP'],ncol=4, loc='lower left', fontsize=10)\n",
    "\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "colors= ['#6495ED','#8B3E2F']\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "\n",
    "#plt.xlabel('Training Data (%)',fontsize=20)\n",
    "plt.ylabel('Error (%)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "#plt.xticks(np.arange(0,8 , step=1))\n",
    "plt.yticks(np.arange(0,61, step=10)) \n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "plt.axhline(y= 10, color = 'green',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 25, color = 'purple',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 30, color = 'orange',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 50, color = 'dodgerblue',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.grid(linestyle='-.',linewidth=0)\n",
    "ax.set_xticks([1,2])\n",
    "ax.set_xticklabels(['Bias','Precision'])\n",
    "#plt.legend(['Bias','Precision'] ,fontsize=16)\n",
    "#plt.title(r\"$CO$\",fontsize=16 )\n",
    "plt.savefig(\"BP_M_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(5, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "data=Data\n",
    "\n",
    "#data2=data2\n",
    "# Creating axes instance\n",
    "plt.axhline(y= 10, color = 'green',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 25, color = 'purple',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 30, color = 'orange',label='RC', linestyle = ':',linewidth=2 )\n",
    "plt.axhline(y= 50, color = 'dodgerblue',label='RC', linestyle = ':',linewidth=2 )\n",
    "#plt.legend(['RC','SGS','IS','HA & CSP'],ncol=4, loc='lower left', fontsize=10)\n",
    "bp = plt.boxplot(data, patch_artist = True,\n",
    "                 vert = 1,showfliers=False)\n",
    "#bp2 = ax.boxplot(data2, patch_artist = True,\n",
    "                 #vert = 1)\n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.62,1),  fontsize=10)\n",
    "#colors= ['#00688B' for i in range(2)]\n",
    "colors= ['#6495ED','#8B3E2F']\n",
    "#colors2= ['teal' for i in range(40)]\n",
    " \n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_color(color)\n",
    "#for patch, color in zip(bp2['boxes'], colors2):\n",
    "    #patch.set_color(color)\n",
    "    \n",
    "#plt.legend(['Randomized','Non-randomized'],loc = 2, bbox_to_anchor = (0.78,1),  fontsize=10)\n",
    "for median in bp['medians']:\n",
    "    median.set(color ='green',\n",
    "               linewidth = 1)\n",
    "\n",
    "plt.ylabel('Error (%)',fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.tick_params(width=1,length=3)\n",
    "#plt.xticks(np.arange(0,5 , step=2))\n",
    "plt.yticks(np.arange(0,61, step=10))\n",
    "#plt.grid(linestyle='-.',linewidth=0)\n",
    "#ax.spines[\"bottom\"].set_linewidth(1)\n",
    "#ax.spines[\"left\"].set_linewidth(1)\n",
    "#ax.spines[\"top\"].set_linewidth(1)\n",
    "#ax.spines[\"right\"].set_linewidth(1)\n",
    "ax.set_xticks([1,2])\n",
    "ax.set_xticklabels(['Bias','Precision'])\n",
    "#plt.legend( fontsize=16)\n",
    "#plt.title(r\"$O_3$\",fontsize=16 )\n",
    "\n",
    "#plt.savefig(\"fig4d.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "#plt.savefig(\"A.pdf\",format=\"pdf\", bbox_inches=\"tight\",dpi=1000)\n",
    "plt.savefig(\"BP_M_O3.pdf\",format=\"pdf\",bbox_inches=\"tight\",dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,21)]\n",
    "Rmse1_rf=[]\n",
    "RMSE1_rf=[]\n",
    "REU1=[]\n",
    "REU2=[]\n",
    "lv=2000\n",
    "L_y1=[]\n",
    "A1=[]\n",
    "M1=[]\n",
    "Bias=[]\n",
    "L=[]\n",
    "KK=[]\n",
    "D1=[]\n",
    "Features1=[]\n",
    "P1=[]\n",
    "for i in range(1,10):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    y_test=y_train[48*i:]\n",
    "    #model=stacked_averaged_models.fit(X_train[:48*i].values, y_train[:48*i])\n",
    "    #pred =model.predict(X_train[48*i:].values)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    kk=round(np.corrcoef(y_train[48*i:], pred)[0, 1],2)\n",
    "    KK.append(kk)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    ind=[]\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.3,lv)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    #U=np.sqrt(np.mean((reu)**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    #b=(np.mean(pred)-np.mean(y_train[48*i:]))/np.mean(y_train[48*i:])\n",
    "    #bias=(abs(b)*np.mean(y_train[48*i:])/(1.67*U))\n",
    "    l=1-(U/np.std(y_train[48*i:]))**2\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    features=regressor.feature_importances_\n",
    "    Features1.append(features)\n",
    "    D1.append(d)\n",
    "    L.append(l)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M1.append(p)\n",
    "    #Bias.append(bias)\n",
    "    \n",
    "    \n",
    "    for i in range(len(reu)):\n",
    "        if reu[i]<30:\n",
    "            ind.append(i)\n",
    "    Rmse1_rf.append(RMSE)\n",
    "    RMSE1_rf.append(rmse)\n",
    "    REU1.append(reu)\n",
    "    L_y1.append(k)\n",
    "    REU2.append(round((len(ind)/len(reu))*100,2))\n",
    "    A1.append(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "KK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train[:480], y_train[:480])\n",
    "pred=regressor.predict(X_train[480:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lv=max\n",
    "A=[i for i in np.arange(1,3,0.1)]\n",
    "K=[]\n",
    "for i in A:\n",
    "    lv=max(y_train[480:])\n",
    "    k=REF2(pred,y_train[480:],i,lv)\n",
    "    K.append(k)\n",
    "for i in range(len(K)):\n",
    "    if K[i]==min(K):\n",
    "        print(A[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(A,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Oct2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Oct2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,21) ]\n",
    "Rmse1_rf2=[]\n",
    "RMSE1_rf2=[]\n",
    "M12=[]\n",
    "for i in range(1,10):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M12.append(p)\n",
    "    Rmse1_rf2.append(mse)\n",
    "    RMSE1_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE1_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "M12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Oct3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Oct3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,21) ]\n",
    "Rmse1_rf3=[]\n",
    "RMSE1_rf3=[]\n",
    "M13=[]\n",
    "for i in range(1,9):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M13.append(p)\n",
    "    Rmse1_rf3.append(mse)\n",
    "    RMSE1_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE1_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "M13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-blues",
   "metadata": {},
   "source": [
    "# Nov 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-transmission",
   "metadata": {},
   "source": [
    "   #   RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov[['Net Signal','Temp1','RH1','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Nov['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum2=sum(A[:100])\n",
    "mean2=np.std(y)/np.mean(y)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf=[]\n",
    "RMSE2_rf=[]\n",
    "REU2=[]\n",
    "L_y2=[]\n",
    "A2=[]\n",
    "M2=[]\n",
    "D2=[]\n",
    "Features2=[]\n",
    "for i in range(1,11):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    reu=REF(pred,y_train[48*i:],1.3)\n",
    "    U=np.sqrt(np.mean((reu)**2))\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    #U=np.sqrt(np.mean((reu)**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    features=regressor.feature_importances_\n",
    "    Features2.append(features)\n",
    "    D2.append(d)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M2.append(p)\n",
    "    Rmse2_rf.append(RMSE)\n",
    "    RMSE2_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.3,lv)\n",
    "    A2.append(re)\n",
    "    REU2.append(reu)\n",
    "    L_y2.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE2_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Nov2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum2=sum(A[:100])\n",
    "mean2=np.std(y)/np.mean(y)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf2=[]\n",
    "RMSE2_rf2=[]\n",
    "M22=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M22.append(p)\n",
    "    Rmse2_rf2.append(mse)\n",
    "    RMSE2_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE2_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "M22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Nov3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Nov3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf3=[]\n",
    "RMSE2_rf3=[]\n",
    "M23=[]\n",
    "for i in range(1,10):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M23.append(p)\n",
    "    Rmse2_rf3.append(mse)\n",
    "    RMSE2_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse2_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE2_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "M23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-conservation",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Dec['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum3=sum(A[:100])\n",
    "mean3=np.std(y)/np.mean(y)\n",
    "Rmse3_rf=[]\n",
    "RMSE3_rf=[]\n",
    "REU3=[]\n",
    "L_y3=[]\n",
    "A3=[]\n",
    "M3=[]\n",
    "for i in range(1,10):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    #lv=max(y_train[48*i:])\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    reu=REF(pred,y_train[48*i:],1.6)\n",
    "    #U=np.sqrt(np.mean((reu)**2))\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M3.append(p)\n",
    "    Rmse3_rf.append(RMSE)\n",
    "    RMSE3_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.3,lv)\n",
    "    A3.append(re)\n",
    "    REU3.append(reu)\n",
    "    L_y3.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE3_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[i for i in range(len(pred))]\n",
    "plt.plot(ind,pred)\n",
    "plt.plot(ind,y_train[432:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Dec2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum3=sum(A[:100])\n",
    "mean3=np.std(y)/np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf2=[]\n",
    "RMSE3_rf2=[]\n",
    "M32=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M32.append(p)\n",
    "    Rmse3_rf2.append(mse)\n",
    "    RMSE3_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE3_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "M32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Dec3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Dec3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf3=[]\n",
    "RMSE3_rf3=[]\n",
    "M33=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M33.append(p)\n",
    "    Rmse3_rf3.append(mse)\n",
    "    RMSE3_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse3_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE3_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "M33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-seeker",
   "metadata": {},
   "source": [
    "# Jan 2020 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-emission",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import KFold\n",
    "X=data_Jan[['Net Signal','Temp1','RH1','Month','Day_of_week','Hour']]\n",
    "y=data_Jan['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "kf5 = KFold(n_splits=5, shuffle=False)\n",
    "X_Train=[]\n",
    "X_Test=[]\n",
    "\n",
    "for i,j in kf5.split(X_train):\n",
    "    X_Train.append(X_train.iloc[i])\n",
    "    X_Test.append(X_train.iloc[j]) \n",
    "    \n",
    "y_Train=[]\n",
    "y_Test=[]\n",
    "\n",
    "for k,l in kf5.split(y_train):\n",
    "    y_Train.append(y_train.iloc[k])\n",
    "    y_Test.append(y_train.iloc[l]) \n",
    "#X_train=preprocessing.scale(X_train)\n",
    "#X_train=preprocessing.normalize(X_train)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum4=sum(A[:1000])\n",
    "mean4=np.std(y)/np.mean(y)\n",
    "Rmse4_rf=[]\n",
    "RMSE4_rf=[]\n",
    "REU4=[]\n",
    "L_y4=[]\n",
    "A4=[]\n",
    "M4=[]\n",
    "D4=[]\n",
    "P4=[]\n",
    "Features=[]\n",
    "for i in range(1,10):\n",
    "    #lv=max(y_train[48*i:])\n",
    "    k=y_Train[0][48*i:].to_list()\n",
    "    #regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    model=regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=model.predict(X_train[48*i:])\n",
    "    #mse=round(sMAE(y_Train[0][48*i:], pred),2)\n",
    "    #rmse=round(sm.r2_score(y_Train[0][48*i:], pred), 2)\n",
    "    #U=np.sqrt(np.mean((0.25*np.array(y_Train[0][48*i:]))**2))\n",
    "    #RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_Train[0][48*i:]))**2))\n",
    "    #m=RMSE/(1*U)\n",
    "    #d=IOA(y_Train[0][48*i:], pred)\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    #features=regressor.feature_importances_\n",
    "    #Features.append(features)\n",
    "    #D4.append(d)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M4.append(p)\n",
    "   \n",
    "    Rmse4_rf.append(RMSE)\n",
    "    #RMSE4_rf.append(rmse)\n",
    "    #reu=REF(pred,y_Train[0][48*i:],1)\n",
    "    #re=REF2(pred,y_Train[0][48*i:],20,lv)\n",
    "    #A4.append(re)\n",
    "    #REU4.append(reu)\n",
    "    #L_y4.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Jan2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Jan2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum4=sum(A[:100])\n",
    "mean4=np.std(y)/np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf2=[]\n",
    "RMSE4_rf2=[]\n",
    "M42=[]\n",
    "D42=[]\n",
    "for i in range(1,9):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #mse= mape=round(mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))/np.mean(y)\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    D42.append(d)\n",
    "    M42.append(p)\n",
    "    Rmse4_rf2.append(RMSE)\n",
    "    RMSE4_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "M42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_Jan3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=data_Jan3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf3=[]\n",
    "RMSE4_rf3=[]\n",
    "M43=[]\n",
    "for i in range(1,10):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #mse= mape=round(mean_absolute_error(y_test,pred)/np.mean(y_test),2)\n",
    "    #mse= mape=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    p=precision(pred,y_train[48*i:])\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-50:])\n",
    "    rmse=round(sm.r2_score(y_train[X_train.shape[0]-50:], Pred), 2)\n",
    "    M43.append(p)\n",
    "    Rmse4_rf3.append(mse)\n",
    "    RMSE4_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse4_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE4_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "M43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[[Rmse1_rf[i],Rmse2_rf[:9][i],Rmse3_rf[:9][i],Rmse4_rf[:9][i]] for i in range(9)]\n",
    "AV=[]\n",
    "for i in range(9):\n",
    "    AV.append(np.mean(A[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day=[2*i for i in range(1,10)]\n",
    "#plt.plot(Day,Rmse1_rf, color='red')\n",
    "#plt.plot(Day,Rmse2_rf[:9], color='blue')\n",
    "#plt.plot(Day,Rmse3_rf[:9], color='teal')\n",
    "plt.plot(Day,AV, color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse1_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-nursing",
   "metadata": {},
   "source": [
    "# Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-beads",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-conflict",
   "metadata": {},
   "source": [
    "X=data_Feb[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Feb['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-wells",
   "metadata": {},
   "source": [
    "Rmse5_rf2=[]\n",
    "RMSE5_rf2=[]\n",
    "M52=[]\n",
    "for i in range(1,7):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.15*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1.67*U)\n",
    "    M52.append(m)\n",
    "    Rmse5_rf2.append(mse)\n",
    "    RMSE5_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-commercial",
   "metadata": {},
   "source": [
    "A=y.to_list()\n",
    "Ext_feb=[]\n",
    "for i in range(len(A)):\n",
    "    if A[i]>3*np.mean(A):\n",
    "        Ext_feb.append(i)\n",
    "N_Ext_feb=len(Ext_feb)\n",
    "N_Ext_feb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-occasion",
   "metadata": {},
   "source": [
    "mean_feb=np.mean(y)\n",
    "N_feb=y.shape[0]\n",
    "Mean_Rmse_feb=np.mean(Rmse5_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-promise",
   "metadata": {},
   "source": [
    "# March 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-alberta",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-eleven",
   "metadata": {},
   "source": [
    "X=data_Mar[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Mar['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.0001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum6=sum(A[:100])\n",
    "mean6=np.std(y)/np.mean(y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-makeup",
   "metadata": {},
   "source": [
    "Rmse6_rf=[]\n",
    "RMSE6_rf=[]\n",
    "REU6=[]\n",
    "L_y6=[]\n",
    "A6=[]\n",
    "M5=[]\n",
    "S=[]\n",
    "D6=[]\n",
    "for i in range(1,11):\n",
    "    k=y_train[48*i:].to_list()\n",
    "    #lv=max(y_train[48*i:])\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    s=(np.mean(y_train[48*i:]))/(np.array(y_train[48*i:]))\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    d=IOA(y_train[48*i:], pred)\n",
    "    D6.append(d)\n",
    "    M5.append(m)\n",
    "    S.append(s)\n",
    "    Rmse6_rf.append(mse)\n",
    "    RMSE6_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[48*i:],1)\n",
    "    re=REF2(pred,y_train[48*i:],1.2,lv)\n",
    "    A6.append(re)\n",
    "    REU6.append(reu)\n",
    "    L_y6.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-regulation",
   "metadata": {},
   "source": [
    "REU_d1=list(REU1[0])+list(REU2[0])+list(REU3[0])+list(REU4[0])+list(REU6[0])\n",
    "REU_d2=list(REU1[1])+list(REU2[1])+list(REU3[1])+list(REU4[1])+list(REU6[1])\n",
    "REU_d3=list(REU1[2])+list(REU2[2])+list(REU3[2])+list(REU4[2])+list(REU6[2])\n",
    "REU_d4=list(REU1[3])+list(REU2[3])+list(REU3[3])+list(REU4[3])+list(REU6[3])\n",
    "REU_d5=list(REU1[4])+list(REU2[4])+list(REU3[4])+list(REU4[4])+list(REU6[4])\n",
    "REU_d6=list(REU1[5])+list(REU2[5])+list(REU3[5])+list(REU4[5])+list(REU6[5])\n",
    "REU_d7=list(REU1[6])+list(REU2[6])+list(REU3[6])+list(REU4[6])+list(REU6[6])\n",
    "REU_d8=list(REU1[7])+list(REU2[7])+list(REU3[7])+list(REU4[7])+list(REU6[7])\n",
    "REU_d9=list(REU1[8])+list(REU2[8])+list(REU3[8])+list(REU4[8])+list(REU6[8])\n",
    "REU_d10=list(REU1[9])+list(REU2[9])+list(REU3[9])+list(REU4[9])+list(REU6[9])\n",
    "\n",
    "REU=[REU_d1,REU_d2,REU_d3,REU_d4,REU_d5,REU_d6,REU_d7,REU_d8,REU_d9,REU_d10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-attribute",
   "metadata": {},
   "source": [
    "L_d1=list(L_y1[0])+list(L_y2[0])+list(L_y3[0])+list(L_y4[0])+list(L_y6[0])\n",
    "L_d2=list(L_y1[1])+list(L_y2[1])+list(L_y3[1])+list(L_y4[1])+list(L_y6[1])\n",
    "L_d3=list(L_y1[2])+list(L_y2[2])+list(L_y3[2])+list(L_y4[2])+list(L_y6[2])\n",
    "L_d4=list(L_y1[3])+list(L_y2[3])+list(L_y3[3])+list(L_y4[3])+list(L_y6[3])\n",
    "L_d5=list(L_y1[4])+list(L_y2[4])+list(L_y3[4])+list(L_y4[4])+list(L_y6[4])\n",
    "L_d6=list(L_y1[5])+list(L_y2[5])+list(L_y3[5])+list(L_y4[5])+list(L_y6[5])\n",
    "L_d7=list(L_y1[6])+list(L_y2[6])+list(L_y3[6])+list(L_y4[6])+list(L_y6[6])\n",
    "L_d8=list(L_y1[7])+list(L_y2[7])+list(L_y3[7])+list(L_y4[7])+list(L_y6[7])\n",
    "L_d9=list(L_y1[8])+list(L_y2[8])+list(L_y3[8])+list(L_y4[8])+list(L_y6[8])\n",
    "L_d10=list(L_y1[9])+list(L_y2[9])+list(L_y3[9])+list(L_y4[9])+list(L_y6[9])\n",
    "L=[L_d1,L_d2,L_d3,L_d4,L_d5,L_d6,L_d7,L_d8,L_d9,L_d10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-economy",
   "metadata": {},
   "source": [
    "import numpy.polynomial.polynomial as poly\n",
    "u_cal=REU\n",
    "L_y=L\n",
    "for i in range(len(REU)):\n",
    "    fig= plt.figure(figsize=(7,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax.patch.set_facecolor('lightblue')\n",
    "    #ax.patch.set_alpha(0.1)\n",
    "    ind=[]\n",
    "    U_cal=[]\n",
    "    Ref=[]\n",
    "    dqo=[25 for i in range(len(Ref))]\n",
    "    plt.scatter(L_y[i],u_cal[i], color='#4B0082')\n",
    "    plt.axhline(y=25, color='black', linestyle='-.',linewidth=3)\n",
    "    plt.ylabel('Relative Expanded Uncertainty(%)', fontsize=16)\n",
    "    plt.xlabel('Reference CO concentration(ppb)',fontsize=16)\n",
    "    plt.grid(linestyle='-.',linewidth=0.1)\n",
    "    #plt.title('LAB',fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-geography",
   "metadata": {},
   "source": [
    "X=data_Mar2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=data_Mar2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum6=sum(A[:100])\n",
    "mean6=np.std(y)/np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-effort",
   "metadata": {},
   "source": [
    "Rmse6_rf2=[]\n",
    "RMSE6_rf2=[]\n",
    "M52=[]\n",
    "for i in range(1,11):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.15*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1.67*U)\n",
    "    M52.append(m)\n",
    "    Rmse6_rf2.append(mse)\n",
    "    RMSE6_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-input",
   "metadata": {},
   "source": [
    "# April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-locator",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-technique",
   "metadata": {},
   "source": [
    "X=data_Apr[['Net Signal','Lab1','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=data_Apr['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=True)\n",
    "A=np.array(y)/np.mean(y)\n",
    "A=sorted(A, reverse=True)\n",
    "sum7=sum(A[:100])\n",
    "mean7=np.std(y)/np.mean(y)\n",
    "Rmse10_rf2=[]\n",
    "RMSE10_rf2=[]\n",
    "M52=[]\n",
    "for i in range(1,5):\n",
    "    regressor.fit(X_train[:48*i], y_train[:48*i])\n",
    "    pred=regressor.predict(X_train[48*i:])\n",
    "    #mse=round(sm.r2_score(y_test, pred), 2)\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse= mape=round(mean_absolute_percentage_error(y_test,pred),2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[48*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[48*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.15*np.array(y_train[48*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[48*i:]))**2))\n",
    "    m=RMSE/(1.67*U)\n",
    "    M52.append(m)\n",
    "    Rmse5_rf2.append(mse)\n",
    "    RMSE5_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-planning",
   "metadata": {},
   "source": [
    "A=y.to_list()\n",
    "Ext_apr=[]\n",
    "for i in range(len(A)):\n",
    "    if A[i]>3*np.mean(A):\n",
    "        Ext_apr.append(i)\n",
    "N_Ext_apr=len(Ext_apr)\n",
    "N_Ext_apr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-cathedral",
   "metadata": {},
   "source": [
    "mean_apr=np.mean(y)\n",
    "N_apr=y.shape[0]\n",
    "Mean_Rmse_apr=np.mean(Rmse10_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-quest",
   "metadata": {},
   "source": [
    "Mean_Rmse=[Mean_Rmse_oct,Mean_Rmse_nov,Mean_Rmse_dec,Mean_Rmse_jan,Mean_Rmse_feb,Mean_Rmse_mar,Mean_Rmse_apr]\n",
    "Mean_conc=[mean_oct,mean_nov,mean_dec,mean_jan,mean_feb,mean_mar,mean_apr]\n",
    "N=[N_oct,N_nov,N_dec,N_jan,N_feb,N_mar,N_apr]\n",
    "N_Ext=[N_Ext_oct,N_Ext_nov,N_Ext_dec,N_Ext_jan,N_Ext_feb,N_Ext_mar,N_Ext_apr]\n",
    "\n",
    "Rmse=[]\n",
    "for i in range(10):\n",
    "    A=[Rmse1_rf[i],Rmse2_rf[i],Rmse3_rf[i],Rmse4_rf[i],Rmse5_rf[i],Rmse6_rf[i],Rmse10_rf[i]]\n",
    "    Rmse.append(A)\n",
    "RMSE=Rmse[0]+Rmse[1]+Rmse[2]+Rmse[3]+Rmse[4]+Rmse[5]+Rmse[6]+Rmse[7]\n",
    "Conc=Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc+Mean_conc\n",
    "N_dp=N+N+N+N+N+N+N+N\n",
    "N_ext=N_Ext+N_Ext+N_Ext+N_Ext+N_Ext+N_Ext+N_Ext+N_Ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-tonight",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "Day_1_rf=[RMSE1_rf[0],RMSE2_rf[0],RMSE3_rf[0],RMSE4_rf[0],RMSE6_rf[0]]\n",
    "Day_2_rf=[RMSE1_rf[1],RMSE2_rf[1],RMSE3_rf[1],RMSE4_rf[1],RMSE6_rf[1]]\n",
    "Day_3_rf=[RMSE1_rf[2],RMSE2_rf[2],RMSE3_rf[2],RMSE4_rf[2],RMSE6_rf[2]]\n",
    "Day_4_rf=[RMSE1_rf[3],RMSE2_rf[3],RMSE3_rf[3],RMSE4_rf[3],RMSE6_rf[3]]\n",
    "Day_5_rf=[RMSE1_rf[4],RMSE2_rf[4],RMSE3_rf[4],RMSE4_rf[4],RMSE6_rf[4]]\n",
    "Day_6_rf=[RMSE1_rf[5],RMSE2_rf[5],RMSE3_rf[5],RMSE4_rf[5],RMSE6_rf[5]]\n",
    "Day_7_rf=[RMSE1_rf[6],RMSE2_rf[6],RMSE3_rf[6],RMSE4_rf[6],RMSE6_rf[6]]\n",
    "Day_8_rf=[RMSE1_rf[7],RMSE2_rf[7],RMSE3_rf[7],RMSE4_rf[7],RMSE6_rf[7]]\n",
    "Day_9_rf=[RMSE1_rf[8],RMSE2_rf[8],RMSE3_rf[8],RMSE4_rf[8],RMSE6_rf[8]]\n",
    "Day_10_rf=[RMSE1_rf[9],RMSE2_rf[9],RMSE3_rf[9],RMSE4_rf[9],RMSE6_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-factory",
   "metadata": {},
   "source": [
    "Day_1_ann=[RMSE1_ann[0],RMSE2_ann[0],RMSE3_ann[0],RMSE4_ann[0],RMSE6_ann[0]]\n",
    "Day_2_ann=[RMSE1_ann[1],RMSE2_ann[1],RMSE3_ann[1],RMSE4_ann[1],RMSE6_ann[1]]\n",
    "Day_3_ann=[RMSE1_ann[2],RMSE2_ann[2],RMSE3_ann[2],RMSE4_ann[2],RMSE6_ann[2]]\n",
    "Day_4_ann=[RMSE1_ann[3],RMSE2_ann[3],RMSE3_ann[3],RMSE4_ann[3],RMSE6_ann[3]]\n",
    "Day_5_ann=[RMSE1_ann[4],RMSE2_ann[4],RMSE3_ann[4],RMSE4_ann[4],RMSE6_ann[4]]\n",
    "Day_6_ann=[RMSE1_ann[5],RMSE2_ann[5],RMSE3_ann[5],RMSE4_ann[5],RMSE6_ann[5]]\n",
    "Day_7_ann=[RMSE1_ann[6],RMSE2_ann[6],RMSE3_ann[6],RMSE4_ann[6],RMSE6_ann[6]]\n",
    "Day_8_ann=[RMSE1_ann[7],RMSE2_ann[7],RMSE3_ann[7],RMSE4_ann[7],RMSE6_ann[7]]\n",
    "Day_9_ann=[RMSE1_ann[8],RMSE2_ann[8],RMSE3_ann[8],RMSE4_ann[8],RMSE6_ann[8]]\n",
    "Day_10_ann=[RMSE1_ann[9],RMSE2_ann[9],RMSE3_ann[9],RMSE4_ann[9],RMSE6_ann[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-librarian",
   "metadata": {},
   "source": [
    "Day_1_RF=[Rmse1_rf[0],Rmse2_rf[0],Rmse3_rf[0],Rmse4_rf[0],Rmse6_rf[0]]\n",
    "Day_2_RF=[Rmse1_rf[1],Rmse2_rf[1],Rmse3_rf[1],Rmse4_rf[1],Rmse6_rf[1]]\n",
    "Day_3_RF=[Rmse1_rf[2],Rmse2_rf[2],Rmse3_rf[2],Rmse4_rf[2],Rmse6_rf[2]]\n",
    "Day_4_RF=[Rmse1_rf[3],Rmse2_rf[3],Rmse3_rf[3],Rmse4_rf[3],Rmse6_rf[3]]\n",
    "Day_5_RF=[Rmse1_rf[4],Rmse2_rf[4],Rmse3_rf[4],Rmse4_rf[4],Rmse6_rf[4]]\n",
    "Day_6_RF=[Rmse1_rf[5],Rmse2_rf[5],Rmse3_rf[5],Rmse4_rf[5],Rmse6_rf[5]]\n",
    "Day_7_RF=[Rmse1_rf[6],Rmse2_rf[6],Rmse3_rf[6],Rmse4_rf[6],Rmse6_rf[6]]\n",
    "Day_8_RF=[Rmse1_rf[7],Rmse2_rf[7],Rmse3_rf[7],Rmse4_rf[7],Rmse6_rf[7]]\n",
    "Day_9_RF=[Rmse1_rf[8],Rmse2_rf[8],Rmse3_rf[8],Rmse4_rf[8],Rmse6_rf[8]]\n",
    "Day_10_RF=[Rmse1_rf[9],Rmse2_rf[9],Rmse3_rf[9],Rmse4_rf[9],Rmse6_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-blind",
   "metadata": {},
   "source": [
    "Day_1_RF2=[Rmse1_rf2[0],Rmse2_rf2[0],Rmse3_rf2[0],Rmse4_rf2[0],Rmse6_rf2[0]]\n",
    "Day_2_RF2=[Rmse1_rf2[1],Rmse2_rf2[1],Rmse3_rf2[1],Rmse4_rf2[1],Rmse6_rf2[1]]\n",
    "Day_3_RF2=[Rmse1_rf2[2],Rmse2_rf2[2],Rmse3_rf2[2],Rmse4_rf2[2],Rmse6_rf2[2]]\n",
    "Day_4_RF2=[Rmse1_rf2[3],Rmse2_rf2[3],Rmse3_rf2[3],Rmse4_rf2[3],Rmse6_rf2[3]]\n",
    "Day_5_RF2=[Rmse1_rf2[4],Rmse2_rf2[4],Rmse3_rf2[4],Rmse4_rf2[4],Rmse6_rf2[4]]\n",
    "Day_6_RF2=[Rmse1_rf2[5],Rmse2_rf2[5],Rmse3_rf2[5],Rmse4_rf2[5],Rmse6_rf2[5]]\n",
    "Day_7_RF2=[Rmse1_rf2[6],Rmse2_rf2[6],Rmse3_rf2[6],Rmse4_rf2[6],Rmse6_rf2[6]]\n",
    "Day_8_RF2=[Rmse1_rf2[7],Rmse2_rf2[7],Rmse3_rf2[7],Rmse4_rf2[7],Rmse6_rf2[7]]\n",
    "Day_9_RF2=[Rmse1_rf2[8],Rmse2_rf2[8],Rmse3_rf2[8],Rmse4_rf2[8],Rmse6_rf2[8]]\n",
    "Day_10_RF2=[Rmse1_rf2[9],Rmse2_rf2[9],Rmse3_rf2[9],Rmse4_rf2[9],Rmse6_rf2[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-reaction",
   "metadata": {},
   "source": [
    "Mean=(np.array(Rmse1_rf)+np.array(Rmse2_rf)+np.array(Rmse3_rf)+np.array(Rmse4_rf)+np.array(Rmse6_rf))/5\n",
    "Mean=list(Mean)+list(Mean)+list(Mean)+list(Mean)\n",
    "Mean=sorted(Mean)\n",
    "\n",
    "Oct=sorted(Rmse1_rf+Rmse1_rf+Rmse1_rf+Rmse1_rf)\n",
    "Nov=sorted(Rmse2_rf+Rmse2_rf+Rmse2_rf+Rmse2_rf)\n",
    "Dec=sorted(Rmse3_rf+Rmse3_rf+Rmse3_rf+Rmse3_rf)\n",
    "Jan=sorted(Rmse4_rf+Rmse4_rf+Rmse4_rf+Rmse4_rf)\n",
    "Mar=sorted(Rmse6_rf+Rmse6_rf+Rmse6_rf+Rmse6_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MPI=[M1[0],M2[0],M3[0],M4[0]]#M5[0]]\n",
    "Day_2_MPI=[M1[1],M2[1],M3[1],M4[1]]#M5[1]]\n",
    "Day_3_MPI=[M1[2],M2[2],M3[2],M4[2]]#M5[2]]\n",
    "Day_4_MPI=[M1[3],M2[3],M3[3],M4[3]]#M5[3]]\n",
    "Day_5_MPI=[M1[4],M2[4],M3[4],M4[4]]#M5[4]]\n",
    "Day_6_MPI=[M1[5],M2[5],M3[5],M4[5]]#M5[5]]\n",
    "Day_7_MPI=[M1[6],M2[6],M3[6],M4[6]]#M5[6]]\n",
    "Day_8_MPI=[M1[7],M2[7],M3[7],M4[7]]#M5[7]]\n",
    "Day_9_MPI=[M1[8],M2[8],M3[8],M4[8]]#M5[8]]\n",
    "#Day_10_MPI=[M1[9],M2[9],M3[9],M4[9]]#M5[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MPI2=[M12[0],M22[0],M32[0],M42[0]]#,M52[0]]\n",
    "Day_2_MPI2=[M12[1],M22[1],M32[1],M42[1]]#,M52[1]]\n",
    "Day_3_MPI2=[M12[2],M22[2],M32[2],M42[2]]#,M52[2]]\n",
    "Day_4_MPI2=[M12[3],M22[3],M32[3],M42[3]]#,M52[3]]\n",
    "Day_5_MPI2=[M12[4],M22[4],M32[4],M42[4]]#,M52[4]]\n",
    "Day_6_MPI2=[M12[5],M22[5],M32[5],M42[5]]#,M52[5]]\n",
    "Day_7_MPI2=[M12[6],M22[6],M32[6],M42[6]]#,M52[6]]\n",
    "Day_8_MPI2=[M12[7],M22[7],M32[7],M42[7]]#,M52[7]]\n",
    "#Day_9_MPI2=[M12[8],M22[8],M32[8],M42[8]]#,M52[8]]\n",
    "#Day_10_MPI2=[M12[9],M22[9],M32[9],M42[9]]#,M52[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MPI3=[M13[0],M23[0],M33[0],M43[0]]#,M52[0]]\n",
    "Day_2_MPI3=[M13[1],M23[1],M33[1],M43[1]]#,M52[1]]\n",
    "Day_3_MPI3=[M13[2],M23[2],M33[2],M43[2]]#,M52[2]]\n",
    "Day_4_MPI3=[M13[3],M23[3],M33[3],M43[3]]#,M52[3]]\n",
    "Day_5_MPI3=[M13[4],M23[4],M33[4],M43[4]]#,M52[4]]\n",
    "Day_6_MPI3=[M13[5],M23[5],M33[5],M43[5]]#,M52[5]]\n",
    "Day_7_MPI3=[M13[6],M23[6],M33[6],M43[6]]#,M52[6]]\n",
    "Day_8_MPI3=[M13[7],M23[7],M33[7],M43[7]]#,M52[7]]\n",
    "#Day_9_MPI3=[M13[8],M23[8],M33[8],M43[8]]#,M52[8]]\n",
    "#Day_10_MPI3=[M13[9],M23[9],M33[9],M43[9]]#,M52[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-italic",
   "metadata": {},
   "source": [
    "Day_1_ANN=[Rmse1_ann[0],Rmse2_ann[0],Rmse3_ann[0],Rmse4_ann[0],Rmse6_ann[0]]\n",
    "Day_2_ANN=[Rmse1_ann[1],Rmse2_ann[1],Rmse3_ann[1],Rmse4_ann[1],Rmse6_ann[1]]\n",
    "Day_3_ANN=[Rmse1_ann[2],Rmse2_ann[2],Rmse3_ann[2],Rmse4_ann[2],Rmse6_ann[2]]\n",
    "Day_4_ANN=[Rmse1_ann[3],Rmse2_ann[3],Rmse3_ann[3],Rmse4_ann[3],Rmse6_ann[3]]\n",
    "Day_5_ANN=[Rmse1_ann[4],Rmse2_ann[4],Rmse3_ann[4],Rmse4_ann[4],Rmse6_ann[4]]\n",
    "Day_6_ANN=[Rmse1_ann[5],Rmse2_ann[5],Rmse3_ann[5],Rmse4_ann[5],Rmse6_ann[5]]\n",
    "Day_7_ANN=[Rmse1_ann[6],Rmse2_ann[6],Rmse3_ann[6],Rmse4_ann[6],Rmse6_ann[6]]\n",
    "Day_8_ANN=[Rmse1_ann[7],Rmse2_ann[7],Rmse3_ann[7],Rmse4_ann[7],Rmse6_ann[7]]\n",
    "Day_9_ANN=[Rmse1_ann[8],Rmse2_ann[8],Rmse3_ann[8],Rmse4_ann[8],Rmse6_ann[8]]\n",
    "Day_10_ANN=[Rmse1_ann[9],Rmse2_ann[9],Rmse3_ann[9],Rmse4_ann[9],Rmse6_ann[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_P=Day_1_rf+Day_2_rf+Day_3_rf+Day_4_rf+Day_5_rf+Day_6_rf+Day_7_rf+Day_8_rf+Day_9_rf+Day_10_rf\n",
    "#ANN_P=Day_1_ann+Day_2_ann+Day_3_ann+Day_4_ann+Day_5_ann+Day_6_ann+Day_7_ann+Day_8_ann+Day_9_ann+Day_10_ann\n",
    "#RF_R=Day_1_RF+Day_2_RF+Day_3_RF+Day_4_RF+Day_5_RF+Day_6_RF+Day_7_RF+Day_8_RF+Day_9_RF+Day_10_RF\n",
    "#RF_R2=Day_1_RF2+Day_2_RF2+Day_3_RF2+Day_4_RF2+Day_5_RF2+Day_6_RF2+Day_7_RF2+Day_8_RF2+Day_9_RF2+Day_10_RF2\n",
    "MPI=Day_1_MPI+Day_2_MPI+Day_3_MPI+Day_4_MPI+Day_5_MPI+Day_6_MPI+Day_7_MPI+Day_8_MPI#+Day_9_MPI+Day_10_MPI\n",
    "MPI2=Day_1_MPI2+Day_2_MPI2+Day_3_MPI2+Day_4_MPI2+Day_5_MPI2+Day_6_MPI2+Day_7_MPI2+Day_8_MPI2#+Day_9_MPI2+Day_10_MPI2\n",
    "MPI3=Day_1_MPI3+Day_2_MPI3+Day_3_MPI3+Day_4_MPI3+Day_5_MPI3+Day_6_MPI3+Day_7_MPI3+Day_8_MPI3#+Day_9_MPI3+Day_10_MPI3\n",
    "#ANN_R=Day_1_ANN+Day_2_ANN+Day_3_ANN+Day_4_ANN+Day_5_ANN+Day_6_ANN+Day_7_ANN+Day_8_ANN+Day_9_ANN+Day_10_ANN\n",
    "x0=['0' for i in range(4)]\n",
    "x1=['2' for i in range(4)]\n",
    "x2=['4' for i in range(4)]\n",
    "x3=['6' for i in range(4)]\n",
    "x4=['8' for i in range(4)]\n",
    "x5=['10' for i in range(4)]\n",
    "x6=['12' for i in range(4)]\n",
    "x7=['14' for i in range(4)]\n",
    "x8=['16' for i in range(4)]\n",
    "x9=['18' for i in range(4)]\n",
    "x10=['20' for i in range(4)]\n",
    "x11=['22' for i in range(4)]\n",
    "Reg=[10 for i in range(48)]\n",
    "Reg=[10 for i in range(48)]\n",
    "Spatial=[25 for i in range(60) ]\n",
    "Intervention=[30 for i in range(60) ]\n",
    "Hs_and_sp=[50 for i in range(60) ]\n",
    "reg=[0.9 for i in range(60)]\n",
    "spatial=[0.75 for i in range(60) ]\n",
    "intervention=[0.7 for i in range(60) ]\n",
    "\n",
    "x=x1+x2+x3+x4+x5+x6+x7+x8#+x9+x10\n",
    "X=x0+x1+x2+x3+x4+x5+x6+x7+x8+x9#+x10+x11\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio\n",
    "fig = go.Figure() \n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([1, 3, 2, 3, 1])\n",
    "# Defining x axis\n",
    "x = x\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=MPI,\n",
    "    x=x,\n",
    "    #name=r'$CO$',\n",
    "    marker_color='darkblue',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=MPI2,\n",
    "    x=x,\n",
    "    #name=r'$NO_2$',\n",
    "    marker_color='Teal',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "     #to x-axis\n",
    "    y=MPI3,\n",
    "    x=x,\n",
    "    #name=r'$O_3$',\n",
    "    marker_color='darkgoldenrod',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "\n",
    "\n",
    "#x = x\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "   # y=RF_R2,\n",
    "    #x=x,\n",
    "    #name='R^2(NO2)',\n",
    "    #marker_color='teal',\n",
    "    #showlegend=True\n",
    "   \n",
    "\n",
    "#))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    #defining y axis in corresponding\n",
    "   # to x-axis\n",
    "   # y=RF_R,\n",
    "    #x=x,\n",
    "    #name='SMAE',\n",
    "    #marker_color='#CD6600',\n",
    "    #showlegend=True\n",
    "   \n",
    "#))\n",
    "#fig.add_trace(go.Box(\n",
    "   #y=ANN_R,\n",
    "    #x=x,\n",
    "    #name='XGBoost(NMAE)',\n",
    "    #marker_color='deeppink',\n",
    "    #showlegend=True\n",
    "\n",
    "#))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Reg, \n",
    "                name=\"RC\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'green', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Spatial, \n",
    "                name=\"SGS\",\n",
    "                mode = 'lines',\n",
    "                marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'purple', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                    \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Intervention, \n",
    "                name=\"IS/IM\",\n",
    "                mode = 'lines',\n",
    "                marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'orange', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Hs_and_sp, \n",
    "                name=\"HA/SP\",\n",
    "                    \n",
    "                mode = 'lines',\n",
    "                marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 1.5, color = 'dodgerblue', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "               \n",
    "                        ))\n",
    "\n",
    "  \n",
    "fig.update_layout(autosize=True,\n",
    "                 #title={'text': r\"$NO_2$\",\n",
    "        #'y':0.78,\n",
    "        #'x':0.5,\n",
    "        #'xanchor': 'center',\n",
    "        #'yanchor': 'top'\n",
    "                       #}, \n",
    "    width=1000,\n",
    "    height=500,\n",
    "                  \n",
    "  legend=dict( yanchor=\"bottom\",\n",
    "    y=0.865,\n",
    "    x=0.01,\n",
    "    orientation=\"h\"\n",
    "),\n",
    "    # group together boxes of the different\n",
    "    # traces for each value of x\n",
    "    boxmode='group',\n",
    "                  plot_bgcolor='rgba(0.0,0.0,0.0,0.0)'\n",
    "                 \n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days of training\",tickfont = dict(size=16),\n",
    "                 titlefont = dict(size=24),linewidth=1.4, linecolor='black',tick0 = 2,\n",
    "        dtick = 2,\n",
    "                 mirror=True)\n",
    "fig.update_yaxes(title_text=\"Precision (%)\",tickfont = dict(size=16),range=[0.4,1],\n",
    "                 titlefont = dict(size=24),linewidth=1.4, linecolor='black',tick0 = 0.2,\n",
    "        dtick =0.2,\n",
    "                 mirror=True)\n",
    "fig.show()\n",
    "chart_studio.plotly.sign_in('vinylango', 'gybbJVWfRSUoTcRRSa6J')\n",
    "chart_studio.plotly.image.save_as(fig, filename='models_boxplot.png')\n",
    "#Image('models_boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-factory",
   "metadata": {},
   "source": [
    "Metric1=['RF' for i in range(len(RF))]\n",
    "Metric2=['XGBoost' for i in range(len(ANN))]\n",
    "Model=Metric1+Metric2\n",
    "Training=x+x\n",
    "Values=RF+ANN\n",
    "len(Values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-chaos",
   "metadata": {},
   "source": [
    "#Violin plot which also show the density of the distribution\n",
    "import plotly.express as px\n",
    "Metric1=['RF' for i in range(len(RF))]\n",
    "Metric2=['XGBoost' for i in range(len(ANN))]\n",
    "Model=Metric1+Metric2\n",
    "Training=x+x\n",
    "Values=RF+ANN\n",
    "lst=[[Training[i],Values[i],Model[i]] for i in range(len(Model))]\n",
    "df = pd.DataFrame(lst, columns =['Training Days', 'Pearson correlation (r)','Model'])\n",
    "\n",
    "#fig = px.violin( df,y=\"Performance\", x=\"Calibration Model\", color='Metric', box=True,points=\"all\",\n",
    "          #hover_data=df.columns)\n",
    "fig = px.violin( df,y=\"Pearson correlation (r)\", x=\"Training Days\", color='Model', box=True,\n",
    "          hover_data=df.columns)\n",
    "\n",
    "\n",
    "fig.update_layout(autosize=False,\n",
    "    width=900,\n",
    "    height=500)\n",
    "fig.show()\n",
    "#chart_studio.plotly.sign_in('vinylango', 'gybbJVWfRSUoTcRRSa6J')\n",
    "#chart_studio.plotly.image.save_as(fig, filename='models_violinplots.png')\n",
    "#Image('models_violinplots.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-anatomy",
   "metadata": {},
   "source": [
    "# Seasonal Calibration Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-house",
   "metadata": {},
   "source": [
    "# Fall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-anger",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_oct,data_nov]\n",
    "fall=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall=fall.resample('h').mean()\n",
    "Fall=Fall.dropna()\n",
    "#Fall=Fall.sample(frac=1)\n",
    "X=Fall[['Net Signal','Temp1','RH1','Month','Day_of_week','Day','Hour']]\n",
    "y=Fall['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean1=np.std(y)\n",
    "Rmse7_rf=[]\n",
    "RMSE7_rf=[]\n",
    "REU7=[]\n",
    "L_y7=[]\n",
    "A7=[]\n",
    "M7=[]\n",
    "B7=[]\n",
    "for i in range(1,9):\n",
    "    k=y_train[120*i:].to_list()\n",
    "    lv=max(y_train[120*i:])\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M7.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B7.append(b)\n",
    "    Rmse7_rf.append(mse)\n",
    "    RMSE7_rf.append(rmse)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse7_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE7_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "M7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_oct2,data_nov2]\n",
    "fall2=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall2=fall2.resample('h').mean()\n",
    "Fall2=Fall2.dropna()\n",
    "#Fall=Fall.sample(frac=1)\n",
    "X=Fall2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=Fall2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean1=np.std(y)\n",
    "Rmse7_rf2=[]\n",
    "RMSE7_rf2=[]\n",
    "M72=[]\n",
    "B72=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M72.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B72.append(b)\n",
    "    Rmse7_rf2.append(mse)\n",
    "    RMSE7_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse7_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE7_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "M72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_oct3,data_nov3]\n",
    "fall3=pd.concat(frame1)\n",
    "#fall=fall.sample(frac=1)\n",
    "Day=[5*i for i in range(1,11) ]\n",
    "Fall3=fall3.resample('h').mean()\n",
    "Fall3=Fall3.dropna()\n",
    "#Fall=Fall.sample(frac=1)\n",
    "X=Fall3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Fall3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001,shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean1=np.std(y)\n",
    "Rmse7_rf3=[]\n",
    "RMSE7_rf3=[]\n",
    "M73=[]\n",
    "B73=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M73.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B73.append(b)\n",
    "    Rmse7_rf3.append(mse)\n",
    "    RMSE7_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse7_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE7_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "M73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-maldives",
   "metadata": {},
   "source": [
    "# Winter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-distribution",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_dec,data_jan]#,data_feb\n",
    "winter=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter=winter.resample('h').mean()\n",
    "Winter=Winter.dropna()\n",
    "#Winter=Winter.sample(frac=1)\n",
    "X=Winter[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour']]\n",
    "y=Winter['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean2=np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf=[]\n",
    "RMSE8_rf=[]\n",
    "REU8=[]\n",
    "L_y8=[]\n",
    "A8=[]\n",
    "M8=[]\n",
    "B8=[]\n",
    "for i in range(1,8):\n",
    "    k=y_train[120*i:].to_list()\n",
    "    lv=max(y_train[120*i:])\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M8.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B8.append(b)\n",
    "    Rmse8_rf.append(mse)\n",
    "    RMSE8_rf.append(rmse)\n",
    "    reu=REF(pred,y_train[120*i:],1)\n",
    "    re=REF2(pred,y_train[120*i:],1,lv)\n",
    "    A8.append(re)\n",
    "    REU8.append(reu)\n",
    "    L_y8.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE8_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "M8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_dec2,data_jan2,data_feb2]\n",
    "winter2=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter2=winter2.resample('h').mean()\n",
    "Winter2=Winter2.dropna()\n",
    "X=Winter2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=Winter2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean2=np.std(y)\n",
    "Rmse8_rf2=[]\n",
    "RMSE8_rf2=[]\n",
    "M82=[]\n",
    "B82=[]\n",
    "for i in range(1,8):\n",
    "    \n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M82.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B82.append(b)\n",
    "    Rmse8_rf2.append(mse)\n",
    "    RMSE8_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE8_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "M82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_dec3,data_jan3,data_feb3]\n",
    "winter3=pd.concat(frame1)\n",
    "#winter=winter.sample(frac=1)\n",
    "Winter3=winter3.resample('h').mean()\n",
    "Winter3=Winter3.dropna()\n",
    "X=Winter3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Winter3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean2=np.std(y)\n",
    "Rmse8_rf3=[]\n",
    "RMSE8_rf3=[]\n",
    "M83=[]\n",
    "B83=[]\n",
    "for i in range(1,9):\n",
    "    \n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y_test,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M83.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B83.append(b)\n",
    "    Rmse8_rf3.append(mse)\n",
    "    RMSE8_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse8_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE8_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "M83"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-general",
   "metadata": {},
   "source": [
    "# Spring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-consequence",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_mar,data_apr]\n",
    "spring=pd.concat(frame1)\n",
    "Spring=spring.resample('h').mean()\n",
    "Spring=Spring.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring[['Net Signal','Temp1','RH1','Month','Day_of_week','Day','Hour']]\n",
    "y=Spring['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean3=np.std(y)\n",
    "Rmse9_rf=[]\n",
    "RMSE9_rf=[]\n",
    "REU9=[]\n",
    "L_y9=[]\n",
    "A9=[]\n",
    "M9=[]\n",
    "B9=[]\n",
    "for i in range(1,8):\n",
    "    k=y_train[120*i:].to_list()\n",
    "    lv=20000\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    Rmse9_rf.append(mse)\n",
    "    RMSE9_rf.append(rmse)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M9.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B9.append(b)\n",
    "    reu=REF(pred,y_train[120*i:],1)\n",
    "    re=REF2(pred,y_train[120*i:],1,lv)\n",
    "    A9.append(re)\n",
    "    REU9.append(reu)\n",
    "    L_y9.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse9_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE9_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "M9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_mar2,data_apr2]\n",
    "spring2=pd.concat(frame1)\n",
    "Spring2=spring2.resample('h').mean()\n",
    "Spring2=Spring2.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring2[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_O3']]\n",
    "y=Spring2['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean3=np.std(y)\n",
    "Rmse9_rf2=[]\n",
    "RMSE9_rf2=[]\n",
    "M92=[]\n",
    "B92=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.25*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M92.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B92.append(b)\n",
    "    Rmse9_rf2.append(mse)\n",
    "    RMSE9_rf2.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmse9_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE9_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "M92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1=[data_mar3,data_apr3]\n",
    "spring3=pd.concat(frame1)\n",
    "Spring3=spring3.resample('h').mean()\n",
    "Spring3=Spring3.dropna()\n",
    "#Spring=Spring.sample(frac=1)\n",
    "X=Spring3[['Net Signal','Temp','RH','Month','Day_of_week','Day','Hour','Ref_NO2']]\n",
    "y=Spring3['Ref']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.001, shuffle=False)\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "mean3=np.std(y)\n",
    "Rmse9_rf3=[]\n",
    "RMSE9_rf3=[]\n",
    "M93=[]\n",
    "B93=[]\n",
    "for i in range(1,8):\n",
    "    regressor.fit(X_train[:120*i], y_train[:120*i])\n",
    "    pred=regressor.predict(X_train[120*i:])\n",
    "    #mse=round(np.sqrt(sm.mean_squared_error(y_test, pred))/np.mean(y_test),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    #mse=round(relative_mean_absolute_error(y,pred),2)\n",
    "    #rmse=round(np.corrcoef(y_test, pred)[0, 1],2)\n",
    "    mse=round(sMAE(y_train[120*i:], pred),2)\n",
    "    rmse=round(sm.r2_score(y_train[120*i:], pred), 2)\n",
    "    U=np.sqrt(np.mean((0.3*np.array(y_train[120*i:]))**2))\n",
    "    RMSE=np.sqrt(np.mean((np.array(pred)-np.array(y_train[120*i:]))**2))\n",
    "    m=RMSE/(1*U)\n",
    "    Pred=regressor.predict(X_train[X_train.shape[0]-100:])\n",
    "    p=precision(Pred,y_train[X_train.shape[0]-100:])\n",
    "    M93.append(p)\n",
    "    b=bias(Pred,y_train[X_train.shape[0]-100:])\n",
    "    B93.append(b)\n",
    "    Rmse9_rf3.append(mse)\n",
    "    RMSE9_rf3.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    " Rmse9_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-product",
   "metadata": {},
   "outputs": [],
   "source": [
    " RMSE9_rf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-village",
   "metadata": {},
   "outputs": [],
   "source": [
    " M93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-digit",
   "metadata": {},
   "source": [
    "#import chart_studio\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "Day_1_rf=[RMSE7_rf[0],RMSE8_rf[0],RMSE9_rf[0]]\n",
    "Day_2_rf=[RMSE7_rf[1],RMSE8_rf[1],RMSE9_rf[1]]\n",
    "Day_3_rf=[RMSE7_rf[2],RMSE8_rf[2],RMSE9_rf[2]]\n",
    "Day_4_rf=[RMSE7_rf[3],RMSE8_rf[3],RMSE9_rf[3]]\n",
    "Day_5_rf=[RMSE7_rf[4],RMSE8_rf[4],RMSE9_rf[4]]\n",
    "Day_6_rf=[RMSE7_rf[5],RMSE8_rf[5],RMSE9_rf[5]]\n",
    "Day_7_rf=[RMSE7_rf[6],RMSE8_rf[6],RMSE9_rf[6]]\n",
    "Day_8_rf=[RMSE7_rf[7],RMSE8_rf[7],RMSE9_rf[7]]\n",
    "#Day_9_rf=[RMSE7_rf[8],RMSE8_rf[8],RMSE9_rf[8]]\n",
    "#Day_10_rf=[RMSE7_rf[9],RMSE8_rf[9],RMSE9_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-sharp",
   "metadata": {},
   "source": [
    "Day_1_ann=[RMSE7_ann[0],RMSE8_ann[0],RMSE9_ann[0]]\n",
    "Day_2_ann=[RMSE7_ann[1],RMSE8_ann[1],RMSE9_ann[1]]\n",
    "Day_3_ann=[RMSE7_ann[2],RMSE8_ann[2],RMSE9_ann[2]]\n",
    "Day_4_ann=[RMSE7_ann[3],RMSE8_ann[3],RMSE9_ann[3]]\n",
    "Day_5_ann=[RMSE7_ann[4],RMSE8_ann[4],RMSE9_ann[4]]\n",
    "Day_6_ann=[RMSE7_ann[5],RMSE8_ann[5],RMSE9_ann[5]]\n",
    "Day_7_ann=[RMSE7_ann[6],RMSE8_ann[6],RMSE9_ann[6]]\n",
    "Day_8_ann=[RMSE7_ann[7],RMSE8_ann[7],RMSE9_ann[7]]\n",
    "Day_9_ann=[RMSE7_ann[8],RMSE8_ann[8],RMSE9_ann[8]]\n",
    "Day_10_ann=[RMSE7_ann[9],RMSE8_ann[9],RMSE9_ann[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-approach",
   "metadata": {},
   "source": [
    "Day_1_RF=[Rmse7_rf[0],Rmse8_rf[0],Rmse9_rf[0]]\n",
    "Day_2_RF=[Rmse7_rf[1],Rmse8_rf[1],Rmse9_rf[1]]\n",
    "Day_3_RF=[Rmse7_rf[2],Rmse8_rf[2],Rmse9_rf[2]]\n",
    "Day_4_RF=[Rmse7_rf[3],Rmse8_rf[3],Rmse9_rf[3]]\n",
    "Day_5_RF=[Rmse7_rf[4],Rmse8_rf[4],Rmse9_rf[4]]\n",
    "Day_6_RF=[Rmse7_rf[5],Rmse8_rf[5],Rmse9_rf[5]]\n",
    "Day_7_RF=[Rmse7_rf[6],Rmse8_rf[6],Rmse9_rf[6]]\n",
    "Day_8_RF=[Rmse7_rf[7],Rmse8_rf[7],Rmse9_rf[7]]\n",
    "#Day_9_RF=[Rmse7_rf[8],Rmse8_rf[8],Rmse9_rf[8]]\n",
    "#Day_10_RF=[Rmse7_rf[9],Rmse8_rf[9],Rmse9_rf[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-luther",
   "metadata": {},
   "source": [
    "Day_1_RF2=[Rmse7_rf2[0],Rmse8_rf2[0],Rmse9_rf2[0]]\n",
    "Day_2_RF2=[Rmse7_rf2[1],Rmse8_rf2[1],Rmse9_rf2[1]]\n",
    "Day_3_RF2=[Rmse7_rf2[2],Rmse8_rf2[2],Rmse9_rf2[2]]\n",
    "Day_4_RF2=[Rmse7_rf2[3],Rmse8_rf2[3],Rmse9_rf2[3]]\n",
    "Day_5_RF2=[Rmse7_rf2[4],Rmse8_rf2[4],Rmse9_rf2[4]]\n",
    "Day_6_RF2=[Rmse7_rf2[5],Rmse8_rf2[5],Rmse9_rf2[5]]\n",
    "Day_7_RF2=[Rmse7_rf2[6],Rmse8_rf2[6],Rmse9_rf2[6]]\n",
    "Day_8_RF2=[Rmse7_rf2[7],Rmse8_rf2[7],Rmse9_rf2[7]]\n",
    "#Day_9_RF=[Rmse7_rf[8],Rmse8_rf[8],Rmse9_rf[8]]\n",
    "#Day_10_RF=[Rmse7_rf[9],Rmse8_rf[9],Rmse9_rf[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import chart_studio\n",
    "import plotly.express as px\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "Day_1_MQI=[M7[0],M8[0],M9[0]]\n",
    "Day_2_MQI=[M7[1],M8[1],M9[1]]\n",
    "Day_3_MQI=[M7[2],M8[2],M9[2]]\n",
    "Day_4_MQI=[M7[3],M8[3],M9[3]]\n",
    "Day_5_MQI=[M7[4],M8[4],M9[4]]\n",
    "Day_6_MQI=[M7[5],M8[5],M9[5]]\n",
    "Day_7_MQI=[M7[6],M8[6],M9[6]]\n",
    "#Day_8_MQI=[M7[7],M8[7],M9[7]]\n",
    "\n",
    "Day_1_BQI=[B7[0],B8[0],B9[0]]\n",
    "Day_2_BQI=[B7[1],B8[1],B9[1]]\n",
    "Day_3_BQI=[B7[2],B8[2],B9[2]]\n",
    "Day_4_BQI=[B7[3],B8[3],B9[3]]\n",
    "Day_5_BQI=[B7[4],B8[4],B9[4]]\n",
    "Day_6_BQI=[B7[5],B8[5],B9[5]]\n",
    "Day_7_BQI=[B7[6],B8[6],B9[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MQI2=[M72[0],M82[0],M92[0]]\n",
    "Day_2_MQI2=[M72[1],M82[1],M92[1]]\n",
    "Day_3_MQI2=[M72[2],M82[2],M92[2]]\n",
    "Day_4_MQI2=[M72[3],M82[3],M92[3]]\n",
    "Day_5_MQI2=[M72[4],M82[4],M92[4]]\n",
    "Day_6_MQI2=[M72[5],M82[5],M92[5]]\n",
    "Day_7_MQI2=[M72[6],M82[6],M92[6]]\n",
    "#Day_8_MQI2=[M72[7],M92[7],M92[7]]\n",
    "\n",
    "Day_1_BQI2=[B72[0],B82[0],B92[0]]\n",
    "Day_2_BQI2=[B72[1],B82[1],B92[1]]\n",
    "Day_3_BQI2=[B72[2],B82[2],B92[2]]\n",
    "Day_4_BQI2=[B72[3],B82[3],B92[3]]\n",
    "Day_5_BQI2=[B72[4],B82[4],B92[4]]\n",
    "Day_6_BQI2=[B72[5],B82[5],B92[5]]\n",
    "Day_7_BQI2=[B72[6],B82[6],B92[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_1_MQI3=[M73[0],M83[0],M93[0]]\n",
    "Day_2_MQI3=[M73[1],M83[1],M93[1]]\n",
    "Day_3_MQI3=[M73[2],M83[2],M93[2]]\n",
    "Day_4_MQI3=[M73[3],M83[3],M93[3]]\n",
    "Day_5_MQI3=[M73[4],M83[4],M93[4]]\n",
    "Day_6_MQI3=[M73[5],M83[5],M93[5]]\n",
    "Day_7_MQI3=[M73[6],M83[6],M93[6]]\n",
    "#Day_8_MQI2=[M72[7],M92[7],M92[7]]\n",
    "\n",
    "Day_1_BQI3=[B73[0],B83[0],B93[0]]\n",
    "Day_2_BQI3=[B73[1],B83[1],B93[1]]\n",
    "Day_3_BQI3=[B73[2],B83[2],B93[2]]\n",
    "Day_4_BQI3=[B73[3],B83[3],B93[3]]\n",
    "Day_5_BQI3=[B73[4],B83[4],B93[4]]\n",
    "Day_6_BQI3=[B73[5],B83[5],B93[5]]\n",
    "Day_7_BQI3=[B73[6],B83[6],B93[6]]\n",
    "#Day_8_MQI2=[M72[7],M92[7],M92[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-opera",
   "metadata": {},
   "source": [
    "Day_1_ANN=[Rmse7_ann[0],Rmse8_ann[0],Rmse9_ann[0]]\n",
    "Day_2_ANN=[Rmse7_ann[1],Rmse8_ann[1],Rmse9_ann[1]]\n",
    "Day_3_ANN=[Rmse7_ann[2],Rmse8_ann[2],Rmse9_ann[2]]\n",
    "Day_4_ANN=[Rmse7_ann[3],Rmse8_ann[3],Rmse9_ann[3]]\n",
    "Day_5_ANN=[Rmse7_ann[4],Rmse8_ann[4],Rmse9_ann[4]]\n",
    "Day_6_ANN=[Rmse7_ann[5],Rmse8_ann[5],Rmse9_ann[5]]\n",
    "Day_7_ANN=[Rmse7_ann[6],Rmse8_ann[6],Rmse9_ann[6]]\n",
    "Day_8_ANN=[Rmse7_ann[7],Rmse8_ann[7],Rmse9_ann[7]]\n",
    "Day_9_ANN=[Rmse7_ann[8],Rmse8_ann[8],Rmse9_ann[8]]\n",
    "Day_10_ANN=[Rmse7_ann[9],Rmse8_ann[9],Rmse9_ann[9]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_P=Day_1_rf+Day_2_rf+Day_3_rf+Day_4_rf+Day_5_rf+Day_6_rf+Day_7_rf+Day_8_rf\n",
    "#XGBoost_P=Day_1_ann+Day_2_ann+Day_3_ann+Day_4_ann+Day_5_ann+Day_6_ann+Day_7_ann+Day_8_ann+Day_9_ann+Day_10_ann\n",
    "#RF_R=Day_1_RF+Day_2_RF+Day_3_RF+Day_4_RF+Day_5_RF+Day_6_RF+Day_7_RF+Day_8_RF\n",
    "#RF_R2=Day_1_RF2+Day_2_RF2+Day_3_RF2+Day_4_RF2+Day_5_RF2+Day_6_RF2+Day_7_RF2+Day_8_RF2\n",
    "#XGBoost_R=Day_1_ANN+Day_2_ANN+Day_3_ANN+Day_4_ANN+Day_5_ANN+Day_6_ANN+Day_7_ANN+Day_8_ANN+Day_9_ANN+Day_10_ANN\n",
    "MQI=Day_1_MQI+Day_2_MQI+Day_3_MQI+Day_4_MQI+Day_5_MQI+Day_6_MQI+Day_7_MQI#+Day_8_MQI\n",
    "MQI2=Day_1_MQI2+Day_2_MQI2+Day_3_MQI2+Day_4_MQI2+Day_5_MQI2+Day_6_MQI2+Day_7_MQI2#+Day_8_MQI2\n",
    "MQI3=Day_1_MQI3+Day_2_MQI3+Day_3_MQI3+Day_4_MQI3+Day_5_MQI3+Day_6_MQI3+Day_7_MQI3\n",
    "\n",
    "BQI=Day_1_BQI+Day_2_BQI+Day_3_BQI+Day_4_BQI+Day_5_BQI+Day_6_BQI+Day_7_BQI#+Day_8_MQI\n",
    "BQI2=Day_1_BQI2+Day_2_BQI2+Day_3_BQI2+Day_4_BQI2+Day_5_BQI2+Day_6_BQI2+Day_7_BQI2#+Day_8_MQI2\n",
    "BQI3=Day_1_BQI3+Day_2_BQI3+Day_3_BQI3+Day_4_BQI3+Day_5_BQI3+Day_6_BQI3+Day_7_BQI3\n",
    "x0=['0' for i in range(3)]\n",
    "x1=['5' for i in range(3)]\n",
    "x2=['10' for i in range(3)]\n",
    "x3=['15' for i in range(3)]\n",
    "x4=['20' for i in range(3)]\n",
    "x5=['25' for i in range(3)]\n",
    "x6=['30' for i in range(3)]\n",
    "x7=['35' for i in range(3)]\n",
    "x8=['40' for i in range(3)]\n",
    "x9=['45' for i in range(3)]\n",
    "\n",
    "Reg=[10 for i in range(30)]\n",
    "Spatial=[25 for i in range(30) ]\n",
    "Intervention=[30 for i in range(30) ]\n",
    "Hs_and_sp=[50 for i in range(30) ]\n",
    "\n",
    "\n",
    "x=x1+x2+x3+x4+x5+x6+x7\n",
    "X=x0+x1+x2+x3+x4+x5+x6+x7+x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio\n",
    "fig = go.Figure() \n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([1, 3, 2, 3, 1])\n",
    "# Defining x axis\n",
    "x = x\n",
    "\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=MQI,\n",
    "    x=x,\n",
    "    name='Accuracy',\n",
    "    marker_color='#8470FF',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    #y=MQI3,\n",
    "    #x=x,\n",
    "    #name=r'$O_3$',\n",
    "    #marker_color='#FF8000',\n",
    "    #showlegend=False\n",
    "   \n",
    "#))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "  \n",
    "    # defining y axis in corresponding\n",
    "    # to x-axis\n",
    "    y=BQI,\n",
    "    x=x,\n",
    "    name='Bias',\n",
    "    marker_color='#FF8000',\n",
    "    showlegend=False\n",
    "   \n",
    "))\n",
    "\n",
    "#fig.add_trace(go.Box(\n",
    "  \n",
    "    #defining y axis in corresponding\n",
    "   # to x-axis\n",
    "    #y=RF_R,\n",
    "    #x=x,\n",
    "    #name='SMAE',\n",
    "    #marker_color='olive',\n",
    "   # showlegend=True\n",
    "   \n",
    "#))\n",
    "#fig.add_trace(go.Box(\n",
    "   #y=ANN_R,\n",
    "    #x=x,\n",
    "    #name='XGBoost(NMAE)',\n",
    "    #marker_color='orangered',\n",
    "    #showlegend=True\n",
    "\n",
    "\n",
    "#))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(autosize=True,\n",
    "                 title={'text':r\"$O_3$\",\n",
    "        'y':0.77,\n",
    "        'x':0.5,\n",
    "        #'xanchor': 'center',\n",
    "        #'yanchor': 'top'\n",
    "                       }, \n",
    "    width=430,\n",
    "    height=400,\n",
    "                  \n",
    "  legend=dict( yanchor=\"bottom\",\n",
    "    y=0.5,\n",
    "    x=0.6,\n",
    "    \n",
    "    orientation=\"v\"\n",
    "),\n",
    "             \n",
    "    # group together boxes of the different\n",
    "    # traces for each value of x\n",
    "    boxmode='group',\n",
    "                  plot_bgcolor='rgba(0,0,0,0)'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days of training\",tickfont = dict(size=16),\n",
    "                 titlefont = dict(size=24),linewidth=1.4, linecolor='black',tick0 = 5,\n",
    "        dtick = 5,mirror=True)\n",
    "fig.update_yaxes(title_text=\"Performance(%)\",range=[0,100],tickfont = dict(size=16),titlefont = dict(size=24),\n",
    "                 linewidth=1.4, linecolor='black',tick0 = 20,\n",
    "        dtick = 20,mirror=True)\n",
    "fig.show()\n",
    "chart_studio.plotly.sign_in('vinylango', 'gybbJVWfRSUoTcRRSa6J')\n",
    "chart_studio.plotly.image.save_as(fig, filename='models_boxplot.png')\n",
    "#Image('models_boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-intro",
   "metadata": {},
   "source": [
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Reg, \n",
    "                name=\"RC\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'green', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Spatial, \n",
    "                name=\"SGS\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'purple', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                    \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Intervention, \n",
    "                name=\"IS/IM\",\n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'orange', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "                \n",
    "                        ))\n",
    "fig.add_trace(go.Scatter(x=X,\n",
    "                y=Hs_and_sp, \n",
    "                name=\"HA/SP\",\n",
    "                    \n",
    "                mode = 'lines',\n",
    "                #marker_color='dodgerblue',\n",
    "                line = dict(shape = 'linear',width = 2, color = 'dodgerblue', dash = 'dot'),\n",
    "                connectgaps = True,\n",
    "                showlegend=False\n",
    "               \n",
    "                        ))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-elite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-traveler",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
