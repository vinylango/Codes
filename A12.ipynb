{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adult-tractor",
   "metadata": {},
   "source": [
    "# Zindi: CGIAR Crop Damage Competion Challenge-Ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from tensorflow import data as tf_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow.compat.v2.keras as keras\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# image processing\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data,dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from keras.layers import Activation, Dense, Flatten,BatchNormalization\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "# model / neural network\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "chinese-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "tf.random.set_seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "accomplished-harassment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn==0.5.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.5.0)\r\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tflearn==0.5.0) (1.23.5)\r\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tflearn==0.5.0) (1.15.0)\r\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tflearn==0.5.0) (8.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-associate",
   "metadata": {},
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-winter",
   "metadata": {},
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-prefix",
   "metadata": {},
   "source": [
    "train['damage'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-reception",
   "metadata": {},
   "source": [
    "train= train[train[\"damage\"]=='other']\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-credit",
   "metadata": {},
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-stephen",
   "metadata": {},
   "source": [
    "od='/Users/kipkemoivincent/Desktop/cgiar-crop-damage-classification-challenge20231201-29880-1s2g3v7/CGFR_Images/Train/'\n",
    "td='/Users/kipkemoivincent/Desktop/cgiar-crop-damage-classification-challenge20231201-29880-1s2g3v7/CGFR_Images/train_data/other/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-ballet",
   "metadata": {},
   "source": [
    "fn_train=train['filename'].to_list()\n",
    "#fn_test=test['filename'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-allah",
   "metadata": {},
   "source": [
    "import shutil\n",
    "for i in range(len(fn_train)):\n",
    "    shutil.copy2(od+fn_train[i],td+fn_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-inspector",
   "metadata": {},
   "source": [
    "import shutil\n",
    "for i in range(len(fn_test)):\n",
    "    shutil.copy2(od+fn_test[i],td+fn_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "internal-potential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26068 files belonging to 5 classes.\n",
      "Using 20855 files for training.\n",
      "Found 26068 files belonging to 5 classes.\n",
      "Using 5213 files for validation.\n"
     ]
    }
   ],
   "source": [
    "image_size = (150, 150)\n",
    "batch_size = 26068\n",
    "train_ds= keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/Users/kipkemoivincent/Desktop/cgiar-crop-damage-classification-challenge20231201-29880-1s2g3v7/train_data\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_ds= keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/Users/kipkemoivincent/Desktop/cgiar-crop-damage-classification-challenge20231201-29880-1s2g3v7/train_data\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weekly-arbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8663 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "Test_ds= keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/Users/kipkemoivincent/Desktop/cgiar-crop-damage-classification-challenge20231201-29880-1s2g3v7/T_data\",seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggregate-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = Test_ds.file_paths\n",
    "Test_fn=[]\n",
    "for i in range(len(file_paths)):\n",
    "    Test_fn.append(file_paths[i][113:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latin-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def input_fn():\n",
    "    dataset = tf.data.TFRecordDataset(filenames = filenames)\n",
    "    \n",
    "    dataset = dataset.map(_parse_function)\n",
    "    iterator = iter(dataset)\n",
    "    next_element = iterator.get_next()\n",
    "    return next_element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-milwaukee",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in train_ds:\n",
    "    labels=[int(labels[i]) for i in range(len(labels))]\n",
    "    for i in range(len(labels)):\n",
    "        if int(labels[i])==1:\n",
    "            labels[i]='DR'\n",
    "        elif int(labels[i])==2:\n",
    "            labels[i]='G'\n",
    "        elif int(labels[i])==3:\n",
    "            labels[i]='NR'\n",
    "        elif int(labels[i])==4:\n",
    "            labels[i]='other'\n",
    "        else:\n",
    "            labels[i]='WR'\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
    "        plt.title(labels[i])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alternate-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1=[]\n",
    "y_train1=[]\n",
    "X_test1=[]\n",
    "y_test1=[]\n",
    "for images, labels in train_ds:\n",
    "    X_train1.append(images)\n",
    "    y_train1.append(labels)\n",
    "for images, labels in val_ds:\n",
    "    X_train1.append(images)\n",
    "    y_train1.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boring-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test1=[]\n",
    "for images, labels in Test_ds:\n",
    "    Test1.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "familiar-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test=np.array(Test1[0])\n",
    "Test=Test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "parental-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train=np.array(X_train1[0])\n",
    "X_test=np.array(X_train1[1])\n",
    "y_train=np.array(y_train1[0])\n",
    "y_test=np.array(y_train1[1])\n",
    "#X_train=X_train.reshape(4800,67500)\n",
    "#X_test=X_test.reshape(1200,67500)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "outside-empty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4], dtype=int32), array([ 883, 2314,   49,  103, 1864]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "rolled-estate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4], dtype=int32), array([3633, 9309,  223,  316, 7374]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "multiple-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-discussion",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "# summarize the new class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-teddy",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-blackberry",
   "metadata": {},
   "source": [
    "!pip install -U threadpoolctl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-fleet",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "X_train = X_train.reshape(20855,67500)\n",
    "oversample = RandomOverSampler()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-johns",
   "metadata": {},
   "source": [
    "X_train = X_train.reshape(-1,150,150,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-passenger",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-moderator",
   "metadata": {},
   "source": [
    "## Normalizing the data to help with the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "molecular-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amazing-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test/=255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-vocabulary",
   "metadata": {},
   "source": [
    "from flaml import AutoML\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 200,  # in seconds\n",
    "    \"metric\": 'log_loss',\n",
    "    \"task\": 'classification'\n",
    "}\n",
    "automl.fit(X_train=X_train, y_train=y_train,\n",
    "           **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "introductory-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1=y_train\n",
    "y_test1=y_test\n",
    "Y_train=tf.keras.utils.to_categorical(y_train1, num_classes=5)\n",
    "Y_test=tf.keras.utils.to_categorical(y_test1, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "endless-panel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20855, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-cleveland",
   "metadata": {},
   "source": [
    "# CNN1 (with Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-optics",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "model = Sequential()\n",
    "# convolutional layer\n",
    "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), \n",
    "                 padding='valid', activation='relu', input_shape=(150,150,3)))\n",
    "model.add(MaxPool2D(pool_size=(1,1)))\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "# hidden layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "Optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07)\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Optimizer)\n",
    "\n",
    "# training the model for 10 epochs\n",
    "history =model.fit(X_train, Y_train, batch_size=2, epochs=4, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "equipped-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    inputs = keras.layers.Input(shape=(150, 150, 3))\n",
    "\n",
    "    x = keras.layers.Conv2D(32, (3,3), padding='same')(inputs)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = keras.layers.MaxPooling2D(2, strides=2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(32,(3,3), padding='same')(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = keras.layers.MaxPooling2D(2, strides=2)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(32,(3,3), padding='same')(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = keras.layers.MaxPooling2D(2, strides=2)(x)\n",
    "\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.1)(x)\n",
    "  \n",
    "    outputs = keras.layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "keras.backend.clear_session()\n",
    "model = Model()\n",
    "model.compile('Adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "written-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20855 samples, validate on 5213 samples\n",
      "Epoch 1/40\n",
      "20855/20855 [==============================] - 713s 34ms/sample - loss: 1.1208 - accuracy: 0.4440 - val_loss: 1.0807 - val_accuracy: 0.4650\n",
      "Epoch 2/40\n",
      "20855/20855 [==============================] - 1050s 50ms/sample - loss: 1.0488 - accuracy: 0.4872 - val_loss: 1.0717 - val_accuracy: 0.4483\n",
      "Epoch 3/40\n",
      "20855/20855 [==============================] - 938s 45ms/sample - loss: 0.9894 - accuracy: 0.5471 - val_loss: 0.9718 - val_accuracy: 0.5958\n",
      "Epoch 4/40\n",
      "20855/20855 [==============================] - 817s 39ms/sample - loss: 0.9536 - accuracy: 0.5756 - val_loss: 0.9379 - val_accuracy: 0.5786\n",
      "Epoch 5/40\n",
      "20855/20855 [==============================] - 1000s 48ms/sample - loss: 0.9374 - accuracy: 0.5850 - val_loss: 0.9102 - val_accuracy: 0.6165\n",
      "Epoch 6/40\n",
      "20855/20855 [==============================] - 874s 42ms/sample - loss: 0.9221 - accuracy: 0.5966 - val_loss: 0.9649 - val_accuracy: 0.5782\n",
      "Epoch 7/40\n",
      "20855/20855 [==============================] - 845s 41ms/sample - loss: 0.9141 - accuracy: 0.6018 - val_loss: 0.9115 - val_accuracy: 0.5964\n",
      "Epoch 8/40\n",
      "20855/20855 [==============================] - 851s 41ms/sample - loss: 0.9045 - accuracy: 0.6072 - val_loss: 0.8831 - val_accuracy: 0.6213\n",
      "Epoch 9/40\n",
      "20855/20855 [==============================] - 801s 38ms/sample - loss: 0.8968 - accuracy: 0.6109 - val_loss: 0.8851 - val_accuracy: 0.6231\n",
      "Epoch 10/40\n",
      "20855/20855 [==============================] - 911s 44ms/sample - loss: 0.8932 - accuracy: 0.6079 - val_loss: 0.9103 - val_accuracy: 0.6171\n",
      "Epoch 11/40\n",
      "20855/20855 [==============================] - 9880s 474ms/sample - loss: 0.8854 - accuracy: 0.6150 - val_loss: 0.8890 - val_accuracy: 0.6238\n",
      "Epoch 12/40\n",
      "20855/20855 [==============================] - 1052s 50ms/sample - loss: 0.8799 - accuracy: 0.6215 - val_loss: 0.8743 - val_accuracy: 0.6152\n",
      "Epoch 13/40\n",
      "20855/20855 [==============================] - 1162s 56ms/sample - loss: 0.8802 - accuracy: 0.6195 - val_loss: 0.8815 - val_accuracy: 0.6271\n",
      "Epoch 14/40\n",
      "20855/20855 [==============================] - 924s 44ms/sample - loss: 0.8782 - accuracy: 0.6244 - val_loss: 0.9968 - val_accuracy: 0.5670\n",
      "Epoch 15/40\n",
      "20855/20855 [==============================] - 788s 38ms/sample - loss: 0.8770 - accuracy: 0.6251 - val_loss: 0.9025 - val_accuracy: 0.6319\n",
      "Epoch 16/40\n",
      "20855/20855 [==============================] - 815s 39ms/sample - loss: 0.8754 - accuracy: 0.6235 - val_loss: 0.8703 - val_accuracy: 0.6407\n",
      "Epoch 17/40\n",
      "20855/20855 [==============================] - 773s 37ms/sample - loss: 0.9218 - accuracy: 0.6296 - val_loss: 0.8840 - val_accuracy: 0.6305\n",
      "Epoch 18/40\n",
      "20855/20855 [==============================] - 919s 44ms/sample - loss: 0.8734 - accuracy: 0.6254 - val_loss: 0.8680 - val_accuracy: 0.6346\n",
      "Epoch 19/40\n",
      "20855/20855 [==============================] - 1017s 49ms/sample - loss: 0.8746 - accuracy: 0.6272 - val_loss: 0.9006 - val_accuracy: 0.6403\n",
      "Epoch 20/40\n",
      "20855/20855 [==============================] - 968s 46ms/sample - loss: 0.8718 - accuracy: 0.6307 - val_loss: 0.8917 - val_accuracy: 0.6204\n",
      "Epoch 21/40\n",
      "20855/20855 [==============================] - 878s 42ms/sample - loss: 0.8683 - accuracy: 0.6285 - val_loss: 0.8730 - val_accuracy: 0.6227\n",
      "Epoch 22/40\n",
      "20855/20855 [==============================] - 838s 40ms/sample - loss: 0.8694 - accuracy: 0.6297 - val_loss: 0.8737 - val_accuracy: 0.6259\n",
      "Epoch 23/40\n",
      "20855/20855 [==============================] - 746s 36ms/sample - loss: 0.8694 - accuracy: 0.6302 - val_loss: 0.8695 - val_accuracy: 0.6355\n",
      "Epoch 24/40\n",
      "20855/20855 [==============================] - 783s 38ms/sample - loss: 0.8594 - accuracy: 0.6317 - val_loss: 0.9950 - val_accuracy: 0.5977\n",
      "Epoch 25/40\n",
      "20855/20855 [==============================] - 880s 42ms/sample - loss: 0.8690 - accuracy: 0.6359 - val_loss: 0.8808 - val_accuracy: 0.6420\n",
      "Epoch 26/40\n",
      "20855/20855 [==============================] - 4949s 237ms/sample - loss: 0.9023 - accuracy: 0.6345 - val_loss: 0.8662 - val_accuracy: 0.6382\n",
      "Epoch 27/40\n",
      "20855/20855 [==============================] - 3524s 169ms/sample - loss: 0.8640 - accuracy: 0.6358 - val_loss: 0.8677 - val_accuracy: 0.6315\n",
      "Epoch 28/40\n",
      "20855/20855 [==============================] - 3474s 167ms/sample - loss: 0.8654 - accuracy: 0.6353 - val_loss: 0.8572 - val_accuracy: 0.6424\n",
      "Epoch 29/40\n",
      "20855/20855 [==============================] - 1120s 54ms/sample - loss: 0.8699 - accuracy: 0.6354 - val_loss: 0.8605 - val_accuracy: 0.6309\n",
      "Epoch 30/40\n",
      "20855/20855 [==============================] - 842s 40ms/sample - loss: 0.8712 - accuracy: 0.6355 - val_loss: 0.8750 - val_accuracy: 0.6409\n",
      "Epoch 31/40\n",
      "20855/20855 [==============================] - 771s 37ms/sample - loss: 0.8689 - accuracy: 0.6341 - val_loss: 0.8763 - val_accuracy: 0.6376\n",
      "Epoch 32/40\n",
      "20855/20855 [==============================] - 809s 39ms/sample - loss: 0.8672 - accuracy: 0.6317 - val_loss: 0.8559 - val_accuracy: 0.6399\n",
      "Epoch 33/40\n",
      "20855/20855 [==============================] - 874s 42ms/sample - loss: 0.8653 - accuracy: 0.6361 - val_loss: 0.9775 - val_accuracy: 0.6140\n",
      "Epoch 34/40\n",
      "20855/20855 [==============================] - 850s 41ms/sample - loss: 0.8649 - accuracy: 0.6360 - val_loss: 0.9039 - val_accuracy: 0.5939\n",
      "Epoch 35/40\n",
      "20855/20855 [==============================] - 839s 40ms/sample - loss: 0.8641 - accuracy: 0.6355 - val_loss: 0.9468 - val_accuracy: 0.6077\n",
      "Epoch 36/40\n",
      "20855/20855 [==============================] - 871s 42ms/sample - loss: 0.8639 - accuracy: 0.6376 - val_loss: 0.9360 - val_accuracy: 0.6144\n",
      "Epoch 37/40\n",
      "20855/20855 [==============================] - 1052s 50ms/sample - loss: 0.8645 - accuracy: 0.6356 - val_loss: 0.9168 - val_accuracy: 0.6204\n",
      "Epoch 38/40\n",
      "20855/20855 [==============================] - 955s 46ms/sample - loss: 0.8706 - accuracy: 0.6334 - val_loss: 0.8967 - val_accuracy: 0.6079\n",
      "Epoch 39/40\n",
      "20855/20855 [==============================] - 1090s 52ms/sample - loss: 0.8711 - accuracy: 0.6288 - val_loss: 0.8825 - val_accuracy: 0.6321\n",
      "Epoch 40/40\n",
      "20855/20855 [==============================] - 975s 47ms/sample - loss: 0.8749 - accuracy: 0.6311 - val_loss: 0.9432 - val_accuracy: 0.6181\n"
     ]
    }
   ],
   "source": [
    "# training the model for 10 epochs\n",
    "history =model.fit(X_train, Y_train, batch_size=1,epochs=40, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "collectible-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 61.81%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "# summarize model.\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "unknown-wright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 662   92    0    0  129]\n",
      " [ 255 1184    0    0  875]\n",
      " [  32    7    0    0   10]\n",
      " [  21   36    0    0   46]\n",
      " [ 186  302    0    0 1376]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.75      0.65       883\n",
      "           1       0.73      0.51      0.60      2314\n",
      "           2       0.00      0.00      0.00        49\n",
      "           3       0.00      0.00      0.00       103\n",
      "           4       0.56      0.74      0.64      1864\n",
      "\n",
      "    accuracy                           0.62      5213\n",
      "   macro avg       0.37      0.40      0.38      5213\n",
      "weighted avg       0.62      0.62      0.61      5213\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_p=model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_p, axis=1)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "through-execution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x149c8fcd0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEGCAYAAAAT05LOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyOElEQVR4nO3deXxU1fn48c8zySQhARKSQAghCCpCERcQEVworuDyE9u6VK3a1lZpsVrUulRb/Wprtdq6b7jUfcGt4Aa4UTd2AdkEIrIHQggJgZBkMvP8/rgXCBCSmWQmM5N53q/XfTH3zp17noHw5Jx7zj1HVBVjjEk0nmgHYIwx0WDJzxiTkCz5GWMSkiU/Y0xCsuRnjElIydEOoL6kjAxNzs6OdhhBSdtYE+0QQqK+umiH0GZJSkq0QwjajroKav07pCXXGHFihm4u8wd17pxvayar6siWlBcpMZX8krOz6X7N2GiHEZRD/r0i2iGEpG7DxmiH0GYldz8g2iEE7eu1L7X4GqVlfmZM7h7Uud7873NbXGCExFTyM8bEA8WvgWgH0WKW/IwxIVEgQPw/HGHJzxgTsgBW8zPGJBhF8Vmz1xiTaBTwW7PXGJOI7J6fMSbhKOBvA7NBWfIzxoQs/u/4WfIzxoRIUbvnZ4xJPKrgi//cZ8nPGBMqwU+LHg+OCZb8jDEhUSBgNT9jTCKymp8xJuE4g5wt+RljEowCPo3/eZAt+RljQqII/jYwCXybSH4dvDXcNeR/9M4qA+CmacOZV9qVS/os4OJDFhFQYeq6Hvxz7lCO67qG6wfMwOsJ4At4uOeboUzfWBCVuM++cBUjfrIWEZj8TncmvHIAv/7jUgafsIm6Og/Fa9J54PZD2b7NG5X4GjNo+FZG37meJI/y4avZjH8kL9oh7VcsxnrNzXMZfNxGyrekMuaSEwH49ZhFDD5uI3U+oXhdBg/cNYDt27wkJwe46ob59O5bTiAgjHuwPwvmRneO0IBas7dRIjISeBBIAp5W1bsjUc6tg77i8+JC/vDFaXg9ftKS6jgmbx0nd1/J2e+fR20giezUHQBsqWnHlVNPp2RHBr0zy3j25Pc44e1LIxFWow44qJIRP1nLtZcOwecT7nzkG2Z+0Zm503N47uHeBPwefnX1Ms7/9Q/856FDWj2+xng8ypi71nHzzw+ktNjLwx8sZ/rkTFYvT4t2aPuI1Vg//qAH773Vi2v/MnfXsbmzOvPcEz9y/u1/t5jzL1nOfx7vx4izVwEw5tITycyq4Y5/TeePvxmGRikBtZV7fhGru4pIEvAocDrQD7hQRPqFu5z23hqOzivmjaK+APgCSVT6UrnokEWMWzSA2kASAGU17QBYvCWXkh0ZACyv6ERakp8UT3DrEYRTYa/tLFuYRU11EgG/hwVzOnHsSRuZOz2XgN/5Z/luQSY5XapbPbam9BlQxfqVKWxYnUqdz8PUCVkMHVER7bAaFKuxLpqfQ+XWPdf+mDuzy+5/+0WdyOni/MLu0bOS+XOcml5FeSrbtnnp3be8VePdk+BXT1BbLItkdIOBIlVdoaq1wGvAqHAXUti+krLqNO4Z+hkTzniDvw+ZSrskH706VDCoSzFvjnybl0+dwGE5Jft8dmSPFSwqy92VIFvTqu/bc+iALXTIrCU1zc+g40vpnLdnojt11DrmfB17SyDkdPWxaf3u/7ilxV5y831RjGj/4inW+k49czVzpnUB4Ieijgw5fgOepAB5+ds5uE85uXk7ohabM5OzJ6gtlkWy2VsArKm3vxY4Zu+TROQK4AqA5KxOIReSJAEOzS7lzlnHM39zHrcO+pIr+88lyRMgM6WGcyf9hMNzSnjwhI846b8XgVtdPzizjD8NmMGvPjmzGV+t5db80J43n+vJ3x6bQ/WOJFYs7YA/sLspccHlK/DXefjsg/yoxGei54JLl+H3C59NcRYJmvJ+Dwp7buPBZz6nZEM6SxZmE/BHr9mpKtRq61cYwi3qHR6qOg4YB5BaWBjyuPENVe3ZUJXB/M3OTexJqw7iyv5z2VDVnilregHCt5vzUBWyU6spq2lH1/RtPPbjyfzp6xNZvS0zrN8nFFMmdGfKBOcH/NKrlrN5YyoAp/y/dRx9wiZuGT0IYvDeyuYNXjp3q921n5vvo7Q49jplIL5iBTjljNUcfdxGbrl6KDv/7QN+D0891H/XOfc98QXr1rSPUoSOQAz+XIYqkvXSdUBhvf3u7rGwKq1Op7iqPb06lgMwNH8tRRWd+HhNT4bkrQegZ4dyvB4/ZTVpdPDWMO7ED7lv7jF8sym6tarMTs7av5277uDYEzcy9cN8jjq2lJ9dtpI7/jiAmurY/O26dF46Bb1qySusIdkbYPiocqZPid4vkcbEU6xHHVPCzy4q4o4bB1NTs7tekppaR2qas+7ykUeX4PcLa1Z2iFaYboeHJ6gtlkWy5jcL6C0ivXCS3s+BiyJR0J2zjudfx32C1+NnzbaO3DTtRHbUJfOPoVN5/6zX8QWSuOHrkwDhkj4LOaBDBVcdNoerDpsDwC8/OWtXh0hr+vN98+mY6aOuTnj8nh+xfZuX0TcuwesN8PfHndi+W5DJo3eFvZ+oRQJ+4dFbCrjrlRV4kmDKa9msWhZ7Pb0Qu7HecPscDhtQSsesWp5/ZwovP9OH8y5Z7vzbPzANcDo9Hr33CDI71XLn/dPQgLB5Uxr33TEwytFL2DozRORZ4CygRFX7u8fuBf4fUAt8D/xKVcvd924GLgf8wNWqOtk9HvLIEtEIzsgqImcAD7gBPauqf2/s/NTCQrVFyyPDFi2PnORe8bVoeUXNhha1WQ8+LF3/NSG44VfnHDR/jqoO2t/7IjIM2Aa8UC/5nQZ8qqp1InIPgKre6I4WeRWnM7Ub8DGwM5BlwKk4fQuzgAtVdXFjsUX0np+qfgB8EMkyjDGtzx+mMYaq+rmI9Nzr2JR6u9OBc93Xo4DXVLUG+EFEinASIbgjSwBEZOfIkuglP2NM26MIPg06deSKyOx6++PcTs5g/Rp43X1dgJMMd1rrHoMgRpbszZKfMSYkOzs8glTaWLO3MSJyC1AHvNyczzfFkp8xJiSKhK3Zuz8i8kucjpCTdXfHRGMjSEIeWRLbfdHGmJgUySc83J7bG4CzVbWq3lsTgZ+LSKo7iqQ3MJN6I0tEJAVnZMnEpsqxmp8xJiSqhHOoy6vAcJx7g2uB24CbgVTgIxEBmK6qo1V1kYiMx+nIqAPGqKrfvc5VwGR2jyxZ1FTZlvyMMSFxOjzCMwBfVS9s4PAzjZz/d2CfIXPNGVliyc8YE7JYf3ojGJb8jDEhUcQmMzXGJCar+RljEo6zbq8lP2NMwpE2MY29JT9jTEicpStjc7q1UFjyM8aERFWs2WuMSUyxvjhRMCz5GWNC4ixgZPf8jDEJJ3wzOUdTTCW/1PIAPd+NvXVqG3LR/2Y3fVIMeaFPYdMnmWZZ/e+MaIcQtNrrWp60nKEuVvMzxiSYcD7bG02W/IwxIYv1BcmDYcnPGBMSZ0ora/YaYxKQ3fMzxiQcZ1YXa/YaYxKM83ibJT9jTMKxmp8xJkHZEx7GmIRjvb3GmIRlzV5jTMKxNTyMMQlJgbo2UPOL/29gjGl1AfUEtTVFRJ4VkRIRWVjvWLaIfCQiy90/O7nHRUQeEpEiEflWRAbW+8xl7vnLReSyYL6DJT9jTGjUafYGswXhOWDkXsduAj5R1d7AJ+4+wOlAb3e7AngcnGQJ3AYcAwwGbtuZMBtjyc8YE5Kdk5kGszV5LdXPgbK9Do8CnndfPw+cU+/4C+qYDmSJSD4wAvhIVctUdQvwEfsm1H3YPT9jTMhC6PDIFZH6k1+OU9VxTXwmT1WL3dcbgDz3dQGwpt55a91j+zveqLhPfp1ztnPDmC/olLkDVeGDTw7hnQ/7ccm5cznj5OVUbE0F4NlXj2LmvO7kda7kmX//l7XrOwKwZHlnHnz62IjF99XNnVg3tR1pOQHOfm8DACs/bMf8RzKp+D6ZM97YSO5hPgACPvj61mzKFnvROuHAc7Zz2JWVu64V8MP7P8sjPc/PyU+WRizmYAwavpXRd64nyaN8+Go24x/Ja/pDURKLsWZM3Ez6x+UA+A5IpfwP3ci5fRWeHQEAPBV+anu3Y8vNhaQs3E72P9bg7+IFYMeQjmy7oHO0Qg91MtNSVR3U7LJUVUS0uZ9vTMSSn4g8C5wFlKhq/0iV4/cLT754NEU/5NAuzcdj/3iXOd92A+Ct9/vx5nv7Fr1+YwdG3zgqUiHt4eCfVtH3F9v46sacXceyDvEx/OFSpt+2522JlZPSCdQKZ7+7kbodwoQzu9LrzCrad/cD8N0L7ck8yIdvW3TvVng8ypi71nHzzw+ktNjLwx8sZ/rkTFYvT4tqXA2JxVg9m31kvF9GyUMHQaqHTveupd2XW9l8V69d53S6Zw3Vgzvs2q/9UTplt/aIRrj7UIS6QER/BjeKSL6qFrvN2hL3+Dqg/pTk3d1j64Dhex2f2lQhkfwGzxFEu7ulysrTKfrBSSw7qr2sXpdJbnZVpIsNWt7RNaRmBvY4lnVQHZkH1u1zrgjU7RACdVBXLXi8ire980tv+4Yk1k5tR+9zt7dK3I3pM6CK9StT2LA6lTqfh6kTshg6oiLaYTUoVmMVvyK1Cn5FagIEsnfXQ6TKT+qC7VQf06GRK0RXuO757cdEYGeP7WXAhHrHL3V7fYcAFW7zeDJwmoh0cjs6TnOPNSpiNT9V/VxEekbq+g3J61zJwb3K+K4ol0P7bGTUiCWcOux7lq3I4ckXj2bbdqcJ3LXzNh6/eyJVO7z85/WBLPwu+s0ggANGVLHmk3a8cXw3/NXCoJvLSc1yEuesu7I46k/l+LZHv48qp6uPTetTdu2XFnvpOzB2fuHUF4uxBnK8bBuVQ94Vy9AUDzVHZlBzZPtd76fNqKTm8Aw0ffdU8SlLd9B57Pf4OyWz9Zd51PWIYi1bwzefn4i8ilNryxWRtTi9tncD40XkcmAVcL57+gfAGUARUAX8CkBVy0TkTmCWe94dqrp3J8o+4v6e305pqT7+eu1UHn9+MFU7Unj3o768/NYRKMIvz5/LlZfM4l9PHE/ZlnQuHnMuldvS6N2rlNuv/5TfXn8OVTtSmi4kwkq/TUE8ynlfrKdmq4fJF3Uh/9hqKoq8pGUHyOnvY8OM1GiHaVpItvlJm1lJyRO9CWQkOc3eqeXsGJ4FQLsvKqg6dfctEd+BaWwc1xtt5yF1TiXZd6+l5LGDoxR9eBcwUtUL9/PWyQ2cq8CY/VznWeDZUMqOejVCRK4QkdkiMrvW17wmXVJSgNuu+4xPvzyQL2ceAEB5RTsC6nE6QT7tTZ+DnQ4CX10Slduc35rLf8ileGMHuudvDc+XaaEf3kun2wnVeLzQLidA54E1bF6QQsk3qaz9NI23Tsrn82tz2DA9lS+uz45anJs3eOncrXbXfm6+j9Jib9TiaUwsxpo6fzt1eSkEMpMhWage0oGUpTsA8GytI2V5NdVH7a4JanoS2s75r1pzVAeoUzxb971t0prCOM4vaqKe/FR1nKoOUtVBKd7mLAGoXDf6K1avy+St9w/ddTQ7a3fT5rijV7NyTRYAmR2q8YjTlOzapZKC/EqKN8bGvZWMfD8bZjiJ2VcllM5PJfPAOgZeV8G5nxfzs0+LGfbvzXQdUsMJ9zVZq4+YpfPSKehVS15hDcneAMNHlTN9SmbU4mlMLMbq75xMyrIdSE0AVEn9djt13Z0afdrXW6ke1B5Sdv/X9Gypc6ZSAbzLdoAqgQ7RWz1NEfwBT1BbLIv7Zu+hfUo4ddj3rFjViSfuce6LPvvqUZx43AoO6lmGqrBxU3seeGooAIf9aAOXnT8Pv9/5zfTgU0Op3B65puTn12azcWYa1Vs8vDksnyP+sJXUrAAz78yiuiyJT6/sTKcf1XLqM6X0uXgbX9+czYQzu4LCQT/dTqe+vojF1lwBv/DoLQXc9coKPEkw5bVsVi2LvZ5eiM1YfYekUz20A7nXrQCP4Dswje2nZQHQ7sutbPtp7h7np03bSsakLZAEmuJhy3Xdnd6xKGoL8/mJakSG0OxxIxPYCNymqs809pmOHQr06IENNuljzi+fnhjtEEJii5ZHzvp3+kU7hKCtuO4pdhStb1Hman9IVz3ysUuDOverU++d05JxfpEUyd7e/d3INMbEOY3x+3nBiPtmrzGmtcV+Z0YwLPkZY0JmNT9jTMJRBX/Akp8xJgG1hd5eS37GmJAo1uw1xiQk6/AwxiSoCA0PblWW/IwxIbNmrzEm4Ti9vbH93G4wLPkZY0JmzV5jTEKyZq8xJuEoYsnPGJOY2kCr15KfMSZECmqPtxljEpE1e40xCalN9/aKyMM00rRX1avDHs32apJmLg77ZSPhhb6xsYB08NrAT2uM6vbTJdEOIWhrAtUtvkYiPNs7u9WiMMbEDwXCt27vWOA37lUX4KzFmw+8BuQAc4BLVLVWRFKBF4CjgM3ABaq6srll7zf5qerzewWZrqqxuTK1MaZVhaPZKyIFwNVAP1XdISLjgZ/jLEx+v6q+JiJPAJcDj7t/blHVg0Xk58A9wAXNLb/JZ1REZKiILAa+c/ePEJHHmlugMSbeCRoIbgtCMtBORJKBdKAYOAl4033/eeAc9/Uodx/3/ZNFmr+MXTAP6D0AjMCpZqKq84FhzS3QGNMGaJAb5IrI7HrbFbsuoboOuA9YjZP0KnCaueWqunNV9rVAgfu6AFjjfrbOPT+nuV8hqN5eVV2zV4L1N7dAY0yc05A6PEr3t3SliHTCqc31AsqBN4CR4QgxGMHU/NaIyLGAiohXRK4H4qd7yxgTfsHX/BpzCvCDqm5SVR/wNnAckOU2gwG6A+vc1+uAQgD3/UzcFmlzBJP8RgNjcKqc64Ej3X1jTMKSILdGrQaGiEi6e+/uZGAx8BlwrnvOZcAE9/VEdx/3/U9Vm9/10mSzV1VLgYubW4Axpg0KtPwSqjpDRN4EvgHqgLnAOOB94DUR+Zt77Bn3I88AL4pIEVCG0zPcbE0mPxE5EHgQGIJTkZ0GjFXVFS0p2BgTp8I4zk9VbwNu2+vwCmBwA+dWA+eFpWCCa/a+AozHGXjYDeem5KvhCsAYE39Ug9tiWTDJL11VX1TVOnd7CUiLdGDGmBgWng6PqGrs2d5s9+WHInITzuMmijOi+oNWiM0YE6va+LO9c3CS3c5veWW99xS4OVJBGWNim8R4rS4YjT3b26s1AzHGxAkVSJTJTEWkP9CPevf6VPWFSAVljIlxbbnmt5OI3AYMx0l+HwCnA1/iTC1jjElEbSD5BdPbey7OyOsNqvor4Aicx0qMMYmqLff21rNDVQMiUiciHYES3OfrYo03JcB945fgTQmQlARffNiJlx7ozg33f88hh2+nzicsnZ/BQ7f0xF8XOyvOdz+omj8/vnLXftcetbx4X1feebpL9IJqwqDhWxl953qSPMqHr2Yz/pG8aIe0X7Ee67X/Ws0xp2ylvDSZK0/uC0CHrDr+/PhK8gpr2bgmhb+P7sm2ihhZdSKMg5yjKZgMMFtEsoCncHqAv8F5yqNRIlIoIp+JyGIRWSQi17Qs1Kb5aoUbL+rL7884jN+feSiDflxB3yO38dmEHH5z8mGMHtmf1LQAIy/YFOlQQrL2+zR+f1pffn9aX64a2YeaHR6++jAr2mHtl8ejjLlrHbde3IvfDu/DiaPK6dG75dOjR0I8xDplfDa3XHzgHsfOH1PC3C878Ovj+zH3yw5cMKYkStE1TDS4LZY1mfxU9feqWq6qTwCnApe5zd+m1AHXqWo/nEfjxohIv5aF2xShuioJgORkJTlZUWDW1Cx2Pmi9dH57cvNrIxtGCxx5fCXFq1IpWZcS7VD2q8+AKtavTGHD6lTqfB6mTshi6IiKaIfVoHiIdeGM9lSWJ+1xbOiICj5+wxlq+/Eb2QwdGVsxt+lmr4gMbOw9Vf2msQurajHOBIWoaqWILMGZGSaiKxR5PMrD7y6i2wHVvPtiHkvntd/1XlJygJN/UsrjdxwQyRBaZPiocqb+NyvaYTQqp6uPTet3J+fSYi99B8bmCgfxFGt9nXJ9lJV4ASgrSaZTri/KEe0p1mt1wWjsJsK/GnlPcaaaDoqI9AQGADMaeO8K4AqANNKDveR+BQLCmDP7k9Ghjr8+uZwDDqli1TLnulfduYoFMzuwaFaHFpcTCcneAENOq+DZf+RHOxQTUyT2VkuLtXiaobFBzieGowARaQ+8BfxRVbc2UM44nGls6OjJCdvvk+2Vycyf1pFBP65g1bJ0Lr56HZnZPh76c+9wFRF2R59YSdGCdMpLvdEOpVGbN3jp3G33rYPcfB+lxbEZczzFWt+WUi/ZXZzaX3YXH+WbY6SzA+KiSRuMiHZ5iogXJ/G9rKpvR7IsgMxsHxkdnKn/U1IDDDyhgjXfpzHyghKOGlbB3VcfHHu/QesZfs6WmG/yAiydl05Br1ryCmtI9gYYPqqc6VNic/RTPMVa3/QpHTnlvDIATjmvjGmTYyzmtnzPr6XcmVmfAZao6r8jVU592V18XHffCpKSFBH4/P1sZn7aifeXz2TjulTuf9u53fjVpE688nBBE1drXant/AwcVsmDN8bkKKI9BPzCo7cUcNcrK/AkwZTXslm1LDYn+omHWG96dCWHD91GZnYdL81exIv3deX1R/O45YmVjLxwMyVrnaEusUTCMJlptEkLZoFu/MIixwNf4CxEvPOv6s+qut8ZYTp6cnRI6ukRiSfctDZ2e4wbFOuTq8Wz5q+e2OpmBD5mq5a1KODUwkLtfs3YoM5d8afr5uxvAaNoC+bxNsGZxv5AVb1DRHoAXVV1ZmOfU9UvCWISf2NMfImHMXzBCOae32PAUOBCd78SeDRiERljYp9KcFsMC+ae3zGqOlBE5gKo6hYRid0RuMaYyGsDNb9gkp9PRJJwv66IdCYsazcZY+JVW2j2BpP8HgLeAbqIyN9xZnm5NaJRGWNil7aN3t5g1u19WUTm4ExrJcA5qrok4pEZY2JXG6j5Ndnh4fbuVgHv4qyYvt09ZoxJVGEa5CwiWSLypoh8JyJLRGSoiGSLyEcistz9s5N7rojIQyJSJCLfNjb/QDCC6e19H3jP/fMTnAWFP2xJocaY+BbGKa0eBCapal+ciZKXADcBn6hqb5ycc5N77ulAb3e7Ani8Jd8hmGbvYfX33Wz7+5YUaowxIpIJDAN+CaCqtUCtiIzCWToD4HlgKnAjMAp4QZ0nM6a7tcZ8dwapkIX8bK87ldUxzSnMGNNGBN/szRWR2fW2K+pdpRewCfiPiMwVkadFJAPIq5fQNgA7p94uANbU+/xa91izBPOEx7X1dj3AQGB9cws0xsS50Hp7Sxt5vC0ZJ5/8QVVniMiD7G7iOkWpqkhkBtYEU/PrUG9Lxbn3NyoSwRhj4kR4OjzWAmtVdec8n2/iJMONIpIP4P65cw7/dey5flB391izNFrzcwc3d1DV65tbgDGmbRHCM8hZVTeIyBoR6aOqS3GG0y12t8uAu90/J7gfmQhcJSKv4dx6q2ju/T5ofBr7ZFWtE5HjmntxY0wbFb6G6B+Al91HZlcAv8JpkY4XkcuBVcD57rkfAGcARTjD74JZS2i/Gqv5zcSpgs4TkYnAG8D2nW+2xuSkxpgYFMZZXVR1HtDQPcGTGzhXgTHhKTm4x9vSgM04a3YoTq1XAUt+xiSqNv54Wxe3p3chu5PeTm3g4RZjTHO19YkNkoD2NDwhaYS+uoLfH5lLh5knNTXaIYQkUB1bC3W3JZ70lq862FqkKkzL9rTx5Fesqne0WiTGmPgQB4sTBaOx5Bfb07AaY6KmrTd79+ltMcYYoG3X/FS1rDUDMcbEj4SYzNQYY/aQAPf8jDFmH0Lb6BCw5GeMCZ3V/Iwxiait9/YaY0zDLPkZYxJOoixdaYwx+7CanzEmEdk9P2NMYrLkZ4xJRFbzM8YkHqXNT2ZqjDH7CNcCRtHWppJfbn4tf7r/B7I614HCB6/kMuHZPE44cwu/GLuewoOruebsviz/NiPaoeJNCXDv64vxpihJScqXk7J56YHugHLZdWs5/owyAn54/+U8Jj7fNdrh7mPQ8K2MvnM9SR7lw1ezGf9IXtMfipJ4iNXjUR5651tKN6Zw+xU/ApTLxq7m+NM3EwgI77/SlYkv5Ec7zN0s+e2fiKQBn+Os9ZsMvKmqt0WqPICAX3jqb4UULUynXYafh99fwtwvOrJyaRp3XnEQV/9jVSSLD4mvVrjp4h9RXZVEUnKA+8YvZvbUTAoPriY3v5YrTjkcVSEzxxftUPfh8Shj7lrHzT8/kNJiLw9/sJzpkzNZvTwt2qHtI15iHXVZMau/b0d6e2cm81N/VuL8HIwY4PwcZNdGOcI9icZ/9gvTnNYNqgFOUtUjgCOBkSIyJILlUVbipWihM6X4ju1JrClKI6erjzVF7Vi7IrZ+2EGorkoCIDlZSU5WVIUzL97IKw8XoOo8Ol6x2RvNIBvUZ0AV61emsGF1KnU+D1MnZDF0REW0w2pQPMSa27WGwcO3MHn87hrpmRdu5JVHuu/+OShLiVZ4+wp2wfIYz48RS37q2Obuet2t1f468rrXcNChVSydG/0m7v54PMoj7y3g1VnfMPerTJbOb09+jxp+fOZmHpywkDue/Y5uPWNv7Y2crj42rd/9n7G02EtufuzVUCE+Yr3ylh945p8HEKjXiZDfo9r5OXh7Pnc8vZhuB+yIXoANEA1uC+paIkkiMldE3nP3e4nIDBEpEpHX3TV9EZFUd7/Ifb9nS75DJGt+O7/UPKAE+EhVZ0SyvJ3S0v3c+uQKnvy/Qqq2JbVGkc0SCAhXnXUYlxw7gEMO38YBh1ThTQlQW+PhmlH9mfR6F8besyLaYZoIGnxiGeWbvRQtar/HcefnQLjmp0cwaXweY/9RFKUIGyaB4LYgXQMsqbd/D3C/qh4MbAEud49fDmxxj9/vntdsEU1+qupX1SOB7sBgEem/9zkicoWIzBaR2T6taXGZScnKX55cwWfvZPPVpE4tvl5r2F6ZzLfTOzJoWAWlG1L4arIT99eTO9Grb1WUo9vX5g1eOnfbfQ8qN99HaXHsNc8h9mPtN7CSISdv4bnP5nDTA8s4YkgFf7pvGaUbUvlqSg4AX0/Jjr2fgzA1e0WkO3Am8LS7LzhrhL/pnvI8cI77epS7j/v+ye75zRLR5LeTqpYDnwEjG3hvnKoOUtVBXmnpcpDK2HtXsroojbefjr0evfoys31kdKgDICU1wIDjt7JmRRrTPurEEUO3AnDYMZWs+yHW7lXC0nnpFPSqJa+whmRvgOGjypk+JTPaYTUo1mN97l8HcMkJg/jliUdx9x8PYf70TO69/hCmfZzNEUOce5OHDd4aWz8HQTZ5g2z2PgDcwO6RgzlAuarWuftrgQL3dQGwBsB9v8I9v1ki2dvbGfCparmItANOpYXV1KYcevR2TvlZGT8sacejHy4G4Ll/FuBNCfC7O9aQmV3HHf8pYsXidG65pHckQ2lSpy4+rr/3ezxJigh88UE2Mz/txKJZHbjhge8559cbqN6exAM39YpqnA0J+IVHbyngrldW4EmCKa9ls2pZDP3nrCeeYq1v/JMF3PDvZZzzy2Kqqzw8cMvB0Q5pT8Hfvc8Vkdn19sep6jgAETkLKFHVOSIyPKzxBUE0Ql3WInI4ThU1CaeGOb6pdYA7erJ1SPKIiMQTbpIcX0MkbdHyyPFkxG6n2t6mV71Hhb+0RbPQt88p1P6njw3q3BkvXzdHVQc19J6I/AO4BKgD0oCOwDvACKCrqtaJyFDgdlUdISKT3dfTRCQZ2AB01mYmsYj9D1bVb4EBkbq+MSZ6JNDySpOq3gzcDODW/K5X1YtF5A3gXOA14DJggvuRie7+NPf9T5ub+KCV7vkZY9qQyI/zuxG4VkSKcO7pPeMefwbIcY9fC9zU7BJoY4+3GWNaR7hnclbVqcBU9/UKYHAD51QD54WrTEt+xpjQxfjTG8Gw5GeMCZnN6mKMSTwKtIGJDSz5GWNCZqu3GWMSjk1maoxJTKrW7DXGJCar+RljEpMlP2NMIrKanzEm8Sjgj//sZ8nPGBMyq/kZYxKT9fYaYxKR1fyMMYknDpalDEZMJT9JS0MOibHpuvejuluHaIcQkpRJs6IdQpv14fKvoh1C0AaP2Nb0SU0QQKzDwxiTiMTu+RljEo41e40xicme7TXGJCjr7TXGJCar+RljEo5ab68xJlHFf+6z5GeMCV1bGOpii5YbY0K3czbnprZGiEihiHwmIotFZJGIXOMezxaRj0RkuftnJ/e4iMhDIlIkIt+KyMCWfAVLfsaY0CgQCHJrXB1wnar2A4YAY0SkH3AT8Imq9gY+cfcBTgd6u9sVwOMt+RqW/IwxIREU0eC2xqhqsap+476uBJYABcAo4Hn3tOeBc9zXo4AX1DEdyBKR/OZ+D7vnZ4wJXSDotStzRWR2vf1xqjpu75NEpCcwAJgB5KlqsfvWBiDPfV0ArKn3sbXusWKawZKfMSY0O5u9wSlV1UGNnSAi7YG3gD+q6lYR2V2UqopEZki1NXuNMSELR7MXQES8OInvZVV92z28cWdz1v2zxD2+Diis9/Hu7rFmseRnjAldeHp7BXgGWKKq/6731kTgMvf1ZcCEescvdXt9hwAV9ZrHIbNmrzEmRGGb2OA44BJggYjMc4/9GbgbGC8ilwOrgPPd9z4AzgCKgCrgVy0p3JKfMSY0YVq9TVW/xJkbtSEnN3C+AmNaXLAr7pPf2LEzGXzMesrLU/nd6NMBOPDALfzhD7PxpgTw+4VHHzmKZctyADjs8BKuvHIuyckBtlakcsMNJ7VarN7kOh686X1SvH6SPAH+N7sXz004iq65lfx19Kd0zKhh2aoc7npqOHX+JM47bQFnDFuK3y9UVLbjn/85gY2bY2MG6UHDtzL6zvUkeZQPX81m/CN5TX8oSmIl1n+NLWTGxx3Jyq1j3GdLAXj+n12ZNjkTEcjK9XH9A6vJ6VrHG4915tO3swHw+2HN8jReX7CQjp38bKtI4v7rC1n5XRoicO2/V9NvUFWrfpe28IRHxJOfiCQBs4F1qnpWuK//0Uc9mfjuwVx//Yxdxy6/fD4vv9yf2bPzOfro9Vz+m/nceMNJZGTUctWYOdx66zA2bcogM7M63OE0yleXxLX3nkF1jZekpAAP3/wuMxYUcv6IBbwxpT+fzTyIsZd8yRknLGXi1H4sX53D6DvOoaY2mbOHL+bK82ZyxxP7/EJsdR6PMuauddz88wMpLfby8AfLmT45k9XL06Id2j5iKdbTLijj7F+Vcu81PXYdO/d3JVx2wwYA/vt0Li/d35Vr7lnLeb/fxHm/3wTA9CkdefupznTs5Afg8b8WMGj4Vv7y1Ep8tULNjijcum8Dya81/tauwRm8GBELF3ahsjJ1j2OKkJ7uAyA9w8fmze0AGH7iKr76ujubNmUAUFHR2v8BhOoaLwDJSQGSkpzxAgP6rud/s3sBMPnr3hw/cBUA877rRk2t8/tp8YoudO60vZXjbVifAVWsX5nChtWp1Pk8TJ2QxdARFdEOq0GxFOthQ7bTwU1gO2V02D1mpHqHB2mgEfjZfzsx/JwtAGzf6mHB9AxGXlQGgDdFaZ/p3/dDkaRAQIPbYlhEa34i0h04E/g7cG0ky6rvyScG8Le//4/f/HYeInDdtU5tqXtBJUnJyj3//JR27eqY8N/efPJJr9YKCwCPBHjytv9S0GUr//20H+tKOrKtKpVAwPk9tKksg9ysfZswZ5ywjBkLCvc5Hg05XX1sWp+ya7+02Evfga3b7ApWPMT6n7u78vEb2WR09PPPN4v2eK+6Spg9tQNj/r4WgA2rU8nMqeNfY3uwYlEavQ/fwe/uXEdaevAD71qubczkHOma3wPADTQyJFJErhCR2SIyu7YuPDWbM88qYtyTR3LpJWcz7skj+eNYZ+UyT5LS++Ay/vqXYdx6y4+58KLFFBRUhqXMYAXUw29v/ynnXXchfXttokd+eZOfOWXIcvr03MTrkw6PfICm1f3qpg28PGcxJ/10CxOf7bzHe9M/yuTQQdt3NXn9fihakM5Zl5by2EfLSEsP8PojXVo/6DAMdYm2iCU/ETkLKFHVOY2dp6rjVHWQqg5KSc4IS9mnnLKSr77qDsAXXxTS55DNAJSWpjNnTj41Ncls3ZrKwoWd6XVgeVjKDNX2HanM+y6fQw8qoX16DR6P8/uhc/Z2SsvTd503sN86fnHWPG556DR8dUlRiXVvmzd46dytdtd+br6P0mJvFCPav3iK9aSfbOHLDzL3OPa/CVm7mrzgxN8537er9nr8WeUULWjXqnE6vb2B4LYYFsma33HA2SKyEngNOElEXopgebts3pzGYYc7N4uPPLKEdeudHtLp0wo49NBNeDwBUlPr6NNnM2tWt17vaWaHHWS0qwEgxVvHUYeuY1VxFnO/68aPB/0AwIhjl/PV3AMAOLhHKdde+iW3PHQa5ZWt/APeiKXz0inoVUteYQ3J3gDDR5UzfUpm0x+MgliPdd2K3U3yaZMzKTy4Ztf+9q0evp3enmNHbt11LLtLHbndallT5NznnvdFB3r03v2Z1qGggeC2GBaxe36qejNwM4CIDAeuV9VfhLucG2+axuGHl9CxYw0vvjiRF1/qz0MPHs2Vo+eSlBSgtjaJhx50Hi1cs6Yjs+fk8/jjkwkoTJ50IKtWZYU7pP3Kyazipss/x+MJ4BGYOqsX0+f3YNX6LP5y5Wdc/pM5LF+dwwdf9AFg9PkzaZfq4/bffwLAxs3tufXh01ot3v0J+IVHbyngrldW4EmCKa9ls2pZ7PX0QmzF+o/fHcC309pTUZbMxUf145LrNjDz046s/T4Vjwe6FNRy9T1rd53/1YdZHDWscp/7eWP+to57rjqAOp/QtUct192/urW/Ssw3aYMh2gpfol7ya3SoS2Z6Nx1yyOURjyccqrvFxni7YKVMmhXtENqsyevnRTuEoA0esYbZ86v3N7A4KJkpeXps1wuDOnfSmgfnNDWxQbS0yiBnVZ0KTG2NsowxraAN1Pzi/gkPY0wUWPIzxiQcVWfMTZyz5GeMCZ3V/IwxCcmSnzEm8cT+c7vBsORnjAmNgsb4AOZgWPIzxoQuxh9dC4YlP2NMaFRDWboyZlnyM8aEzjo8jDGJSK3mZ4xJPLE/V18wLPkZY0Kzcxr7OGfJzxgTEgW0DTzeFoVln4wxcU3DN5mpiIwUkaUiUiQiN7VC9LtYzc8YEzINQ7PXXdb2UeBUYC0wS0QmquriFl88CFbzM8aELjw1v8FAkaquUNVanOUuRkU8dlerzOQcLBHZBKwK82VzgdIwXzOS4ineeIoV4iveSMV6gKp2bvq0/RORSTjxBSMNqK63P05Vx7nXORcYqaq/cfcvAY5R1ataEl+wYqrZ29J/lIaIyOxYnUa7IfEUbzzFCvEVbyzHqqojox1DOFiz1xgTLeuAwnr73d1jrcKSnzEmWmYBvUWkl4ikAD8HJrZW4THV7I2QcdEOIETxFG88xQrxFW88xdosqlonIlcBk4Ek4FlVXdRa5cdUh4cxxrQWa/YaYxKSJT9jTEJq08kvmo/OhEpEnhWREhFZGO1YmiIihSLymYgsFpFFInJNtGPaHxFJE5GZIjLfjfX/oh1TMEQkSUTmish70Y6lrWqzya/eozOnA/2AC0WkX3SjatRzQLyMn6oDrlPVfsAQYEwM/93WACep6hHAkcBIERkS3ZCCcg2wJNpBtGVtNvkR5UdnQqWqnwNl0Y4jGKparKrfuK8rcf6TFkQ3qoapY5u763W3mO7lE5HuwJnA09GOpS1ry8mvAFhTb38tMfofNJ6JSE9gADAjyqHsl9uEnAeUAB+paszG6noAuAGI/+mSY1hbTn4mwkSkPfAW8EdV3RrtePZHVf2qeiTOEwSDRaR/lEPaLxE5CyhR1TnRjqWta8vJL6qPzrR1IuLFSXwvq+rb0Y4nGKpaDnxGbN9bPQ44W0RW4tyqOUlEXopuSG1TW05+UX10pi0TEQGeAZao6r+jHU9jRKSziGS5r9vhzB33XVSDaoSq3qyq3VW1J87P7Keq+osoh9Umtdnkp6p1wM5HZ5YA41vz0ZlQicirwDSgj4isFZHLox1TI44DLsGplcxztzOiHdR+5AOfici3OL8QP1JVGz5i7PE2Y0xiarM1P2OMaYwlP2NMQrLkZ4xJSJb8jDEJyZKfMSYhWfKLIyLid4eVLBSRN0QkvQXXes5dPQsRebqxiQlEZLiIHNuMMlaKyD6rfO3v+F7nbGvs/QbOv11Erg81RpO4LPnFlx2qeqSq9gdqgdH13xSRZi1LoKq/aWKh6OFAyMnPmFhmyS9+fQEc7NbKvhCRicBi9yH+e0Vkloh8KyJXgvNUhog84s5v+DHQZeeFRGSqiAxyX48UkW/c+e8+cScuGA2MdWudJ7hPTbzlljFLRI5zP5sjIlPcefOeBqSpLyEi/xWROe5nrtjrvfvd45+ISGf32EEiMsn9zBci0jcsf5sm4STCAkZtjlvDOx2Y5B4aCPRX1R/cBFKhqkeLSCrwlYhMwZl5pQ/O3IZ5wGLg2b2u2xl4ChjmXitbVctE5Algm6re5573CnC/qn4pIj1wnqL5EXAb8KWq3iEiZwLBPKXya7eMdsAsEXlLVTcDGcBsVR0rIn91r30VzsI+o1V1uYgcAzwGnNSMv0aT4Cz5xZd27tRM4NT8nsFpjs5U1R/c46cBh++8nwdkAr2BYcCrquoH1ovIpw1cfwjw+c5rqer+5hc8BejnPOILQEd3hpdhwE/dz74vIluC+E5Xi8hP3NeFbqybcaZzet09/hLwtlvGscAb9cpODaIMY/ZhyS++7HCnZtrFTQLb6x8C/qCqk/c6L5zP3nqAIapa3UAsQROR4TiJdKiqVonIVCBtP6erW2753n8HxjSH3fNreyYDv3OnnEJEDhGRDOBz4AL3nmA+cGIDn50ODBORXu5ns93jlUCHeudNAf6wc0dEjnRffg5c5B47HejURKyZwBY38fXFqXnu5AF21l4vwmlObwV+EJHz3DJERI5oogxjGmTJr+15Gud+3jfiLIb0JE4N/x1gufveCzgzyOxBVTcBV+A0Meezu9n5LvCTnR0ewNXAILdDZTG7e53/Dyd5LsJp/q5uItZJQLKILAHuxkm+O23HmXh0Ic49vTvc4xcDl7vxLSKGlyYwsc1mdTHGJCSr+RljEpIlP2NMQrLkZ4xJSJb8jDEJyZKfMSYhWfIzxiQkS37GmIT0/wFp6kpG2d58ZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[0,1,2,3,4])\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-assurance",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "yellow-marathon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu (TensorFlow (None, 150, 150, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 32)        9248      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_1 (TensorFl (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 32)        9248      \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Relu_2 (TensorFl (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 21,829\n",
      "Trainable params: 21,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import save_model\n",
    "save_model(model, \"model1.h5\")\n",
    "# load and evaluate a saved model\n",
    "loaded_model = models.load_model('model1.h5')\n",
    "# summarize model.\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ranking-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 4, 4, ..., 1, 1, 4])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=loaded_model\n",
    "train_pred_p=model.predict(X_train)\n",
    "train_pred = np.argmax(train_pred_p, axis=1)\n",
    "train_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "occupational-elevation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2798  401    0    0  434]\n",
      " [ 948 4992    0    0 3369]\n",
      " [ 146   31    0    0   46]\n",
      " [  51  123    0    3  139]\n",
      " [ 663 1199    0    0 5512]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "regulated-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_p=model.predict(X_test)\n",
    "test_pred = np.argmax(test_pred_p, axis=1)\n",
    "test_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "instructional-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 61.81%\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "# summarize model.\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-copying",
   "metadata": {},
   "source": [
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # dimesion reduction\n",
    "        rotation_range=30,  # randomly rotate images in the range 5 degrees\n",
    "        zoom_range = 0.3, # Randomly zoom image 10%\n",
    "        width_shift_range=0.3,  # randomly shift images horizontally 10%\n",
    "        height_shift_range=0.4,  # randomly shift images vertically 10%\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True)  # randomly flip images\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-approval",
   "metadata": {},
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-federation",
   "metadata": {},
   "source": [
    "model=Sequential()      \n",
    "Pre_trained_model= ResNet50(include_top=False, weights='imagenet', \n",
    "                            pooling='avg',classes=5,input_shape=(150,150,3))\n",
    "for layer in Pre_trained_model.layers:\n",
    "    layer.trainable=False\n",
    "model.add(Pre_trained_model)\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax')) \n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-natural",
   "metadata": {},
   "source": [
    "model.fit(X_train, Y_train, batch_size=24,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-flood",
   "metadata": {},
   "source": [
    "from sklearn.metrics import log_loss\n",
    "Pred=model.predict(X_test)\n",
    "Loss=log_loss(Pred,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "speaking-expense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8663, 5), (5213, 5))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "killing-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "corporate-reynolds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DR</th>\n",
       "      <th>G</th>\n",
       "      <th>ND</th>\n",
       "      <th>WD</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_QW9GQM</td>\n",
       "      <td>0.663280</td>\n",
       "      <td>0.161265</td>\n",
       "      <td>5.029074e-02</td>\n",
       "      <td>0.117644</td>\n",
       "      <td>7.519532e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_G9VPQ9</td>\n",
       "      <td>0.895024</td>\n",
       "      <td>0.099911</td>\n",
       "      <td>1.679032e-06</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>7.742774e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_QIMIEM</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.953264</td>\n",
       "      <td>8.458109e-12</td>\n",
       "      <td>0.025384</td>\n",
       "      <td>1.323434e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_TSINUQ</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.985767</td>\n",
       "      <td>2.105870e-21</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>6.537254e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_I7OOHH</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.427475</td>\n",
       "      <td>1.124914e-08</td>\n",
       "      <td>0.534736</td>\n",
       "      <td>2.815915e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID        DR         G            ND        WD         other\n",
       "0  ID_QW9GQM  0.663280  0.161265  5.029074e-02  0.117644  7.519532e-03\n",
       "1  ID_G9VPQ9  0.895024  0.099911  1.679032e-06  0.005064  7.742774e-11\n",
       "2  ID_QIMIEM  0.021352  0.953264  8.458109e-12  0.025384  1.323434e-10\n",
       "3  ID_TSINUQ  0.000115  0.985767  2.105870e-21  0.014118  6.537254e-16\n",
       "4  ID_I7OOHH  0.009630  0.427475  1.124914e-08  0.534736  2.815915e-02"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys=Test_fn\n",
    "values=pred\n",
    "my_dict = dict(zip(keys, values))\n",
    "sub=pd.read_csv('test.csv')\n",
    "filenames=sub['filename'].to_list()\n",
    "Pred1=[my_dict[filenames[i]] for i in range(len(filenames))]\n",
    "\n",
    "Pred=pd.DataFrame(data=Pred1, columns=['DR','G','ND','other','WD'])\n",
    "submission = pd.concat([sub, Pred], axis=1)\n",
    "columns=['ID','DR','G','ND','WD','other']\n",
    "submission=submission[columns]\n",
    "# Create submission csv file csv file\n",
    "submission.to_csv('submission5.csv', index = False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-african",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
